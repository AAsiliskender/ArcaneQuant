{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6fdd67-383f-4bc2-bbca-2bfb8e819e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tkdesigner\n",
      "  Using cached tkdesigner-1.0.7-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting Jinja2==3.0.1 (from tkdesigner)\n",
      "  Using cached Jinja2-3.0.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting Pillow<9.0.0,>=8.4.0 (from tkdesigner)\n",
      "  Using cached Pillow-8.4.0.tar.gz (49.4 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting requests==2.25.1 (from tkdesigner)\n",
      "  Using cached requests-2.25.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting urllib3<2.0.0,>=1.26.6 (from tkdesigner)\n",
      "  Using cached urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from Jinja2==3.0.1->tkdesigner) (2.1.3)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests==2.25.1->tkdesigner) (4.0.0)\n",
      "Collecting idna<3,>=2.5 (from requests==2.25.1->tkdesigner)\n",
      "  Using cached idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests==2.25.1->tkdesigner) (2025.1.31)\n",
      "Using cached tkdesigner-1.0.7-py3-none-any.whl (14 kB)\n",
      "Using cached Jinja2-3.0.1-py3-none-any.whl (133 kB)\n",
      "Using cached requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
      "Using cached urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
      "Using cached idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "Building wheels for collected packages: Pillow\n",
      "  Building wheel for Pillow (setup.py): started\n",
      "  Building wheel for Pillow (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for Pillow\n",
      "Failed to build Pillow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py bdist_wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [185 lines of output]\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\BdfFontFile.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\BlpImagePlugin.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\BmpImagePlugin.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\BufrStubImagePlugin.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\ContainerIO.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\CurImagePlugin.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\DcxImagePlugin.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\DdsImagePlugin.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\EpsImagePlugin.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\ExifTags.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\features.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\FitsStubImagePlugin.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\FliImagePlugin.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\FontFile.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\FpxImagePlugin.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\FtexImagePlugin.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\GbrImagePlugin.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\GdImageFile.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\GifImagePlugin.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\GimpGradientFile.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\GimpPaletteFile.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\GribStubImagePlugin.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\Hdf5StubImagePlugin.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\IcnsImagePlugin.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\IcoImagePlugin.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\Image.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\ImageChops.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\ImageCms.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\ImageColor.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\ImageDraw.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\ImageDraw2.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\ImageEnhance.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\ImageFile.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\ImageFilter.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\ImageFont.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\ImageGrab.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\ImageMath.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\ImageMode.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\ImageMorph.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\ImageOps.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\ImagePalette.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\ImagePath.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\ImageQt.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\ImageSequence.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\ImageShow.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\ImageStat.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\ImageTk.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\ImageTransform.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\ImageWin.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\ImImagePlugin.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\ImtImagePlugin.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\IptcImagePlugin.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\Jpeg2KImagePlugin.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\JpegImagePlugin.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\JpegPresets.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\McIdasImagePlugin.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\MicImagePlugin.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\MpegImagePlugin.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\MpoImagePlugin.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\MspImagePlugin.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\PaletteFile.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\PalmImagePlugin.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\PcdImagePlugin.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\PcfFontFile.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\PcxImagePlugin.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\PdfImagePlugin.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\PdfParser.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\PixarImagePlugin.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\PngImagePlugin.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\PpmImagePlugin.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\PsdImagePlugin.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\PSDraw.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\PyAccess.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\SgiImagePlugin.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\SpiderImagePlugin.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\SunImagePlugin.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\TarIO.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\TgaImagePlugin.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\TiffImagePlugin.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\TiffTags.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\WalImageFile.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\WebPImagePlugin.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\WmfImagePlugin.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\XbmImagePlugin.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\XpmImagePlugin.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\XVThumbImagePlugin.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\_binary.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\_tkinter_finder.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\_util.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\_version.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\__init__.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      copying src\\PIL\\__main__.py -> build\\lib.win-amd64-cpython-312\\PIL\n",
      "      running egg_info\n",
      "      writing src\\Pillow.egg-info\\PKG-INFO\n",
      "      writing dependency_links to src\\Pillow.egg-info\\dependency_links.txt\n",
      "      writing top-level names to src\\Pillow.egg-info\\top_level.txt\n",
      "      reading manifest file 'src\\Pillow.egg-info\\SOURCES.txt'\n",
      "      reading manifest template 'MANIFEST.in'\n",
      "      warning: no files found matching '*.c'\n",
      "      warning: no files found matching '*.h'\n",
      "      warning: no files found matching '*.sh'\n",
      "      warning: no previously-included files found matching '.appveyor.yml'\n",
      "      warning: no previously-included files found matching '.clang-format'\n",
      "      warning: no previously-included files found matching '.coveragerc'\n",
      "      warning: no previously-included files found matching '.editorconfig'\n",
      "      warning: no previously-included files found matching '.readthedocs.yml'\n",
      "      warning: no previously-included files found matching 'codecov.yml'\n",
      "      warning: no previously-included files matching '.git*' found anywhere in distribution\n",
      "      warning: no previously-included files matching '*.pyc' found anywhere in distribution\n",
      "      warning: no previously-included files matching '*.so' found anywhere in distribution\n",
      "      no previously-included directories found matching '.ci'\n",
      "      adding license file 'LICENSE'\n",
      "      writing manifest file 'src\\Pillow.egg-info\\SOURCES.txt'\n",
      "      running build_ext\n",
      "      \n",
      "      \n",
      "      The headers or library files could not be found for zlib,\n",
      "      a required dependency when compiling Pillow from source.\n",
      "      \n",
      "      Please see the install instructions at:\n",
      "         https://pillow.readthedocs.io/en/latest/installation.html\n",
      "      \n",
      "      Traceback (most recent call last):\n",
      "        File \"C:\\Users\\a_asi\\AppData\\Local\\Temp\\pip-install-llkis_b3\\pillow_f8b621a1d0744fe2aebb6af50fe7baa6\\setup.py\", line 978, in <module>\n",
      "          setup(\n",
      "        File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\setuptools\\__init__.py\", line 117, in setup\n",
      "          return distutils.core.setup(**attrs)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\setuptools\\_distutils\\core.py\", line 183, in setup\n",
      "          return run_commands(dist)\n",
      "                 ^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\setuptools\\_distutils\\core.py\", line 199, in run_commands\n",
      "          dist.run_commands()\n",
      "        File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 954, in run_commands\n",
      "          self.run_command(cmd)\n",
      "        File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\setuptools\\dist.py\", line 950, in run_command\n",
      "          super().run_command(command)\n",
      "        File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 973, in run_command\n",
      "          cmd_obj.run()\n",
      "        File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\setuptools\\command\\bdist_wheel.py\", line 398, in run\n",
      "          self.run_command(\"build\")\n",
      "        File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\setuptools\\_distutils\\cmd.py\", line 316, in run_command\n",
      "          self.distribution.run_command(command)\n",
      "        File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\setuptools\\dist.py\", line 950, in run_command\n",
      "          super().run_command(command)\n",
      "        File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 973, in run_command\n",
      "          cmd_obj.run()\n",
      "        File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\setuptools\\_distutils\\command\\build.py\", line 135, in run\n",
      "          self.run_command(cmd_name)\n",
      "        File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\setuptools\\_distutils\\cmd.py\", line 316, in run_command\n",
      "          self.distribution.run_command(command)\n",
      "        File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\setuptools\\dist.py\", line 950, in run_command\n",
      "          super().run_command(command)\n",
      "        File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 973, in run_command\n",
      "          cmd_obj.run()\n",
      "        File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\setuptools\\command\\build_ext.py\", line 98, in run\n",
      "          _build_ext.run(self)\n",
      "        File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\setuptools\\_distutils\\command\\build_ext.py\", line 359, in run\n",
      "          self.build_extensions()\n",
      "        File \"C:\\Users\\a_asi\\AppData\\Local\\Temp\\pip-install-llkis_b3\\pillow_f8b621a1d0744fe2aebb6af50fe7baa6\\setup.py\", line 790, in build_extensions\n",
      "          raise RequiredDependencyException(f)\n",
      "      RequiredDependencyException: zlib\n",
      "      \n",
      "      During handling of the above exception, another exception occurred:\n",
      "      \n",
      "      Traceback (most recent call last):\n",
      "        File \"<string>\", line 2, in <module>\n",
      "        File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "        File \"C:\\Users\\a_asi\\AppData\\Local\\Temp\\pip-install-llkis_b3\\pillow_f8b621a1d0744fe2aebb6af50fe7baa6\\setup.py\", line 1037, in <module>\n",
      "          raise RequiredDependencyException(msg)\n",
      "      RequiredDependencyException:\n",
      "      \n",
      "      The headers or library files could not be found for zlib,\n",
      "      a required dependency when compiling Pillow from source.\n",
      "      \n",
      "      Please see the install instructions at:\n",
      "         https://pillow.readthedocs.io/en/latest/installation.html\n",
      "      \n",
      "      \n",
      "      C:\\Users\\a_asi\\AppData\\Local\\Temp\\pip-install-llkis_b3\\pillow_f8b621a1d0744fe2aebb6af50fe7baa6\\setup.py:46: RuntimeWarning: Pillow 8.4.0 does not support Python 3.12 and does not provide prebuilt Windows binaries. We do not recommend building from source on Windows.\n",
      "        lambda: warnings.warn(\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for Pillow\n",
      "ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (Pillow)\n"
     ]
    }
   ],
   "source": [
    "#%%writefile __init__.py\n",
    "###### -- Python Script in Jupyter to access, create and test financial data and models\n",
    "###### -- By Ahmed Asiliskender, initial write date 25 June 2024\n",
    "###### -- May also access MATLAB scripts through here and .py files.\n",
    "\n",
    "### Here we initialise important libraries and variables.\n",
    "\n",
    "## To download packages using pip\n",
    "import sys #! allows to use command terminal code in here\n",
    "#!{sys.executable} --version\n",
    "#!pip install html5lib\n",
    "#!pip install bs4\n",
    "#!pip install yfinance\n",
    "#!pip install tradingview-scraper\n",
    "#!pip install --upgrade --no-cache tradingview-scraper\n",
    "#!pip install selenium\n",
    "#!pip install sqlalchemy\n",
    "#!pip install python-dotenv\n",
    "#!pip install pandas-ta\n",
    "#!pip install pytest\n",
    "#!pip install python-on-whales\n",
    "\n",
    "# GUI Designer\n",
    "!pip3 install pyqt6\n",
    "\n",
    "# Security testing\n",
    "#!pip install bandit\n",
    "\n",
    "# Cmd terminal environment install (psycopg2)\n",
    "#!pip install psycopg2-binary \n",
    "# Conda environment install\n",
    "#!conda install -c anaconda psycopg2 \n",
    "\n",
    "\n",
    "\n",
    "# Import pandas (python data analysis lib) and data analysis packages\n",
    "#%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "#user_header = {'user-agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) \\\n",
    "#                                AppleWebKit/537.36 (KHTML, like Gecko) \\\n",
    "#                                Chrome/122.0.0.0 Safari/537.36'}\n",
    "\n",
    "# Webscrape libs\n",
    "from bs4 import BeautifulSoup\n",
    "import yfinance as yf\n",
    "import tradingview_scraper as tvs\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from tradingview_scraper.symbols.ideas import Ideas\n",
    "\n",
    "# Other libs (system, graphical or time-compute analysis)\n",
    "import os\n",
    "from io import StringIO\n",
    "import psycopg2\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from dotenv.main import set_key\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy import types as sqltype\n",
    "import sqlalchemy.exc as sqlexc\n",
    "from colorama import Fore, Back, Style\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import time\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from ast import literal_eval\n",
    "import json\n",
    "import unittest\n",
    "import pytest\n",
    "\n",
    "#import warnings\n",
    "\n",
    "\n",
    "# My libs\n",
    "#from arcanequant.quantlib import *\n",
    "\n",
    "# Paid APIs, (not used, left here)\n",
    "\n",
    "# Bloomberg (not free)\n",
    "#!pip install blpapi --index-url=https://blpapi.bloomberg.com/repository/releases/python/simple/ blpapi\n",
    "#!pip install xbbg\n",
    "#from xbbg import blp\n",
    "#blp.bdh( tickers='SPX Index', flds=['High', 'Low', 'Last_Price'], start_date='2018-10-10', end_date='2018-10-20')\n",
    "#blp.bdp('AAPL US Equity', 'Eqy_Weighted_Avg_Px', VWAP_Dt='20181224')\n",
    "#blp.bdp(tickers='NVDA US Equity', flds=['Security_Name', 'GICS_Sector_Name'])\n",
    "\n",
    "\n",
    "### RAPID API FOR INTRADAYS (not free)\n",
    "#import http.client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45a8de56-48a1-4076-a32d-4502bacb6379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a_asi\\FinanceProject\\ArcaneQuant\n"
     ]
    }
   ],
   "source": [
    "import tkinter\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "x = Path(\".\")\n",
    "print(x.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77f18e65-bb22-43e3-b8da-4b6e68aff79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#conn = http.client.HTTPSConnection(\"yahoo-finance127.p.rapidapi.com\")\n",
    "\n",
    "#headers = {\n",
    "#    'x-rapidapi-key': \"2e7bf1e71cmsh8f7a5babc8f5197p1f02bejsn72107f046a86\",\n",
    "#    'x-rapidapi-host': \"yahoo-finance127.p.rapidapi.com\"\n",
    "#}\n",
    "\n",
    "#conn.request(\"GET\", \"/finance-analytics/nvda\", headers=headers)\n",
    "\n",
    "#res = conn.getresponse()\n",
    "#data = res.read()\n",
    "\n",
    "#print(data.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4cf7b04-0dd2-44d7-925f-746fa44e2c6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Data Manifest Initialised\n",
      "Data Manifest Initialised\n",
      "Connecting to engine: Engine(postgresql+psycopg2://postgres:***@localhost:5432/marketdata)\n",
      "Loading Manifest Data\n",
      "Load path/name: data/StockHistData/dataManifest.json\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from arcanequant.quantlib import *\n",
    "## Here we create settings for the database building/development:\n",
    "# - Tickers to download\n",
    "# - Months of intraday data to request\n",
    "# - Time intervals (granularity/resolution)\n",
    "# - API request key (taken from file)\n",
    "\n",
    "savepath = r'data/StockHistData/'\n",
    "\n",
    "# Tickers to request, make sure it is the correct one (there is a search API call to check)\n",
    "dltickers = [\"KO\",'ARM','NVDA',\"MSFT\"]\n",
    "\n",
    "# Months to request (string, \"year-month\") i.e. 2020-02\n",
    "dlmonths = [\"2025-07\",\"2025-06\",\"2025-05\"] # Need to add functionality to take whole years\n",
    "\n",
    "# Options for time resolution: 1, 5, 15, 30, 60\n",
    "dlintervals = [1, 5, 15, 30, 60]\n",
    "\n",
    "##### Alphavantage API key for data acquisition\n",
    "# .env file method\n",
    "env_path = Path(\".\") / \"APIkey.env\" # Environment variables file must be same folder as this code (#REGEXSEARCHFOR API IF NO FILE)\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "alphaAPIkey = os.getenv(\"ALPHA_API_KEY\") #MAYBE TRY .TXT IF NO .ENV\n",
    "# .txt file method\n",
    "#with open('APIkey.txt',) as keyfile:\n",
    "#    alphaAPIkey = keyfile.read()\n",
    "\n",
    "\n",
    "##### Setting environment variables for the SQL login details\n",
    "SQLloginfilename = \"SQLlogin\" # Default filename \"SQLlogin\" \n",
    "#SQLdetails_path = Path(\".\") / f\"{SQLloginfilename}.env\"\n",
    "#set_key(SQLdetails_path, 'DRIVER', 'hello') #dotenv.set_key\n",
    "#set_key(SQLdetails_path, 'DIALECT', 'mate')\n",
    "#set_key(SQLdetails_path, 'ENV_USER', 'put')\n",
    "#set_key(SQLdetails_path, 'PASSWORD', 'your')\n",
    "#set_key(SQLdetails_path, 'HOST_MACHINE', 'own')\n",
    "#set_key(SQLdetails_path, 'PORT', 'stuff')\n",
    "#set_key(SQLdetails_path, 'DBNAME', 'here')\n",
    "\n",
    "x = DataManifest()\n",
    "\n",
    "checkManifest = DataManifest()\n",
    "checkManifest.connectSQL(SQLloginfilename)\n",
    "checkManifest.loadManifest(loadFrom = 'direct', path = savepath)\n",
    "\n",
    "\n",
    "fileVal = checkManifest.DF.loc[(('GOOG',5),'2023-01')]\n",
    "print(fileVal)\n",
    "\n",
    "\n",
    "##### Sourcing market data\n",
    "#DownloadIntraday(savepath, dltickers, dlintervals, dlmonths, alphaAPIkey, saveMode='both', verbose=True) # TODO: ADD SAVING TO BOTH STORAGE\n",
    "#x = checkManifest.DF.copy()\n",
    "\n",
    "#DownloadIntraday(dltickers, dlintervals, dlmonths, alphaAPIkey, saveMode = 'both', dataManifest=checkManifest, verbose = True)\n",
    "#print(x == checkManifest.DF) # Check what is changed\n",
    "#x = checkManifest.DF.copy()\n",
    "#checkManifest.loadManifest(loadFrom = 'database')\n",
    "#print('sql vs direct')\n",
    "#print(x == checkManifest.DF) # Check what is different\n",
    "\n",
    "#checkManifest = DataManifest()\n",
    "#checkManifest.loadManifest(path = savepath)\n",
    "#checkManifest.connectSQL(SQLloginfilename)\n",
    "\n",
    "#checkManifest.validateManifest(fastValidate = False, show = False)\n",
    "#print(checkManifest.DF) # Maybe I can add method to show manifest in a more compact form\n",
    "#checkManifest.saveManifest(saveTo='direct',savePath=checkManifest.directory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81eb65b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking data unit -> Ticker, Interval, Month: ('KO', 1, '2025-01')\n",
      "Extracting data from SQL database\n",
      "Searching for file KO_1_2025-01\n",
      "Loading data file: KO_1_2025-01.csv\n",
      "Loading meta data file: KO_1_2025-01_meta.csv\n",
      "File KO_1_2025-01 exists both in files and database.\n",
      "Loading data file: KO_1_2025-01.csv\n",
      "Loading meta data file: KO_1_2025-01_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('KO', 5, '2025-01')\n",
      "Extracting data from SQL database\n",
      "Searching for file KO_5_2025-01\n",
      "Loading data file: KO_5_2025-01.csv\n",
      "Loading meta data file: KO_5_2025-01_meta.csv\n",
      "File KO_5_2025-01 exists both in files and database.\n",
      "Loading data file: KO_5_2025-01.csv\n",
      "Loading meta data file: KO_5_2025-01_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('KO', 15, '2025-01')\n",
      "Extracting data from SQL database\n",
      "Searching for file KO_15_2025-01\n",
      "Loading data file: KO_15_2025-01.csv\n",
      "Loading meta data file: KO_15_2025-01_meta.csv\n",
      "File KO_15_2025-01 exists both in files and database.\n",
      "Loading data file: KO_15_2025-01.csv\n",
      "Loading meta data file: KO_15_2025-01_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('KO', 30, '2025-01')\n",
      "Extracting data from SQL database\n",
      "Searching for file KO_30_2025-01\n",
      "Loading data file: KO_30_2025-01.csv\n",
      "Loading meta data file: KO_30_2025-01_meta.csv\n",
      "File KO_30_2025-01 exists both in files and database.\n",
      "Loading data file: KO_30_2025-01.csv\n",
      "Loading meta data file: KO_30_2025-01_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('KO', 60, '2025-01')\n",
      "Extracting data from SQL database\n",
      "Searching for file KO_60_2025-01\n",
      "Loading data file: KO_60_2025-01.csv\n",
      "Loading meta data file: KO_60_2025-01_meta.csv\n",
      "File KO_60_2025-01 exists both in files and database.\n",
      "Loading data file: KO_60_2025-01.csv\n",
      "Loading meta data file: KO_60_2025-01_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('NVDA', 1, '2025-01')\n",
      "Extracting data from SQL database\n",
      "Searching for file NVDA_1_2025-01\n",
      "Loading data file: NVDA_1_2025-01.csv\n",
      "Loading meta data file: NVDA_1_2025-01_meta.csv\n",
      "File NVDA_1_2025-01 exists both in files and database.\n",
      "Loading data file: NVDA_1_2025-01.csv\n",
      "Loading meta data file: NVDA_1_2025-01_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('NVDA', 5, '2025-01')\n",
      "Extracting data from SQL database\n",
      "Searching for file NVDA_5_2025-01\n",
      "Loading data file: NVDA_5_2025-01.csv\n",
      "Loading meta data file: NVDA_5_2025-01_meta.csv\n",
      "File NVDA_5_2025-01 exists both in files and database.\n",
      "Loading data file: NVDA_5_2025-01.csv\n",
      "Loading meta data file: NVDA_5_2025-01_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('NVDA', 15, '2025-01')\n",
      "Extracting data from SQL database\n",
      "Searching for file NVDA_15_2025-01\n",
      "Loading data file: NVDA_15_2025-01.csv\n",
      "Loading meta data file: NVDA_15_2025-01_meta.csv\n",
      "File NVDA_15_2025-01 exists both in files and database.\n",
      "Loading data file: NVDA_15_2025-01.csv\n",
      "Loading meta data file: NVDA_15_2025-01_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('NVDA', 30, '2025-01')\n",
      "Extracting data from SQL database\n",
      "Searching for file NVDA_30_2025-01\n",
      "Loading data file: NVDA_30_2025-01.csv\n",
      "Loading meta data file: NVDA_30_2025-01_meta.csv\n",
      "File NVDA_30_2025-01 exists both in files and database.\n",
      "Loading data file: NVDA_30_2025-01.csv\n",
      "Loading meta data file: NVDA_30_2025-01_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('NVDA', 60, '2025-01')\n",
      "Extracting data from SQL database\n",
      "Searching for file NVDA_60_2025-01\n",
      "Loading data file: NVDA_60_2025-01.csv\n",
      "Loading meta data file: NVDA_60_2025-01_meta.csv\n",
      "File NVDA_60_2025-01 exists both in files and database.\n",
      "Loading data file: NVDA_60_2025-01.csv\n",
      "Loading meta data file: NVDA_60_2025-01_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('GOOG', 1, '2025-01')\n",
      "Extracting data from SQL database\n",
      "Searching for file GOOG_1_2025-01\n",
      "Loading data file: GOOG_1_2025-01.csv\n",
      "Loading meta data file: GOOG_1_2025-01_meta.csv\n",
      "File GOOG_1_2025-01 exists both in files and database.\n",
      "Loading data file: GOOG_1_2025-01.csv\n",
      "Loading meta data file: GOOG_1_2025-01_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('GOOG', 5, '2025-01')\n",
      "Extracting data from SQL database\n",
      "Searching for file GOOG_5_2025-01\n",
      "Loading data file: GOOG_5_2025-01.csv\n",
      "Loading meta data file: GOOG_5_2025-01_meta.csv\n",
      "File GOOG_5_2025-01 exists both in files and database.\n",
      "Loading data file: GOOG_5_2025-01.csv\n",
      "Loading meta data file: GOOG_5_2025-01_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('GOOG', 15, '2025-01')\n",
      "Extracting data from SQL database\n",
      "Searching for file GOOG_15_2025-01\n",
      "Loading data file: GOOG_15_2025-01.csv\n",
      "Loading meta data file: GOOG_15_2025-01_meta.csv\n",
      "File GOOG_15_2025-01 exists both in files and database.\n",
      "Loading data file: GOOG_15_2025-01.csv\n",
      "Loading meta data file: GOOG_15_2025-01_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('GOOG', 30, '2025-01')\n",
      "Extracting data from SQL database\n",
      "Searching for file GOOG_30_2025-01\n",
      "Loading data file: GOOG_30_2025-01.csv\n",
      "Loading meta data file: GOOG_30_2025-01_meta.csv\n",
      "File GOOG_30_2025-01 exists both in files and database.\n",
      "Loading data file: GOOG_30_2025-01.csv\n",
      "Loading meta data file: GOOG_30_2025-01_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('GOOG', 60, '2025-01')\n",
      "Extracting data from SQL database\n",
      "Searching for file GOOG_60_2025-01\n",
      "Loading data file: GOOG_60_2025-01.csv\n",
      "Loading meta data file: GOOG_60_2025-01_meta.csv\n",
      "File GOOG_60_2025-01 exists both in files and database.\n",
      "Loading data file: GOOG_60_2025-01.csv\n",
      "Loading meta data file: GOOG_60_2025-01_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('MSFT', 1, '2025-01')\n",
      "Extracting data from SQL database\n",
      "Searching for file MSFT_1_2025-01\n",
      "Loading data file: MSFT_1_2025-01.csv\n",
      "Loading meta data file: MSFT_1_2025-01_meta.csv\n",
      "File MSFT_1_2025-01 exists both in files and database.\n",
      "Loading data file: MSFT_1_2025-01.csv\n",
      "Loading meta data file: MSFT_1_2025-01_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('MSFT', 5, '2025-01')\n",
      "Extracting data from SQL database\n",
      "Searching for file MSFT_5_2025-01\n",
      "Loading data file: MSFT_5_2025-01.csv\n",
      "Loading meta data file: MSFT_5_2025-01_meta.csv\n",
      "File MSFT_5_2025-01 exists both in files and database.\n",
      "Loading data file: MSFT_5_2025-01.csv\n",
      "Loading meta data file: MSFT_5_2025-01_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('MSFT', 15, '2025-01')\n",
      "Extracting data from SQL database\n",
      "Searching for file MSFT_15_2025-01\n",
      "Loading data file: MSFT_15_2025-01.csv\n",
      "Loading meta data file: MSFT_15_2025-01_meta.csv\n",
      "File MSFT_15_2025-01 exists both in files and database.\n",
      "Loading data file: MSFT_15_2025-01.csv\n",
      "Loading meta data file: MSFT_15_2025-01_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('MSFT', 30, '2025-01')\n",
      "Extracting data from SQL database\n",
      "Searching for file MSFT_30_2025-01\n",
      "Loading data file: MSFT_30_2025-01.csv\n",
      "Loading meta data file: MSFT_30_2025-01_meta.csv\n",
      "File MSFT_30_2025-01 exists both in files and database.\n",
      "Loading data file: MSFT_30_2025-01.csv\n",
      "Loading meta data file: MSFT_30_2025-01_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('MSFT', 60, '2025-01')\n",
      "Extracting data from SQL database\n",
      "Searching for file MSFT_60_2025-01\n",
      "Loading data file: MSFT_60_2025-01.csv\n",
      "Loading meta data file: MSFT_60_2025-01_meta.csv\n",
      "File MSFT_60_2025-01 exists both in files and database.\n",
      "Loading data file: MSFT_60_2025-01.csv\n",
      "Loading meta data file: MSFT_60_2025-01_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('KO', 1, '2025-02')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('KO', 5, '2025-02')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('KO', 15, '2025-02')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('KO', 30, '2025-02')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('KO', 60, '2025-02')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('NVDA', 1, '2025-02')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('NVDA', 5, '2025-02')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('NVDA', 15, '2025-02')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('NVDA', 30, '2025-02')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('NVDA', 60, '2025-02')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('GOOG', 1, '2025-02')\n",
      "Extracting data from SQL database\n",
      "Searching for file GOOG_1_2025-02\n",
      "Loading data file: GOOG_1_2025-02.csv\n",
      "Loading meta data file: GOOG_1_2025-02_meta.csv\n",
      "File GOOG_1_2025-02 exists both in files and database.\n",
      "Loading data file: GOOG_1_2025-02.csv\n",
      "Loading meta data file: GOOG_1_2025-02_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('GOOG', 5, '2025-02')\n",
      "Extracting data from SQL database\n",
      "Searching for file GOOG_5_2025-02\n",
      "Loading data file: GOOG_5_2025-02.csv\n",
      "Loading meta data file: GOOG_5_2025-02_meta.csv\n",
      "File GOOG_5_2025-02 exists both in files and database.\n",
      "Loading data file: GOOG_5_2025-02.csv\n",
      "Loading meta data file: GOOG_5_2025-02_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('GOOG', 15, '2025-02')\n",
      "Extracting data from SQL database\n",
      "Searching for file GOOG_15_2025-02\n",
      "Loading data file: GOOG_15_2025-02.csv\n",
      "Loading meta data file: GOOG_15_2025-02_meta.csv\n",
      "File GOOG_15_2025-02 exists both in files and database.\n",
      "Loading data file: GOOG_15_2025-02.csv\n",
      "Loading meta data file: GOOG_15_2025-02_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('GOOG', 30, '2025-02')\n",
      "Extracting data from SQL database\n",
      "Searching for file GOOG_30_2025-02\n",
      "Loading data file: GOOG_30_2025-02.csv\n",
      "Loading meta data file: GOOG_30_2025-02_meta.csv\n",
      "File GOOG_30_2025-02 exists both in files and database.\n",
      "Loading data file: GOOG_30_2025-02.csv\n",
      "Loading meta data file: GOOG_30_2025-02_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('GOOG', 60, '2025-02')\n",
      "Extracting data from SQL database\n",
      "Searching for file GOOG_60_2025-02\n",
      "Loading data file: GOOG_60_2025-02.csv\n",
      "Loading meta data file: GOOG_60_2025-02_meta.csv\n",
      "File GOOG_60_2025-02 exists both in files and database.\n",
      "Loading data file: GOOG_60_2025-02.csv\n",
      "Loading meta data file: GOOG_60_2025-02_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('MSFT', 1, '2025-02')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('MSFT', 5, '2025-02')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('MSFT', 15, '2025-02')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('MSFT', 30, '2025-02')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('MSFT', 60, '2025-02')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('KO', 1, '2025-03')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('KO', 5, '2025-03')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('KO', 15, '2025-03')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('KO', 30, '2025-03')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('KO', 60, '2025-03')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('NVDA', 1, '2025-03')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('NVDA', 5, '2025-03')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('NVDA', 15, '2025-03')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('NVDA', 30, '2025-03')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('NVDA', 60, '2025-03')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('GOOG', 1, '2025-03')\n",
      "Extracting data from SQL database\n",
      "Searching for file GOOG_1_2025-03\n",
      "Loading data file: GOOG_1_2025-03.csv\n",
      "Loading meta data file: GOOG_1_2025-03_meta.csv\n",
      "File GOOG_1_2025-03 exists both in files and database.\n",
      "Loading data file: GOOG_1_2025-03.csv\n",
      "Loading meta data file: GOOG_1_2025-03_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('GOOG', 5, '2025-03')\n",
      "Extracting data from SQL database\n",
      "Searching for file GOOG_5_2025-03\n",
      "Loading data file: GOOG_5_2025-03.csv\n",
      "Loading meta data file: GOOG_5_2025-03_meta.csv\n",
      "File GOOG_5_2025-03 exists both in files and database.\n",
      "Loading data file: GOOG_5_2025-03.csv\n",
      "Loading meta data file: GOOG_5_2025-03_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('GOOG', 15, '2025-03')\n",
      "Extracting data from SQL database\n",
      "Searching for file GOOG_15_2025-03\n",
      "Loading data file: GOOG_15_2025-03.csv\n",
      "Loading meta data file: GOOG_15_2025-03_meta.csv\n",
      "File GOOG_15_2025-03 exists both in files and database.\n",
      "Loading data file: GOOG_15_2025-03.csv\n",
      "Loading meta data file: GOOG_15_2025-03_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('GOOG', 30, '2025-03')\n",
      "Extracting data from SQL database\n",
      "Searching for file GOOG_30_2025-03\n",
      "Loading data file: GOOG_30_2025-03.csv\n",
      "Loading meta data file: GOOG_30_2025-03_meta.csv\n",
      "File GOOG_30_2025-03 exists both in files and database.\n",
      "Loading data file: GOOG_30_2025-03.csv\n",
      "Loading meta data file: GOOG_30_2025-03_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('GOOG', 60, '2025-03')\n",
      "Extracting data from SQL database\n",
      "Searching for file GOOG_60_2025-03\n",
      "Loading data file: GOOG_60_2025-03.csv\n",
      "Loading meta data file: GOOG_60_2025-03_meta.csv\n",
      "File GOOG_60_2025-03 exists both in files and database.\n",
      "Loading data file: GOOG_60_2025-03.csv\n",
      "Loading meta data file: GOOG_60_2025-03_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('MSFT', 1, '2025-03')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('MSFT', 5, '2025-03')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('MSFT', 15, '2025-03')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('MSFT', 30, '2025-03')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('MSFT', 60, '2025-03')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('KO', 1, '2025-04')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('KO', 5, '2025-04')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('KO', 15, '2025-04')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('KO', 30, '2025-04')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('KO', 60, '2025-04')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('NVDA', 1, '2025-04')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('NVDA', 5, '2025-04')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('NVDA', 15, '2025-04')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('NVDA', 30, '2025-04')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('NVDA', 60, '2025-04')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('GOOG', 1, '2025-04')\n",
      "Extracting data from SQL database\n",
      "Searching for file GOOG_1_2025-04\n",
      "Loading data file: GOOG_1_2025-04.csv\n",
      "Loading meta data file: GOOG_1_2025-04_meta.csv\n",
      "File GOOG_1_2025-04 exists both in files and database.\n",
      "Loading data file: GOOG_1_2025-04.csv\n",
      "Loading meta data file: GOOG_1_2025-04_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('GOOG', 5, '2025-04')\n",
      "Extracting data from SQL database\n",
      "Searching for file GOOG_5_2025-04\n",
      "Loading data file: GOOG_5_2025-04.csv\n",
      "Loading meta data file: GOOG_5_2025-04_meta.csv\n",
      "File GOOG_5_2025-04 exists both in files and database.\n",
      "Loading data file: GOOG_5_2025-04.csv\n",
      "Loading meta data file: GOOG_5_2025-04_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('GOOG', 15, '2025-04')\n",
      "Extracting data from SQL database\n",
      "Searching for file GOOG_15_2025-04\n",
      "Loading data file: GOOG_15_2025-04.csv\n",
      "Loading meta data file: GOOG_15_2025-04_meta.csv\n",
      "File GOOG_15_2025-04 exists both in files and database.\n",
      "Loading data file: GOOG_15_2025-04.csv\n",
      "Loading meta data file: GOOG_15_2025-04_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('GOOG', 30, '2025-04')\n",
      "Extracting data from SQL database\n",
      "Searching for file GOOG_30_2025-04\n",
      "Loading data file: GOOG_30_2025-04.csv\n",
      "Loading meta data file: GOOG_30_2025-04_meta.csv\n",
      "File GOOG_30_2025-04 exists both in files and database.\n",
      "Loading data file: GOOG_30_2025-04.csv\n",
      "Loading meta data file: GOOG_30_2025-04_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('GOOG', 60, '2025-04')\n",
      "Extracting data from SQL database\n",
      "Searching for file GOOG_60_2025-04\n",
      "Loading data file: GOOG_60_2025-04.csv\n",
      "Loading meta data file: GOOG_60_2025-04_meta.csv\n",
      "File GOOG_60_2025-04 exists both in files and database.\n",
      "Loading data file: GOOG_60_2025-04.csv\n",
      "Loading meta data file: GOOG_60_2025-04_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('MSFT', 1, '2025-04')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('MSFT', 5, '2025-04')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('MSFT', 15, '2025-04')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('MSFT', 30, '2025-04')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('MSFT', 60, '2025-04')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('KO', 1, '2025-05')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('KO', 5, '2025-05')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('KO', 15, '2025-05')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('KO', 30, '2025-05')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('KO', 60, '2025-05')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('NVDA', 1, '2025-05')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('NVDA', 5, '2025-05')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('NVDA', 15, '2025-05')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('NVDA', 30, '2025-05')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('NVDA', 60, '2025-05')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('GOOG', 1, '2025-05')\n",
      "Extracting data from SQL database\n",
      "Searching for file GOOG_1_2025-05\n",
      "Loading data file: GOOG_1_2025-05.csv\n",
      "Loading meta data file: GOOG_1_2025-05_meta.csv\n",
      "File GOOG_1_2025-05 exists both in files and database.\n",
      "Loading data file: GOOG_1_2025-05.csv\n",
      "Loading meta data file: GOOG_1_2025-05_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('GOOG', 5, '2025-05')\n",
      "Extracting data from SQL database\n",
      "Searching for file GOOG_5_2025-05\n",
      "Loading data file: GOOG_5_2025-05.csv\n",
      "Loading meta data file: GOOG_5_2025-05_meta.csv\n",
      "File GOOG_5_2025-05 exists both in files and database.\n",
      "Loading data file: GOOG_5_2025-05.csv\n",
      "Loading meta data file: GOOG_5_2025-05_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('GOOG', 15, '2025-05')\n",
      "Extracting data from SQL database\n",
      "Searching for file GOOG_15_2025-05\n",
      "Loading data file: GOOG_15_2025-05.csv\n",
      "Loading meta data file: GOOG_15_2025-05_meta.csv\n",
      "File GOOG_15_2025-05 exists both in files and database.\n",
      "Loading data file: GOOG_15_2025-05.csv\n",
      "Loading meta data file: GOOG_15_2025-05_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('GOOG', 30, '2025-05')\n",
      "Extracting data from SQL database\n",
      "Searching for file GOOG_30_2025-05\n",
      "Loading data file: GOOG_30_2025-05.csv\n",
      "Loading meta data file: GOOG_30_2025-05_meta.csv\n",
      "File GOOG_30_2025-05 exists both in files and database.\n",
      "Loading data file: GOOG_30_2025-05.csv\n",
      "Loading meta data file: GOOG_30_2025-05_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('GOOG', 60, '2025-05')\n",
      "Extracting data from SQL database\n",
      "Searching for file GOOG_60_2025-05\n",
      "Loading data file: GOOG_60_2025-05.csv\n",
      "Loading meta data file: GOOG_60_2025-05_meta.csv\n",
      "File GOOG_60_2025-05 exists both in files and database.\n",
      "Loading data file: GOOG_60_2025-05.csv\n",
      "Loading meta data file: GOOG_60_2025-05_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('MSFT', 1, '2025-05')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('MSFT', 5, '2025-05')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('MSFT', 15, '2025-05')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('MSFT', 30, '2025-05')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('MSFT', 60, '2025-05')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('KO', 1, '2025-06')\n",
      "Extracting data from SQL database\n",
      "Searching for file KO_1_2025-06\n",
      "Loading data file: KO_1_2025-06.csv\n",
      "Loading meta data file: KO_1_2025-06_meta.csv\n",
      "File KO_1_2025-06 exists both in files and database.\n",
      "Loading data file: KO_1_2025-06.csv\n",
      "Loading meta data file: KO_1_2025-06_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('KO', 5, '2025-06')\n",
      "Extracting data from SQL database\n",
      "Searching for file KO_5_2025-06\n",
      "Loading data file: KO_5_2025-06.csv\n",
      "Loading meta data file: KO_5_2025-06_meta.csv\n",
      "File KO_5_2025-06 exists both in files and database.\n",
      "Loading data file: KO_5_2025-06.csv\n",
      "Loading meta data file: KO_5_2025-06_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('KO', 15, '2025-06')\n",
      "Extracting data from SQL database\n",
      "Searching for file KO_15_2025-06\n",
      "Loading data file: KO_15_2025-06.csv\n",
      "Loading meta data file: KO_15_2025-06_meta.csv\n",
      "File KO_15_2025-06 exists both in files and database.\n",
      "Loading data file: KO_15_2025-06.csv\n",
      "Loading meta data file: KO_15_2025-06_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('KO', 30, '2025-06')\n",
      "Extracting data from SQL database\n",
      "Searching for file KO_30_2025-06\n",
      "Loading data file: KO_30_2025-06.csv\n",
      "Loading meta data file: KO_30_2025-06_meta.csv\n",
      "File KO_30_2025-06 exists both in files and database.\n",
      "Loading data file: KO_30_2025-06.csv\n",
      "Loading meta data file: KO_30_2025-06_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('KO', 60, '2025-06')\n",
      "Extracting data from SQL database\n",
      "Searching for file KO_60_2025-06\n",
      "Loading data file: KO_60_2025-06.csv\n",
      "Loading meta data file: KO_60_2025-06_meta.csv\n",
      "File KO_60_2025-06 exists both in files and database.\n",
      "Loading data file: KO_60_2025-06.csv\n",
      "Loading meta data file: KO_60_2025-06_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('NVDA', 1, '2025-06')\n",
      "Extracting data from SQL database\n",
      "Searching for file NVDA_1_2025-06\n",
      "Loading data file: NVDA_1_2025-06.csv\n",
      "Loading meta data file: NVDA_1_2025-06_meta.csv\n",
      "File NVDA_1_2025-06 exists both in files and database.\n",
      "Loading data file: NVDA_1_2025-06.csv\n",
      "Loading meta data file: NVDA_1_2025-06_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('NVDA', 5, '2025-06')\n",
      "Extracting data from SQL database\n",
      "Searching for file NVDA_5_2025-06\n",
      "Loading data file: NVDA_5_2025-06.csv\n",
      "Loading meta data file: NVDA_5_2025-06_meta.csv\n",
      "File NVDA_5_2025-06 exists both in files and database.\n",
      "Loading data file: NVDA_5_2025-06.csv\n",
      "Loading meta data file: NVDA_5_2025-06_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('NVDA', 15, '2025-06')\n",
      "Extracting data from SQL database\n",
      "Searching for file NVDA_15_2025-06\n",
      "Loading data file: NVDA_15_2025-06.csv\n",
      "Loading meta data file: NVDA_15_2025-06_meta.csv\n",
      "File NVDA_15_2025-06 exists both in files and database.\n",
      "Loading data file: NVDA_15_2025-06.csv\n",
      "Loading meta data file: NVDA_15_2025-06_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('NVDA', 30, '2025-06')\n",
      "Extracting data from SQL database\n",
      "Searching for file NVDA_30_2025-06\n",
      "Loading data file: NVDA_30_2025-06.csv\n",
      "Loading meta data file: NVDA_30_2025-06_meta.csv\n",
      "File NVDA_30_2025-06 exists both in files and database.\n",
      "Loading data file: NVDA_30_2025-06.csv\n",
      "Loading meta data file: NVDA_30_2025-06_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('NVDA', 60, '2025-06')\n",
      "Extracting data from SQL database\n",
      "Searching for file NVDA_60_2025-06\n",
      "Loading data file: NVDA_60_2025-06.csv\n",
      "Loading meta data file: NVDA_60_2025-06_meta.csv\n",
      "File NVDA_60_2025-06 exists both in files and database.\n",
      "Loading data file: NVDA_60_2025-06.csv\n",
      "Loading meta data file: NVDA_60_2025-06_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('GOOG', 1, '2025-06')\n",
      "Extracting data from SQL database\n",
      "Searching for file GOOG_1_2025-06\n",
      "Loading data file: GOOG_1_2025-06.csv\n",
      "Loading meta data file: GOOG_1_2025-06_meta.csv\n",
      "File GOOG_1_2025-06 exists both in files and database.\n",
      "Loading data file: GOOG_1_2025-06.csv\n",
      "Loading meta data file: GOOG_1_2025-06_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('GOOG', 5, '2025-06')\n",
      "Extracting data from SQL database\n",
      "Searching for file GOOG_5_2025-06\n",
      "Loading data file: GOOG_5_2025-06.csv\n",
      "Loading meta data file: GOOG_5_2025-06_meta.csv\n",
      "File GOOG_5_2025-06 exists both in files and database.\n",
      "Loading data file: GOOG_5_2025-06.csv\n",
      "Loading meta data file: GOOG_5_2025-06_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('GOOG', 15, '2025-06')\n",
      "Extracting data from SQL database\n",
      "Searching for file GOOG_15_2025-06\n",
      "Loading data file: GOOG_15_2025-06.csv\n",
      "Loading meta data file: GOOG_15_2025-06_meta.csv\n",
      "File GOOG_15_2025-06 exists both in files and database.\n",
      "Loading data file: GOOG_15_2025-06.csv\n",
      "Loading meta data file: GOOG_15_2025-06_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('GOOG', 30, '2025-06')\n",
      "Extracting data from SQL database\n",
      "Searching for file GOOG_30_2025-06\n",
      "Loading data file: GOOG_30_2025-06.csv\n",
      "Loading meta data file: GOOG_30_2025-06_meta.csv\n",
      "File GOOG_30_2025-06 exists both in files and database.\n",
      "Loading data file: GOOG_30_2025-06.csv\n",
      "Loading meta data file: GOOG_30_2025-06_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('GOOG', 60, '2025-06')\n",
      "Extracting data from SQL database\n",
      "Searching for file GOOG_60_2025-06\n",
      "Loading data file: GOOG_60_2025-06.csv\n",
      "Loading meta data file: GOOG_60_2025-06_meta.csv\n",
      "File GOOG_60_2025-06 exists both in files and database.\n",
      "Loading data file: GOOG_60_2025-06.csv\n",
      "Loading meta data file: GOOG_60_2025-06_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('MSFT', 1, '2025-06')\n",
      "Extracting data from SQL database\n",
      "Searching for file MSFT_1_2025-06\n",
      "Loading data file: MSFT_1_2025-06.csv\n",
      "Loading meta data file: MSFT_1_2025-06_meta.csv\n",
      "File MSFT_1_2025-06 exists both in files and database.\n",
      "Loading data file: MSFT_1_2025-06.csv\n",
      "Loading meta data file: MSFT_1_2025-06_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('MSFT', 5, '2025-06')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('MSFT', 15, '2025-06')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('MSFT', 30, '2025-06')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('MSFT', 60, '2025-06')\n",
      "Extracting data from SQL database\n",
      "This dataset does not exist in the database.\n",
      "Checking data unit -> Ticker, Interval, Month: ('KO', 1, '2025-07')\n",
      "Extracting data from SQL database\n",
      "Searching for file KO_1_2025-07\n",
      "Loading data file: KO_1_2025-07.csv\n",
      "Loading meta data file: KO_1_2025-07_meta.csv\n",
      "File KO_1_2025-07 exists both in files and database.\n",
      "Loading data file: KO_1_2025-07.csv\n",
      "Loading meta data file: KO_1_2025-07_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('KO', 5, '2025-07')\n",
      "Extracting data from SQL database\n",
      "Searching for file KO_5_2025-07\n",
      "Loading data file: KO_5_2025-07.csv\n",
      "Loading meta data file: KO_5_2025-07_meta.csv\n",
      "File KO_5_2025-07 exists both in files and database.\n",
      "Loading data file: KO_5_2025-07.csv\n",
      "Loading meta data file: KO_5_2025-07_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('KO', 15, '2025-07')\n",
      "Extracting data from SQL database\n",
      "Searching for file KO_15_2025-07\n",
      "Loading data file: KO_15_2025-07.csv\n",
      "Loading meta data file: KO_15_2025-07_meta.csv\n",
      "File KO_15_2025-07 exists both in files and database.\n",
      "Loading data file: KO_15_2025-07.csv\n",
      "Loading meta data file: KO_15_2025-07_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('KO', 30, '2025-07')\n",
      "Extracting data from SQL database\n",
      "Searching for file KO_30_2025-07\n",
      "Loading data file: KO_30_2025-07.csv\n",
      "Loading meta data file: KO_30_2025-07_meta.csv\n",
      "File KO_30_2025-07 exists both in files and database.\n",
      "Loading data file: KO_30_2025-07.csv\n",
      "Loading meta data file: KO_30_2025-07_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('KO', 60, '2025-07')\n",
      "Extracting data from SQL database\n",
      "Searching for file KO_60_2025-07\n",
      "Loading data file: KO_60_2025-07.csv\n",
      "Loading meta data file: KO_60_2025-07_meta.csv\n",
      "File KO_60_2025-07 exists both in files and database.\n",
      "Loading data file: KO_60_2025-07.csv\n",
      "Loading meta data file: KO_60_2025-07_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('NVDA', 1, '2025-07')\n",
      "Extracting data from SQL database\n",
      "Searching for file NVDA_1_2025-07\n",
      "Loading data file: NVDA_1_2025-07.csv\n",
      "Loading meta data file: NVDA_1_2025-07_meta.csv\n",
      "File NVDA_1_2025-07 exists both in files and database.\n",
      "Loading data file: NVDA_1_2025-07.csv\n",
      "Loading meta data file: NVDA_1_2025-07_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('NVDA', 5, '2025-07')\n",
      "Extracting data from SQL database\n",
      "Searching for file NVDA_5_2025-07\n",
      "Loading data file: NVDA_5_2025-07.csv\n",
      "Loading meta data file: NVDA_5_2025-07_meta.csv\n",
      "File NVDA_5_2025-07 exists both in files and database.\n",
      "Loading data file: NVDA_5_2025-07.csv\n",
      "Loading meta data file: NVDA_5_2025-07_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('NVDA', 15, '2025-07')\n",
      "Extracting data from SQL database\n",
      "Searching for file NVDA_15_2025-07\n",
      "Loading data file: NVDA_15_2025-07.csv\n",
      "Loading meta data file: NVDA_15_2025-07_meta.csv\n",
      "File NVDA_15_2025-07 exists both in files and database.\n",
      "Loading data file: NVDA_15_2025-07.csv\n",
      "Loading meta data file: NVDA_15_2025-07_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('NVDA', 30, '2025-07')\n",
      "Extracting data from SQL database\n",
      "Searching for file NVDA_30_2025-07\n",
      "Loading data file: NVDA_30_2025-07.csv\n",
      "Loading meta data file: NVDA_30_2025-07_meta.csv\n",
      "File NVDA_30_2025-07 exists both in files and database.\n",
      "Loading data file: NVDA_30_2025-07.csv\n",
      "Loading meta data file: NVDA_30_2025-07_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('NVDA', 60, '2025-07')\n",
      "Extracting data from SQL database\n",
      "Searching for file NVDA_60_2025-07\n",
      "Loading data file: NVDA_60_2025-07.csv\n",
      "Loading meta data file: NVDA_60_2025-07_meta.csv\n",
      "File NVDA_60_2025-07 exists both in files and database.\n",
      "Loading data file: NVDA_60_2025-07.csv\n",
      "Loading meta data file: NVDA_60_2025-07_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('GOOG', 1, '2025-07')\n",
      "Extracting data from SQL database\n",
      "Searching for file GOOG_1_2025-07\n",
      "Loading data file: GOOG_1_2025-07.csv\n",
      "Loading meta data file: GOOG_1_2025-07_meta.csv\n",
      "File GOOG_1_2025-07 exists both in files and database.\n",
      "Loading data file: GOOG_1_2025-07.csv\n",
      "Loading meta data file: GOOG_1_2025-07_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('GOOG', 5, '2025-07')\n",
      "Extracting data from SQL database\n",
      "Searching for file GOOG_5_2025-07\n",
      "Loading data file: GOOG_5_2025-07.csv\n",
      "Loading meta data file: GOOG_5_2025-07_meta.csv\n",
      "File GOOG_5_2025-07 exists both in files and database.\n",
      "Loading data file: GOOG_5_2025-07.csv\n",
      "Loading meta data file: GOOG_5_2025-07_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('GOOG', 15, '2025-07')\n",
      "Extracting data from SQL database\n",
      "Searching for file GOOG_15_2025-07\n",
      "Loading data file: GOOG_15_2025-07.csv\n",
      "Loading meta data file: GOOG_15_2025-07_meta.csv\n",
      "File GOOG_15_2025-07 exists both in files and database.\n",
      "Loading data file: GOOG_15_2025-07.csv\n",
      "Loading meta data file: GOOG_15_2025-07_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('GOOG', 30, '2025-07')\n",
      "Extracting data from SQL database\n",
      "Searching for file GOOG_30_2025-07\n",
      "Loading data file: GOOG_30_2025-07.csv\n",
      "Loading meta data file: GOOG_30_2025-07_meta.csv\n",
      "File GOOG_30_2025-07 exists both in files and database.\n",
      "Loading data file: GOOG_30_2025-07.csv\n",
      "Loading meta data file: GOOG_30_2025-07_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('GOOG', 60, '2025-07')\n",
      "Extracting data from SQL database\n",
      "Searching for file GOOG_60_2025-07\n",
      "Loading data file: GOOG_60_2025-07.csv\n",
      "Loading meta data file: GOOG_60_2025-07_meta.csv\n",
      "File GOOG_60_2025-07 exists both in files and database.\n",
      "Loading data file: GOOG_60_2025-07.csv\n",
      "Loading meta data file: GOOG_60_2025-07_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('MSFT', 1, '2025-07')\n",
      "Extracting data from SQL database\n",
      "Searching for file MSFT_1_2025-07\n",
      "Loading data file: MSFT_1_2025-07.csv\n",
      "Loading meta data file: MSFT_1_2025-07_meta.csv\n",
      "File MSFT_1_2025-07 exists both in files and database.\n",
      "Loading data file: MSFT_1_2025-07.csv\n",
      "Loading meta data file: MSFT_1_2025-07_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('MSFT', 5, '2025-07')\n",
      "Extracting data from SQL database\n",
      "Searching for file MSFT_5_2025-07\n",
      "Loading data file: MSFT_5_2025-07.csv\n",
      "Loading meta data file: MSFT_5_2025-07_meta.csv\n",
      "File MSFT_5_2025-07 exists both in files and database.\n",
      "Loading data file: MSFT_5_2025-07.csv\n",
      "Loading meta data file: MSFT_5_2025-07_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('MSFT', 15, '2025-07')\n",
      "Extracting data from SQL database\n",
      "Searching for file MSFT_15_2025-07\n",
      "Loading data file: MSFT_15_2025-07.csv\n",
      "Loading meta data file: MSFT_15_2025-07_meta.csv\n",
      "File MSFT_15_2025-07 exists both in files and database.\n",
      "Loading data file: MSFT_15_2025-07.csv\n",
      "Loading meta data file: MSFT_15_2025-07_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('MSFT', 30, '2025-07')\n",
      "Extracting data from SQL database\n",
      "Searching for file MSFT_30_2025-07\n",
      "Loading data file: MSFT_30_2025-07.csv\n",
      "Loading meta data file: MSFT_30_2025-07_meta.csv\n",
      "File MSFT_30_2025-07 exists both in files and database.\n",
      "Loading data file: MSFT_30_2025-07.csv\n",
      "Loading meta data file: MSFT_30_2025-07_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Checking data unit -> Ticker, Interval, Month: ('MSFT', 60, '2025-07')\n",
      "Extracting data from SQL database\n",
      "Searching for file MSFT_60_2025-07\n",
      "Loading data file: MSFT_60_2025-07.csv\n",
      "Loading meta data file: MSFT_60_2025-07_meta.csv\n",
      "File MSFT_60_2025-07 exists both in files and database.\n",
      "Loading data file: MSFT_60_2025-07.csv\n",
      "Loading meta data file: MSFT_60_2025-07_meta.csv\n",
      "No errors in SQL/file matching post-save\n",
      "Saved 0/140 data units from database to files.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: MANUAL WAY TO CHECK FILE COUNT MATCH BETWEEN DATABASE/FILES (FILE COUNT SHOULD BE TWICE THE NUMBER OF METADATA ROWS)\n",
    "# TODO: CONVERT THIS INTO A CROSS CHECKER FUNCTION LATER\n",
    "\n",
    "# HERE MAKE CODE TO TAKE DATA FROM SQL AND SAVE INTO .CSV\n",
    "checktickers = [\"KO\",'NVDA', 'GOOG', \"MSFT\"]\n",
    "checkintervals = [1,5,15,30,60]\n",
    "checkmonths = ['2025-01','2025-02','2025-03','2025-04','2025-05','2025-06','2025-07']\n",
    "echo = True\n",
    "saved = 0\n",
    "total = len(checktickers)*len(checkintervals)*len(checkmonths)\n",
    "for month in checkmonths:\n",
    "    for ticker in checktickers:\n",
    "        for interval in checkintervals:\n",
    "            print(f'Checking data unit -> Ticker, Interval, Month: {ticker, interval, month}')\n",
    "            stockDF, metaDF = checkManifest.loadData_fromsql(ticker,interval,month,meta=True) # Automatically post-processed\n",
    "            if (not metaDF.empty) and (not stockDF.empty): # If both not empty, then both data exists on database (check both to be secure against errors)\n",
    "                # Check if (direct) file exists (if not, save into as it)\n",
    "                try:\n",
    "                    fileString = r'' + ticker + \"_\" + str(interval) + \"_\" + month\n",
    "                    if echo: print('Searching for file ' + fileString)\n",
    "                    \n",
    "                    fileRead, fileMetaRead = checkManifest.loadData_fromcsv(ticker, interval, month, meta=True, echo=echo)\n",
    "                except FileNotFoundError: # If any of the two files don't exist, save data from SQL into csv\n",
    "                    print('File ' + fileString + ' not found, saving from SQL database to files.')\n",
    "                    saved += 1\n",
    "                    stockDF.to_csv(rf\"{savepath}{ticker}/{ticker}_{interval}_{month}.csv\",index=False)\n",
    "                    metaDF.to_csv(rf\"{savepath}{ticker}/{ticker}_{interval}_{month}_meta.csv\",index=True)\n",
    "                else:\n",
    "                    if echo: print('File ' + fileString + ' exists both in files and database.')\n",
    "                \n",
    "                fileRead, fileMetaRead = checkManifest.loadData_fromcsv(ticker, interval, month, meta=True, echo=echo)\n",
    "                if stockDF.equals(fileRead) and metaDF.equals(fileMetaRead):\n",
    "                    print('No errors in SQL/file matching post-save')\n",
    "            else:\n",
    "                print('This dataset does not exist in the database.')\n",
    "                ### TODO: CAN CHECK IF FILE EXISTS, SYNC INTO DATABASE (TO DEVELOP LATER, FOCUS ON FINISHING INFRA FIRST) \n",
    "print(f'Saved {saved}/{total} data units from database to files.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fdb622-1758-4fbb-9354-9ed17cfbcdbd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pytest\n",
    "from python_on_whales import docker\n",
    "import time\n",
    "\n",
    "\n",
    "# Run a container\n",
    "#docker.run(\"hello-world\")\n",
    "\n",
    "# Pull an image\n",
    "#docker.pull(\"postgres:15\")\n",
    "\n",
    "\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def test_postgres():\n",
    "    container = docker.run(\n",
    "        \"postgres:15\",\n",
    "        detach=True,\n",
    "        name=\"test_pg\",\n",
    "        envs={\"POSTGRES_PASSWORD\": \"test\", \"POSTGRES_USER\": \"test\", \"POSTGRES_DB\": \"testdb\"},\n",
    "        publish=[\"5433:5432\"]\n",
    "    )\n",
    "    time.sleep(3)  # wait for the DB to boot\n",
    "\n",
    "    yield \"postgresql://test:test@localhost:5433/testdb\"\n",
    "\n",
    "    docker.container.remove(\"test_pg\", force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c77da0-37fc-4441-8a12-00d6acf59eef",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#%%writefile DockerInstallWrapper.py\n",
    "# Docker Installation Process\n",
    "import os\n",
    "\n",
    "def install_docker():\n",
    "    import subprocess\n",
    "    import shutil\n",
    "\n",
    "    docker_path = shutil.which(\"docker\")\n",
    "    if docker_path:\n",
    "        print(f\"Docker already installed, located at: {docker_path}\")\n",
    "        return\n",
    "    else:\n",
    "        print(\"Docker not found in PATH\")\n",
    "\n",
    "    # Windows installation procedure\n",
    "    if os.name == \"nt\":\n",
    "        dockerInstaller_path = shutil.which(\"dockerinstaller\")    \n",
    "        if dockerInstaller_path:\n",
    "            print(f\"Docker installer found at: {dockerInstaller_path}\")\n",
    "        else:\n",
    "            print(\"Downloading docker installer...\")\n",
    "            subprocess.run([\"powershell\", \"-Command\", \"Invoke-WebRequest -Uri 'https://desktop.docker.com/win/main/amd64/Docker Desktop Installer.exe' -OutFile 'DockerInstaller.exe'\"], shell=True)\n",
    "        \n",
    "        quietChoice = input(\"Do you want to install quietly (without GUI, automatically)? (y/n): \").strip().lower()\n",
    "        if quietChoice == 'y':\n",
    "            print(\"Installing docker quietly...\")\n",
    "            subprocess.run([\"powershell\", \"-Command\", 'Start-Process -FilePath \"DockerInstaller.exe\" -ArgumentList \"install --quiet\" -Verb RunAs -Wait'], shell=True)\n",
    "        else:\n",
    "            print(\"Installing docker...\")\n",
    "            subprocess.run([\"powershell\", \"-Command\", 'Start-Process -FilePath \"DockerInstaller.exe\" -ArgumentList \"install\" -Verb RunAs -Wait'], shell=True)\n",
    "\n",
    "    # Linux/macOS installation procedure\n",
    "    elif os.name == \"posix\":\n",
    "        print(\"Installing docker...\")\n",
    "        subprocess.run(\"curl -fsSL https://get.docker.com | sh\", shell=True)\n",
    "        subprocess.run(\"sudo usermod -aG docker $USER\", shell=True)\n",
    "\n",
    "    prompt_restart()\n",
    "\n",
    "def prompt_restart():\n",
    "    choice = input(\"Docker installation complete. Do you want to restart now? (y/n): \").strip().lower()\n",
    "    if choice == 'y':\n",
    "        force_restart()\n",
    "    else:\n",
    "        print(\"Restart skipped. You may need to restart manually for changes to take full effect.\")\n",
    "\n",
    "def force_restart():\n",
    "    print(\"Restarting device.\")\n",
    "    if os.name == \"nt\":  # Windows\n",
    "        subprocess.run([\"powershell\", \"Restart-Computer -Force\"], shell=True)\n",
    "    elif os.name == \"posix\":  # Linux/macOS\n",
    "        subprocess.run(\"sudo shutdown -r now\", shell=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    installChoice = input(\"Do you want to install Docker to gain access to SQL functionality? (y/n): \").strip().lower()\n",
    "    \n",
    "    if installChoice == 'y':\n",
    "        install_docker()\n",
    "    else:\n",
    "        print(\"Install skipped. You need to install docker to be able to use SQL functionality.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacd1cf1-236e-44db-84c7-385f1d7b27e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataManifest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m savepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/StockHistData/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      5\u001b[0m SQLloginfilename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSQLlogin\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# Default filename \"SQLlogin\" \u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m checkManifest \u001b[38;5;241m=\u001b[39m DataManifest()\n\u001b[0;32m      8\u001b[0m checkManifest\u001b[38;5;241m.\u001b[39mloadManifest(loadFrom \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdirect\u001b[39m\u001b[38;5;124m'\u001b[39m, path \u001b[38;5;241m=\u001b[39m savepath)\n\u001b[0;32m      9\u001b[0m checkManifest\u001b[38;5;241m.\u001b[39mvalidateManifest()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DataManifest' is not defined"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import pandas as pd\n",
    "from arcanequant.quantlib import *\n",
    "\n",
    "# Testing manifest saving\n",
    "savepath = r'data/StockHistData/'\n",
    "SQLloginfilename = \"SQLlogin\" # Default filename \"SQLlogin\" \n",
    "\n",
    "checkManifest = DataManifest()\n",
    "checkManifest.connectSQL(SQLloginfilename)\n",
    "checkManifest.loadManifest(loadFrom = 'direct', path = savepath)\n",
    "#checkManifest.validateManifest()\n",
    "#checkManifest.saveManifest()\n",
    "\n",
    "# Saving market data from csv needs tagging of Ticker and Interval\n",
    "#SQLEstablish(checkManifest.SQLengine)\n",
    "#SQLRepair(dataManifest = checkManifest)\n",
    "\n",
    "t0 = time()\n",
    "# Testing manifest loading\n",
    "print('manifest/DM')\n",
    "zz = ExtractData(checkManifest,  'manifest',  start = 'all', end = '2022-11', fromSQL = False, condition = None)\n",
    "print(zz)\n",
    "t1 = time()\n",
    "print(f\"Total time elapsed : {t1-t0} seconds\")\n",
    "\n",
    "print('manifest/SQL')\n",
    "#zz2 = ExtractData(checkManifest, 'manifest',  start = '2000-01', end = '2000-05', fromSQL = True, condition = None)\n",
    "#print(zz2)\n",
    "t2 = time()\n",
    "print(f\"Total time elapsed : {t2-t1} seconds\")\n",
    "\n",
    "print('market/DM')\n",
    "# Testing stock data loading\n",
    "#xx = ExtractData(checkManifest, 'market',  start = 'all', end = '2022-11', fromSQL = False, condition = None)\n",
    "#print(xx)\n",
    "t3 = time()\n",
    "print(f\"Total time elapsed : {t3-t2} seconds\")\n",
    "\n",
    "print('manifest/SQL')\n",
    "#yy = ExtractData(checkManifest, 'market',  start = 'all', end = '2022-11', fromSQL = True, condition = None)\n",
    "#print(yy)\n",
    "t4 = time()\n",
    "print(f\"Total time elapsed : {t4-t3} seconds\")\n",
    "\n",
    "# Testing stock data saving\n",
    "#SQLSave(xx, checkManifest.SQLengine, 'marketTable', echo = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1680e275",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2734c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we read and post-process the .csv data and do a time-series analysis (rolling mean and std. deviation)\n",
    "# Intention is to figure out a good window size by assessing the variance and if it follows a chi-sq. distribution\n",
    "# The assumption here is the stock values are normally distributed\n",
    "# The issue is the stock mean/variance may change over time, how do I account for this?\n",
    "\n",
    "#### TO DO:\n",
    "# We are supposed to assess the % change of stock values\n",
    "# I will calculate this, and also its variance, I will do this for different resolutions and in a rolling window (I am thinking of sizes, 5, 10, 20, 50, 100)\n",
    "# Should I calculate a global (or yearly or monthly) variance and compare this to the window to see if theres a match for specific sizes (which correspond to same timeframe)?\n",
    "# Once I do this I can really go into the trade models I made previously\n",
    "\n",
    "##### THOUGHTS:\n",
    "# Is there a point in hypothesis testing a 'true' variance or mean for growth stocks? perhaps other than to test for a non-constant mean model?\n",
    "# There is point in hypothesis testing dividend stocks/ETFs (i.e. FTSE 100 Vanguard ETF)\n",
    "# Is it reasonable to model the mean (or exp. stock value) as fct of time in context of a fourier transform\n",
    "# (as many growth+dividend stocks have cyclical behaviour around quarterlies, typically goes up then down later) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_MSFT_test = checkManifest.loadData_fromcsv(\"MSFT\", 15, \"2024-08\")\n",
    "data_MSFT_test.set_index(pd.to_datetime(data_MSFT_test['DateTime']), inplace=True)\n",
    "\n",
    "\n",
    "############\n",
    "############\n",
    "# Modelling prices:\n",
    "# Get nominal price for each time, use to get percent change from last instance\n",
    "# Convert to histogram/empirical distribution and model the distribution\n",
    "\n",
    "# TO DO:\n",
    "# OBTAIN HISTOGRAM\n",
    "# I EXPECT OUTLIERS TO PROVIDE INFORMATION (PERHAPS W.R.T. INTER-DAY PRICE OR HIGHER VOLATILITY AT TRADING DAY START/END)\n",
    "\n",
    "# THE BIN RESOLUTION FOR HISTOGRAMS ARE IMPORTANT IN ASSESSING HOW CLOSELY THE DISTRIBUTION MATCHES THE DATA, MAYBE.\n",
    "# IF TOO COARSE, THE BIN CENTRE WILL BE TOO HIGH, OTHERWISE IT WILL BE TOO FLAT (AND UNEVEN)\n",
    "# IS THE BIN REPRESENTATION A BETTER ASSESSMENT OF THE RESIDUAL OF THE DATA <-> DISTRIBUTION FIT, OR CAN THE RAW DATA BE COMPARED DIRECTLY TO THE DISTRIBUTION?\n",
    "# THE LATTER SOUNDS LIKE IT DOESNT MAKE SENSE AS THE PDF IS TO DO WITH \"PROBABILITY DENSITY\" SO WE MUST USE BINS TO COLLECT REGIONS IN THE PRICE (X) DOMAIN\n",
    "# IF SO I MUST STUDY WHAT BIN SIZE IS OPTIMAL FOR RESIDUAL, IF THE OPTIMAL BIN SIZE CHANGES FOR DIFFERENT DISTRIBUTIONS (PROBABLY YES) OR FOR OUTLIER EFFECTS (ALSO YES)\n",
    "\n",
    "# TO CONSIDER:\n",
    "# MODELLING EACH TIME INSTANCE WITH ITS OWN LIMITED-DOMAIN DISTRIBUTION BASED ON MAX-MIN PRICE AND VOLUME\n",
    "# PRESUME EACH TRADE IS A RANDOM VARIABLE FOLLOWING (WHICH??) DISTRIBUTION\n",
    "# THEREFORE THE COLLECTIVE VOLUME FOLLOWS (WHICH? T-DISTRI.?) DISTRIBUTION\n",
    "\n",
    "# Obtain nominal price as mean of high and low prices (as we have no idea where the mean trade of each time instance may be so our most 'accurate' is probably the center of the limits\n",
    "# Note: can possibly assume a finite domain bell distribution for the nominal value\n",
    "data_MSFT_test['Nominal'] = (data_MSFT_test['High'] + data_MSFT_test['Low']) /2\n",
    "data_MSFT_test['PctChange_N'] = data_MSFT_test['Nominal'].diff()/data_MSFT_test['Nominal']*100\n",
    "#data_MSFT_test['PctChange_H'] = data_MSFT_test['High'].diff()/data_MSFT_test['High']*100\n",
    "#data_MSFT_test['PctChange_L'] = data_MSFT_test['Low'].diff()/data_MSFT_test['Low']*100\n",
    "\n",
    "\n",
    "# Bin count for histogram\n",
    "bincount_list = [5,10,20,40,80,160,320] # Note if too high, worse bell-curve representation\n",
    "# Interval to exclude outliers from\n",
    "exclusion_int = 0.01\n",
    "# Exclude outlier (in display)\n",
    "no_outliers = True\n",
    "\n",
    "# [min, max] ranges of the histogram\n",
    "rangemin = data_MSFT_test['PctChange_N'].min()\n",
    "rangemax = data_MSFT_test['PctChange_N'].max()\n",
    "# Exclusive interval range for histogram (excludes outliers)\n",
    "excl_min = data_MSFT_test['PctChange_N'].quantile(exclusion_int)\n",
    "excl_max = data_MSFT_test['PctChange_N'].quantile(1-exclusion_int)\n",
    "\n",
    "# Outlier data filtering\n",
    "data_filtered = data_MSFT_test[(data_MSFT_test['PctChange_N'] < excl_max) & (data_MSFT_test['PctChange_N'] > excl_min)]\n",
    "\n",
    "xmin = None\n",
    "xmax = None\n",
    "if no_outliers:\n",
    "    xmin = excl_min\n",
    "    xmax = excl_max\n",
    "else:\n",
    "    xmin = rangemin\n",
    "    xmax = rangemax\n",
    "\n",
    "# Bin sequence list for dataset histogram (for the range of bincounts)\n",
    "binseq = []\n",
    "for bincount in bincount_list:\n",
    "    binseq += [[xmin + i*(xmax-xmin)/bincount for i in range(bincount+1)]] # Can truncate for efficiency\n",
    "\n",
    "# CREATE MODEL TO FIT/SHOW ON PLOT AND ASSESS ACCURACY\n",
    "# CREATE CURVE AND EVALUATE CURVE VALUE ON HIST LOCATION, GET ITS RESIDUAL\n",
    "# JUST USE STATS TO FIND WHAT RESIDUAL WOULD BE FOR CURVES GIVEN DATAPOINTS\n",
    "# TEST CONFIDENCE INTERVALS?\n",
    "# WHAT ABOUT KERNEL DENSITY ESTIMATION?\n",
    "\n",
    "# Distribution testing\n",
    "mean_out = data_MSFT_test['PctChange_N'].mean()\n",
    "std_out = data_MSFT_test['PctChange_N'].std()\n",
    "mean_nout = data_filtered['PctChange_N'].mean()\n",
    "std_nout = data_filtered['PctChange_N'].std()\n",
    "\n",
    "pdf = stats.norm.pdf(data_MSFT_test['PctChange_N'].sort_values(), mean_out, std_out) # Values sorted as iterated through index\n",
    "pdf2 = stats.norm.pdf(data_MSFT_test['PctChange_N'].sort_values(), mean_nout, std_nout) # Values sorted as iterated through index\n",
    "plt.plot(data_MSFT_test['PctChange_N'].sort_values(), pdf, label = 'Inc. Outliers')\n",
    "plt.plot(data_MSFT_test['PctChange_N'].sort_values(), pdf2, label = 'Excl. Outliers')\n",
    "plt.xlim([xmin, xmax])\n",
    "\n",
    "\n",
    "\n",
    "# Histogram (density) plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "for bins in binseq:\n",
    "    data_MSFT_test['PctChange_N'].hist(density = True, bins=bins)\n",
    "plt.plot(data_MSFT_test['PctChange_N'].sort_values(), pdf, label = 'Inc. Outliers')\n",
    "plt.plot(data_MSFT_test['PctChange_N'].sort_values(), pdf2, label = 'Excl. Outliers')\n",
    "plt.xlim([xmin, xmax])\n",
    "\n",
    "# Histogram - Residual relation\n",
    "# CALCULATE CENTER OF BIN, ASSESS VALUE OF PDF AT THAT LOCATION, (X-Y)^2\n",
    "\n",
    "\n",
    "\n",
    "# Plot of percentage change vs. time\n",
    "plt.title('Microsoft Data Test Plot')\n",
    "plt.figure()\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "plt.title('PctChange_N')\n",
    "plt.plot(data_MSFT_test['PctChange_N'], label='PctChange_N', marker = 'o',markersize=3,linestyle='dashed')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cols_to_plot = ['Open', 'Close', 'SMA', 'Standard Deviation (O, 10)']\n",
    "\n",
    "data_MSFT_test['SMA'] = data_MSFT_test['Open'].rolling(10).mean()\n",
    "data_MSFT_test['SMAC'] = data_MSFT_test['Close'].rolling(10).mean()\n",
    "data_MSFT_test['Standard Deviation (O, 10)'] = data_MSFT_test['Open'].rolling(10).std()\n",
    "data_MSFT_test['Standard Deviation (O, 5)'] = data_MSFT_test['Open'].rolling(5).std()\n",
    "data_MSFT_test['Standard Deviation (O, 3)'] = data_MSFT_test['Open'].rolling(3).std()\n",
    "data_MSFT_test['Standard Deviation (O, 20)'] = data_MSFT_test['Open'].rolling(20).std()\n",
    "data_MSFT_test['Standard Deviation (O, 40)'] = data_MSFT_test['Open'].rolling(40).std()\n",
    "data_MSFT_test['Standard Deviation (C, 10)'] = data_MSFT_test['Close'].rolling(10).std()\n",
    "# TO DO: MAKE HISTOGRAM OF STD.D.\n",
    "\n",
    "## PLOTTING DATA\n",
    "data_MSFT_test[cols_to_plot].plot()\n",
    "plt.title('Microsoft Data Test Plot')\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Volume')\n",
    "plt.plot(data_MSFT_test['Volume'], label='Volume')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "plt.title('SMAO/C')\n",
    "plt.plot(data_MSFT_test['SMA'], label='SMA (Open)', marker = 'o',markersize=3,linestyle='dashed')\n",
    "plt.plot(data_MSFT_test['SMAC'], label='SMA (Close)', marker = 'x',markersize=3,linestyle='dashed')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "plt.title('Standard Deviation')\n",
    "plt.plot(data_MSFT_test['Standard Deviation (O, 10)'], label='Standard Deviation (Open)', marker = 'o',markersize=3,linestyle='dashed')\n",
    "plt.plot(data_MSFT_test['Standard Deviation (C, 10)'], label='Standard Deviation (Close)', marker = 'o',markersize=3,linestyle='dashed')\n",
    "\n",
    "std_dev_cols = ['Standard Deviation (O, 3)', 'Standard Deviation (O, 5)', 'Standard Deviation (O, 10)', 'Standard Deviation (O, 20)', 'Standard Deviation (O, 40)']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "plt.title('Standard Deviation')\n",
    "plt.plot(data_MSFT_test['Standard Deviation (O, 3)'], label='Standard Deviation (Open)', marker = 'o',markersize=3,linestyle='dashed')\n",
    "plt.plot(data_MSFT_test['Standard Deviation (O, 5)'], label='Standard Deviation (Open)', marker = 'o',markersize=3,linestyle='dashed')\n",
    "plt.plot(data_MSFT_test['Standard Deviation (O, 10)'], label='Standard Deviation (Open)', marker = 'o',markersize=3,linestyle='dashed')\n",
    "plt.plot(data_MSFT_test['Standard Deviation (O, 20)'], label='Standard Deviation (Open)', marker = 'o',markersize=3,linestyle='dashed')\n",
    "plt.plot(data_MSFT_test['Standard Deviation (O, 40)'], label='Standard Deviation (Open)', marker = 'o',markersize=3,linestyle='dashed')\n",
    "\n",
    "plt.title('Standard Deviation')\n",
    "data_MSFT_test[std_dev_cols].plot()\n",
    "\n",
    "## Here we also do a auto-correlation test\n",
    "#fig, ax = plt.subplots(figsize=(12, 6))\n",
    "#sm.graphics.tsa.plot_acf(data_MSFT_test['Close'], lags=195, ax=ax)\n",
    "#plt.title('Autocorrelation Function (ACF)')\n",
    "#plt.xlabel('Lags')\n",
    "#plt.ylabel('ACF')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6166a2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### TO DO:\n",
    "# We are supposed to assess the % change of stock values\n",
    "# I will calculate this, and also its variance, I will do this for different resolutions and in a rolling window (I am thinking of sizes, 5, 10, 20, 50, 100)\n",
    "# Should I calculate a global (or yearly or monthly) variance and compare this to the window to see if theres a match for specific sizes (which correspond to same timeframe)?\n",
    "# Once I do this I can really go into the trade models I made previously\n",
    "\n",
    "##### THOUGHTS:\n",
    "# Is there a point in hypothesis testing a 'true' variance or mean for growth stocks? perhaps other than to test for a non-constant mean model?\n",
    "# There is point in hypothesis testing dividend stocks/ETFs (i.e. FTSE 100 Vanguard ETF)\n",
    "# Is it reasonable to model the mean (or exp. stock value) as fct of time in context of a fourier transform\n",
    "# (as many growth+dividend stocks have cyclical behaviour around quarterlies, typically goes up then down later) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_MSFT_test = checkManifest.loadData_fromcsv(\"MSFT\", 15, \"2024-08\")\n",
    "data_MSFT_test.set_index(pd.to_datetime(data_MSFT_test['DateTime']), inplace=True)\n",
    "\n",
    "\n",
    "############\n",
    "############\n",
    "# Modelling prices:\n",
    "# Get nominal price for each time, use to get percent change from last instance\n",
    "# Convert to histogram/empirical distribution and model the distribution\n",
    "\n",
    "# TO DO:\n",
    "# OBTAIN HISTOGRAM\n",
    "# I EXPECT OUTLIERS TO PROVIDE INFORMATION (PERHAPS W.R.T. INTER-DAY PRICE OR HIGHER VOLATILITY AT TRADING DAY START/END)\n",
    "\n",
    "# THE BIN RESOLUTION FOR HISTOGRAMS ARE IMPORTANT IN ASSESSING HOW CLOSELY THE DISTRIBUTION MATCHES THE DATA, MAYBE.\n",
    "# IF TOO COARSE, THE BIN CENTRE WILL BE TOO HIGH, OTHERWISE IT WILL BE TOO FLAT (AND UNEVEN)\n",
    "# IS THE BIN REPRESENTATION A BETTER ASSESSMENT OF THE RESIDUAL OF THE DATA <-> DISTRIBUTION FIT, OR CAN THE RAW DATA BE COMPARED DIRECTLY TO THE DISTRIBUTION?\n",
    "# THE LATTER SOUNDS LIKE IT DOESNT MAKE SENSE AS THE PDF IS TO DO WITH \"PROBABILITY DENSITY\" SO WE MUST USE BINS TO COLLECT REGIONS IN THE PRICE (X) DOMAIN\n",
    "# IF SO I MUST STUDY WHAT BIN SIZE IS OPTIMAL FOR RESIDUAL, IF THE OPTIMAL BIN SIZE CHANGES FOR DIFFERENT DISTRIBUTIONS (PROBABLY YES) OR FOR OUTLIER EFFECTS (ALSO YES)\n",
    "\n",
    "# TO CONSIDER:\n",
    "# MODELLING EACH TIME INSTANCE WITH ITS OWN LIMITED-DOMAIN DISTRIBUTION BASED ON MAX-MIN PRICE AND VOLUME\n",
    "# PRESUME EACH TRADE IS A RANDOM VARIABLE FOLLOWING (WHICH??) DISTRIBUTION\n",
    "# THEREFORE THE COLLECTIVE VOLUME FOLLOWS (WHICH? T-DISTRI.?) DISTRIBUTION\n",
    "\n",
    "# Obtain nominal price as mean of high and low prices (as we have no idea where the mean trade of each time instance may be so our most 'accurate' is probably the center of the limits\n",
    "# Note: can possibly assume a finite domain bell distribution for the nominal value\n",
    "data_MSFT_test['Nominal'] = (data_MSFT_test['High'] + data_MSFT_test['Low']) /2\n",
    "data_MSFT_test['PctChange_N'] = data_MSFT_test['Nominal'].diff()/data_MSFT_test['Nominal']*100\n",
    "#data_MSFT_test['PctChange_H'] = data_MSFT_test['High'].diff()/data_MSFT_test['High']*100\n",
    "#data_MSFT_test['PctChange_L'] = data_MSFT_test['Low'].diff()/data_MSFT_test['Low']*100\n",
    "\n",
    "\n",
    "# Bin count for histogram\n",
    "bincount_list = [5,10,20,40,80,160,320] # Note if too high, worse bell-curve representation\n",
    "# Interval to exclude outliers from\n",
    "exclusion_int = 0.01\n",
    "# Exclude outlier (in display)\n",
    "no_outliers = True\n",
    "\n",
    "# [min, max] ranges of the histogram\n",
    "rangemin = data_MSFT_test['PctChange_N'].min()\n",
    "rangemax = data_MSFT_test['PctChange_N'].max()\n",
    "# Exclusive interval range for histogram (excludes outliers)\n",
    "excl_min = data_MSFT_test['PctChange_N'].quantile(exclusion_int)\n",
    "excl_max = data_MSFT_test['PctChange_N'].quantile(1-exclusion_int)\n",
    "\n",
    "# Outlier data filtering\n",
    "data_filtered = data_MSFT_test[(data_MSFT_test['PctChange_N'] < excl_max) & (data_MSFT_test['PctChange_N'] > excl_min)]\n",
    "\n",
    "xmin = None\n",
    "xmax = None\n",
    "if no_outliers:\n",
    "    xmin = excl_min\n",
    "    xmax = excl_max\n",
    "else:\n",
    "    xmin = rangemin\n",
    "    xmax = rangemax\n",
    "\n",
    "# Bin sequence list for dataset histogram (for the range of bincounts)\n",
    "binseq = []\n",
    "for bincount in bincount_list:\n",
    "    binseq += [[xmin + i*(xmax-xmin)/bincount for i in range(bincount+1)]] # Can truncate for efficiency\n",
    "\n",
    "# CREATE MODEL TO FIT/SHOW ON PLOT AND ASSESS ACCURACY\n",
    "# CREATE CURVE AND EVALUATE CURVE VALUE ON HIST LOCATION, GET ITS RESIDUAL\n",
    "# JUST USE STATS TO FIND WHAT RESIDUAL WOULD BE FOR CURVES GIVEN DATAPOINTS\n",
    "# TEST CONFIDENCE INTERVALS?\n",
    "# WHAT ABOUT KERNEL DENSITY ESTIMATION?\n",
    "\n",
    "# Distribution testing\n",
    "mean_out = data_MSFT_test['PctChange_N'].mean()\n",
    "std_out = data_MSFT_test['PctChange_N'].std()\n",
    "mean_nout = data_filtered['PctChange_N'].mean()\n",
    "std_nout = data_filtered['PctChange_N'].std()\n",
    "\n",
    "pdf = stats.norm.pdf(data_MSFT_test['PctChange_N'].sort_values(), mean_out, std_out) # Values sorted as iterated through index\n",
    "pdf2 = stats.norm.pdf(data_MSFT_test['PctChange_N'].sort_values(), mean_nout, std_nout) # Values sorted as iterated through index\n",
    "plt.plot(data_MSFT_test['PctChange_N'].sort_values(), pdf, label = 'Inc. Outliers')\n",
    "plt.plot(data_MSFT_test['PctChange_N'].sort_values(), pdf2, label = 'Excl. Outliers')\n",
    "plt.xlim([xmin, xmax])\n",
    "\n",
    "\n",
    "\n",
    "# Histogram (density) plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "for bins in binseq:\n",
    "    data_MSFT_test['PctChange_N'].hist(density = True, bins=bins)\n",
    "plt.plot(data_MSFT_test['PctChange_N'].sort_values(), pdf, label = 'Inc. Outliers')\n",
    "plt.plot(data_MSFT_test['PctChange_N'].sort_values(), pdf2, label = 'Excl. Outliers')\n",
    "plt.xlim([xmin, xmax])\n",
    "\n",
    "# Histogram - Residual relation\n",
    "# CALCULATE CENTER OF BIN, ASSESS VALUE OF PDF AT THAT LOCATION, (X-Y)^2\n",
    "\n",
    "\n",
    "\n",
    "# Plot of percentage change vs. time\n",
    "plt.title('Microsoft Data Test Plot')\n",
    "plt.figure()\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "plt.title('PctChange_N')\n",
    "plt.plot(data_MSFT_test['PctChange_N'], label='PctChange_N', marker = 'o',markersize=3,linestyle='dashed')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cols_to_plot = ['Open', 'Close', 'SMA', 'Standard Deviation (O, 10)']\n",
    "\n",
    "data_MSFT_test['SMA'] = data_MSFT_test['Open'].rolling(10).mean()\n",
    "data_MSFT_test['SMAC'] = data_MSFT_test['Close'].rolling(10).mean()\n",
    "data_MSFT_test['Standard Deviation (O, 10)'] = data_MSFT_test['Open'].rolling(10).std()\n",
    "data_MSFT_test['Standard Deviation (O, 5)'] = data_MSFT_test['Open'].rolling(5).std()\n",
    "data_MSFT_test['Standard Deviation (O, 3)'] = data_MSFT_test['Open'].rolling(3).std()\n",
    "data_MSFT_test['Standard Deviation (O, 20)'] = data_MSFT_test['Open'].rolling(20).std()\n",
    "data_MSFT_test['Standard Deviation (O, 40)'] = data_MSFT_test['Open'].rolling(40).std()\n",
    "data_MSFT_test['Standard Deviation (C, 10)'] = data_MSFT_test['Close'].rolling(10).std()\n",
    "# TO DO: MAKE HISTOGRAM OF STD.D.\n",
    "\n",
    "## PLOTTING DATA\n",
    "data_MSFT_test[cols_to_plot].plot()\n",
    "plt.title('Microsoft Data Test Plot')\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Volume')\n",
    "plt.plot(data_MSFT_test['Volume'], label='Volume')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "plt.title('SMAO/C')\n",
    "plt.plot(data_MSFT_test['SMA'], label='SMA (Open)', marker = 'o',markersize=3,linestyle='dashed')\n",
    "plt.plot(data_MSFT_test['SMAC'], label='SMA (Close)', marker = 'x',markersize=3,linestyle='dashed')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "plt.title('Standard Deviation')\n",
    "plt.plot(data_MSFT_test['Standard Deviation (O, 10)'], label='Standard Deviation (Open)', marker = 'o',markersize=3,linestyle='dashed')\n",
    "plt.plot(data_MSFT_test['Standard Deviation (C, 10)'], label='Standard Deviation (Close)', marker = 'o',markersize=3,linestyle='dashed')\n",
    "\n",
    "std_dev_cols = ['Standard Deviation (O, 3)', 'Standard Deviation (O, 5)', 'Standard Deviation (O, 10)', 'Standard Deviation (O, 20)', 'Standard Deviation (O, 40)']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "plt.title('Standard Deviation')\n",
    "plt.plot(data_MSFT_test['Standard Deviation (O, 3)'], label='Standard Deviation (Open)', marker = 'o',markersize=3,linestyle='dashed')\n",
    "plt.plot(data_MSFT_test['Standard Deviation (O, 5)'], label='Standard Deviation (Open)', marker = 'o',markersize=3,linestyle='dashed')\n",
    "plt.plot(data_MSFT_test['Standard Deviation (O, 10)'], label='Standard Deviation (Open)', marker = 'o',markersize=3,linestyle='dashed')\n",
    "plt.plot(data_MSFT_test['Standard Deviation (O, 20)'], label='Standard Deviation (Open)', marker = 'o',markersize=3,linestyle='dashed')\n",
    "plt.plot(data_MSFT_test['Standard Deviation (O, 40)'], label='Standard Deviation (Open)', marker = 'o',markersize=3,linestyle='dashed')\n",
    "\n",
    "plt.title('Standard Deviation')\n",
    "data_MSFT_test[std_dev_cols].plot()\n",
    "\n",
    "## Here we also do a auto-correlation test\n",
    "#fig, ax = plt.subplots(figsize=(12, 6))\n",
    "#sm.graphics.tsa.plot_acf(data_MSFT_test['Close'], lags=195, ax=ax)\n",
    "#plt.title('Autocorrelation Function (ACF)')\n",
    "#plt.xlabel('Lags')\n",
    "#plt.ylabel('ACF')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8275aa8d-a137-4e2f-8df4-6d13fa6d987f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "## Here we read the .csv data and do a time-series analysis (rolling mean)\n",
    "\n",
    "data_ARM = pd.read_csv('StockHistData\\ARM.csv', index_col='Date', parse_dates=['Date'])\n",
    "\n",
    "#cols_to_plot = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'SMA', 'Standard Deviation']\n",
    "cols_to_plot = ['Close', 'SMA', 'Standard Deviation']\n",
    "print(data_ARM)\n",
    "data_ARM['SMA'] = data_ARM['Open'].rolling(10).mean()\n",
    "data_ARM['Standard Deviation'] = data_ARM['Open'].rolling(10).std()\n",
    "\n",
    "df_ARM = pd.DataFrame(data_ARM)\n",
    "\n",
    "data_ARM[cols_to_plot].plot()\n",
    "plt.title('ARM Technologies')\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Volume')\n",
    "plt.plot(data_ARM['Volume'], label='Volume')\n",
    "\n",
    "## Here we also do a auto-correlation test\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "sm.graphics.tsa.plot_acf(data_ARM['Close'], lags=195, ax=ax)\n",
    "plt.title('Autocorrelation Function (ACF)')\n",
    "plt.xlabel('Lags')\n",
    "plt.ylabel('ACF')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc7cd06-871d-43bd-8185-e34b0b16a972",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Same for MSFT\n",
    "data_MSFT = pd.read_csv('StockHistData\\MSFT.csv', index_col='Date', parse_dates=['Date'])\n",
    "\n",
    "#cols_to_plot = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'SMA', 'Standard Deviation']\n",
    "cols_to_plot = ['Close', 'SMA', 'Standard Deviation']\n",
    "\n",
    "data_MSFT['SMA'] = data_MSFT['Open'].rolling(10).mean()\n",
    "data_MSFT['Standard Deviation'] = data_MSFT['Open'].rolling(10).std()\n",
    "\n",
    "df_MSFT = pd.DataFrame(data_MSFT)\n",
    "\n",
    "data_MSFT[cols_to_plot].plot()\n",
    "plt.title('Microsoft')\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Volume')\n",
    "plt.plot(data_MSFT['Volume'], label='Volume')\n",
    "\n",
    "## Here we also do a auto-correlation test\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "sm.graphics.tsa.plot_acf(data_MSFT['Close'], lags=1257, ax=ax)\n",
    "plt.title('Autocorrelation Function (ACF)')\n",
    "plt.xlabel('Lags')\n",
    "plt.ylabel('ACF')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b3cd4b-a5f1-4d7b-ac2b-aabf79312517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebb5f8a-dc16-4314-8ddb-b50ff410405d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Then NVDA\n",
    "data_NVDA = pd.read_csv('StockHistData\\\\NVDA.csv', index_col='Date', parse_dates=['Date'])\n",
    "\n",
    "#cols_to_plot = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'SMA', 'Standard Deviation']\n",
    "cols_to_plot = ['Close', 'SMA', 'Standard Deviation']\n",
    "\n",
    "data_NVDA['SMA'] = data_NVDA['Open'].rolling(10).mean()\n",
    "data_NVDA['Standard Deviation'] = data_NVDA['Open'].rolling(10).std()\n",
    "\n",
    "df_NVDA = pd.DataFrame(data_NVDA)\n",
    "\n",
    "data_NVDA[cols_to_plot].plot()\n",
    "plt.title('Nvidia')\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Volume')\n",
    "plt.plot(data_NVDA['Volume'], label='Volume')\n",
    "\n",
    "## Here we also do a auto-correlation test\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "sm.graphics.tsa.plot_acf(data_NVDA['Close'], lags=1257, ax=ax)\n",
    "plt.title('Autocorrelation Function (ACF)')\n",
    "plt.xlabel('Lags')\n",
    "plt.ylabel('ACF')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5673a70-cb95-4f60-9607-2cfa82f35117",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets try for KO\n",
    "data_KO = pd.read_csv('StockHistData\\\\KO.csv', index_col='Date', parse_dates=['Date'])\n",
    "\n",
    "#cols_to_plot = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'SMA', 'Standard Deviation']\n",
    "cols_to_plot = ['Close', 'SMA', 'Standard Deviation']\n",
    "\n",
    "data_KO['SMA'] = data_KO['Open'].rolling(10).mean()\n",
    "data_KO['Standard Deviation'] = data_KO['Open'].rolling(10).std()\n",
    "\n",
    "df_KO = pd.DataFrame(data_KO)\n",
    "\n",
    "data_KO[cols_to_plot].plot()\n",
    "plt.title('Coca-cola')\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Volume')\n",
    "plt.plot(data_KO['Volume'], label='Volume')\n",
    "\n",
    "## Here we also do a auto-correlation test\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "sm.graphics.tsa.plot_acf(data_KO['Close'], lags=1257, ax=ax)\n",
    "plt.title('Autocorrelation Function (ACF)')\n",
    "plt.xlabel('Lags')\n",
    "plt.ylabel('ACF')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d167f87b-4ec3-44ad-90fe-4a6b8e9f7f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlations between the different stocks\n",
    "correlation1 = df_MSFT['Close'][-1257:].corr(df_NVDA['Close'][-1257:])\n",
    "correlation1k = df_MSFT['Close'][-1257:].corr(df_NVDA['Close'][-1257:], method='kendall')\n",
    "correlation1s = df_MSFT['Close'][-1257:].corr(df_NVDA['Close'][-1257:], method='spearman')\n",
    "\n",
    "print(f\"Correlation coefficient between MSFT and NVDA: {correlation1} ({correlation1k} with Kendall and {correlation1s} with Spearman)\")\n",
    "\n",
    "correlation2 = df_MSFT['Close'][-195:].corr(df_ARM['Close'][-195:])\n",
    "correlation2k = df_MSFT['Close'][-195:].corr(df_ARM['Close'][-195:], method='kendall')\n",
    "correlation2s = df_MSFT['Close'][-195:].corr(df_ARM['Close'][-195:], method='spearman')\n",
    "\n",
    "print(f\"Correlation coefficient between MSFT and ARM: {correlation2} ({correlation2k} with Kendall and {correlation2s} with Spearman)\")\n",
    "\n",
    "correlation3 = df_ARM['Close'][-195:].corr(df_NVDA['Close'][-195:])\n",
    "correlation3k = df_ARM['Close'][-195:].corr(df_NVDA['Close'][-195:], method='kendall')\n",
    "correlation3s = df_ARM['Close'][-195:].corr(df_NVDA['Close'][-195:], method='spearman')\n",
    "\n",
    "print(f\"Correlation coefficient between ARM and NVDA: {correlation3} ({correlation3k} with Kendall and {correlation3s} with Spearman)\")\n",
    "\n",
    "correlation4 = df_KO['Close'][-1257:].corr(df_NVDA['Close'][-1257:])\n",
    "correlation4k = df_KO['Close'][-1257:].corr(df_NVDA['Close'][-1257:], method='kendall')\n",
    "correlation4s = df_KO['Close'][-1257:].corr(df_NVDA['Close'][-1257:], method='spearman')\n",
    "\n",
    "print(f\"Correlation coefficient between KO and NVDA: {correlation4} ({correlation4k} with Kendall and {correlation4s} with Spearman)\")\n",
    "\n",
    "correlation5 = df_KO['Close'][-1257:].corr(df_MSFT['Close'][-1257:])\n",
    "correlation5k = df_KO['Close'][-1257:].corr(df_MSFT['Close'][-1257:], method='kendall')\n",
    "correlation5s = df_KO['Close'][-1257:].corr(df_MSFT['Close'][-1257:], method='spearman')\n",
    "\n",
    "print(f\"Correlation coefficient between ARM and NVDA: {correlation5} ({correlation5k} with Kendall and {correlation5s} with Spearman)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1165160f-72e0-4c4b-a765-a4c425686004",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we do simply Quantitative analysis (mean, std)\n",
    "import numpy as np\n",
    "\n",
    "# Sample Data\n",
    "returns = np.array([0.01, 0.02, -0.01, 0.03, 0.005])\n",
    "\n",
    "# Calculate Mean and Standard Deviation\n",
    "mean_return = np.mean(returns)\n",
    "std_deviation = np.std(returns)\n",
    "print(f\"Mean Return: {mean_return}, Standard Deviation: {std_deviation}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01726d8-1d92-4be2-b01e-2be0831d3c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we have code to do hypothesis testing\n",
    "from scipy import stats\n",
    "\n",
    "group_A = [0.01, 0.02, 0.015, 0.023, 0.016]\n",
    "group_B = [0.02, 0.025, 0.03, 0.019, 0.021]\n",
    "sig_value = 0.05\n",
    "\n",
    "## We are doing a t-test so see if the mean of group A and group B are the same\n",
    "## The test is bayesian, we assume they are until proven otherwise with a significance value of 0.05 (5%)\n",
    "t_statistic, p_value = stats.ttest_ind(group_A, group_B)\n",
    "print(f\"t-statistic: {t_statistic}, p-value: {p_value}\")\n",
    "\n",
    "trunc_p = '%.3f'%(100*p_value)\n",
    "\n",
    "if p_value >= sig_value:\n",
    "    print(f\"Hypothesis of Group A and Group B means being equal is NOT REJECTED (significance value of {100*sig_value}%, p-value of {trunc_p}%)\")\n",
    "else:\n",
    "    print(f\"Hypothesis of Group A and Group B means being equal is REJECTED (significance value of {100*sig_value}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e153e5-c7b7-4a20-ab09-cd41b6940bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we do some more time-series analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Seed for reproducibility (we will create synthetic data)\n",
    "np.random.seed(99)\n",
    "\n",
    "# Generate synthetic data\n",
    "n_samples = 100\n",
    "dates = pd.date_range(start='2024-01-01', periods=n_samples, freq='B')  # Business days\n",
    "price_changes = np.random.normal(loc=0, scale=1, size=n_samples)  # Random price changes\n",
    "prices = np.cumsum(price_changes) + 100  # Simulated stock prices (random walk)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Date': dates,\n",
    "    'Close': prices\n",
    "})\n",
    "\n",
    "# Plot the time series\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df['Date'], df['Close'], label='Close Price')\n",
    "plt.title('Synthetic Stock Closing Prices')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and plot ACF\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "sm.graphics.tsa.plot_acf(df['Close'], lags=30, ax=ax)\n",
    "plt.title('Autocorrelation Function (ACF)')\n",
    "plt.xlabel('Lags')\n",
    "plt.ylabel('ACF')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c125341-7ef7-408f-a23d-fc4ca588924d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we do some predictive modelling\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Sample Data\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([2, 3, 2.5, 4, 4.5])\n",
    "\n",
    "# Train Model\n",
    "model = LinearRegression().fit(X, y)\n",
    "print(f\"Coefficient: {model.coef_}, Intercept: {model.intercept_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba1aa73-a950-4905-9c48-f4a6e0958223",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we have some code for option pricing (black-scholes)\n",
    "\n",
    "import scipy.stats as si\n",
    "import numpy as np\n",
    "\n",
    "def black_scholes(S, K, T, r, sigma, option_type='call'):\n",
    "    d1 = (np.log(S / K) + (r + 0.5 * sigma ** 2) * T) / (sigma * np.sqrt(T))\n",
    "    d2 = d1 - sigma * np.sqrt(T)\n",
    "    if option_type == 'call':\n",
    "        option_price = (S * si.norm.cdf(d1, 0, 1) - K * np.exp(-r * T) * si.norm.cdf(d2, 0, 1))\n",
    "    elif option_type == 'put':\n",
    "        option_price = (K * np.exp(-r * T) * si.norm.cdf(-d2, 0, 1) - S * si.norm.cdf(-d1, 0, 1))\n",
    "    return option_price\n",
    "\n",
    "# Example Parameters\n",
    "S = 100  # Current stock price\n",
    "K = 105  # Strike price\n",
    "T = 1    # Time to maturity in years\n",
    "r = 0.05 # Risk-free rate\n",
    "sigma = 0.2 # Volatility\n",
    "\n",
    "call_price = black_scholes(S, K, T, r, sigma, option_type='call')\n",
    "print(f\"Call Option Price: {call_price}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30655935-ea66-43cb-b321-97777facd378",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here's some code of logistic regression\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'Open': [100.0, 101.0, 100.5, 101.2],\n",
    "    'High': [102.0, 103.0, 102.5, 101.5],\n",
    "    'Low': [98.0, 99.5, 99.0, 99.8],\n",
    "    'Close': [101.0, 100.5, 101.5, 100.8],\n",
    "    'Volume': [1500000, 1700000, 1800000, 1300000],\n",
    "    'SMA_10': [99.5, 100.0, 100.2, 100.8],\n",
    "    'RSI': [55.0, 52.5, 58.0, 59],\n",
    "    'Label': [1, 0, 1, 0]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Features and target\n",
    "X = df[['Open', 'High', 'Low', 'Close', 'Volume', 'SMA_10', 'RSI']]\n",
    "y = df['Label']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler(Y) # type: ignore\n",
    "X_train_scaled = scaler.it_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled,y_train)\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c00d66-9492-4728-9d7e-738a8015fd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Seed for reproducibility\n",
    "np.random.seed(10)\n",
    "\n",
    "# Generate synthetic data\n",
    "n_samples = 1000\n",
    "dates = pd.date_range(start='2024-01-01', periods=n_samples, freq='B')\n",
    "open_prices = np.random.uniform(low=100, high=200, size=n_samples)\n",
    "high_prices = open_prices + np.random.uniform(low=0, high=10, size=n_samples)\n",
    "low_prices = open_prices - np.random.uniform(low=0, high=10, size=n_samples)\n",
    "close_prices = open_prices + np.random.uniform(low=-5, high=5, size=n_samples)\n",
    "volume = np.random.randint(low=100000, high=5000000, size=n_samples)\n",
    "\n",
    "# Simple Moving Average (SMA) with window of 10\n",
    "sma_10 = pd.Series(close_prices).rolling(window=10).mean().ffill()\n",
    "\n",
    "# Relative Strength Index (RSI)\n",
    "def calculate_rsi(series, window=14):\n",
    "    delta = series.diff().ffill()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "rsi = calculate_rsi(pd.Series(close_prices))\n",
    "\n",
    "# Label: 1 if next day's close price is higher, else 0\n",
    "labels = np.where(np.roll(close_prices, -1) > close_prices, 1, 0)\n",
    "labels[-1] = 0  # Last label cannot be determined\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Date': dates,\n",
    "    'Open': open_prices,\n",
    "    'High': high_prices,\n",
    "    'Low': low_prices,\n",
    "    'Close': close_prices,\n",
    "    'Volume': volume,\n",
    "    'SMA_10': sma_10,\n",
    "    'RSI': rsi,\n",
    "    'Label': labels\n",
    "})\n",
    "\n",
    "# Remove the last row as it doesn't have a valid label\n",
    "df = df[:-1]\n",
    "\n",
    "# Features and target\n",
    "X = df[['Open', 'High', 'Low', 'Close', 'Volume', 'SMA_10', 'RSI']]\n",
    "y = df['Label']\n",
    "t = df['Date']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test, t_train, t_test = train_test_split(X, y, t, test_size=0.33, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(\"Data type: \",X_train_scaled.dtype )\n",
    "print(\"Data type: \",y_train.dtype )\n",
    "print(\"Dimensions: \",np.shape(X_train_scaled))\n",
    "print(\"Dimensions: \",np.shape(y_train))\n",
    "\n",
    "# Train the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Plot some of the data to visualize\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(dates, close_prices, label='Close Price')\n",
    "plt.plot(dates, sma_10, label='SMA 10')\n",
    "plt.title('Stock Prices and SMA 10')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369ca1d0-f34e-4b65-87ce-d68b79efbb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Information store of how we run python script elsewhere\n",
    "\n",
    "#%run \"D:\\Finance Study\\Python and MATLAB Code\\Core.py\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9d3225-e353-4a5f-b3e5-d8550b4e7085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we try to make a signal indicator from the SMIIO (Stochastic Momentum Index ergodic Indicator plus SMI ergodic Oscillator) of a stock\n",
    "# The code makes an SMI of the stock by calculating the True Strength Index (TSI) of the stock using a slow and a fast period. Also makes an\n",
    "# (ergodic) Indicator by using an Exponential Moving Average (EMA) using the signal period. Finally, makes (ergodic) Oscillator signal by\n",
    "# subtracting the Indicator from the TSI.\n",
    "\n",
    "\n",
    "\n",
    "fast_period = 3 # In the freq of file (days)\n",
    "slow_period = 15\n",
    "signal_period = 3\n",
    "\n",
    "\n",
    "df_smi = ta.momentum.smi(df_NVDA['Close'],fast_period,slow_period,signal_period)\n",
    "\n",
    "\n",
    "smi_name = \"_{}_{}_{}\".format(fast_period,slow_period,signal_period)\n",
    "#SMI + smi_name is the SMI of the stock\n",
    "#SMIs + smi_name is the indicator made from signal line\n",
    "#SMIo + smi_name is the oscillator made by SMI - SMIs\n",
    "\n",
    "\n",
    "cols_plot = [\"SMI\"+smi_name,\"SMIs\"+smi_name,\"SMIo\"+smi_name]\n",
    "\n",
    "df_smi[cols_plot].plot()\n",
    "\n",
    "df_smi.fillna(0, inplace = True) # Removing NaN values\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(df_smi)\n",
    "\n",
    "subtrac=df_smi.iloc[:,0] - df_smi.iloc[:,1] # This is the same as SMIo\n",
    "#plt.figure(figsize=(12,6))\n",
    "#plt.plot(subtrac)\n",
    "\n",
    "\n",
    "df_smi = df_smi.reset_index() # Get date as a non index col to use for bar plot\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.bar(df_smi[\"Date\"],df_smi[\"SMIo\"+smi_name],2,12,color=np.where(df_smi[\"SMIo\"+smi_name] < 0, 'crimson', 'green'))\n",
    "\n",
    "\n",
    "## BEAUTIFULSOUP FOR WEBSCRAPING IN PYTHON\n",
    "## SELENIUM FOR dynamic html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c3f118-822a-4f85-88c7-ae27322de30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we try to make a simple backtesting simulation model where we use an indicator to test approximate success in trading over course of a day\n",
    "# or longer. We use the SMIIO model and simulate a trade whenever a condition is reached (e.g. change from negative to positive).\n",
    "# The trade is initially done as 1 stock trade per instance and we assess the percentage of success (profit made), and size of success.\n",
    "\n",
    "fast = 3\n",
    "slow = 15\n",
    "sig = 4\n",
    "\n",
    "df_smiio = ta.momentum.smi(df_NVDA['Close'],fast,slow,sig)\n",
    "\n",
    "smiio_name = \"_{}_{}_{}\".format(fast,slow,sig)\n",
    "\n",
    "df_smiio = df_smiio.reset_index()\n",
    "\n",
    "#df_smiio.fillna(0, inplace = True) # Removing NaN values\n",
    "\n",
    "\n",
    "for i in range(0,len(df_smiio['SMIo'+smiio_name])):\n",
    "\n",
    "    if np.isnan(df_smiio['SMIo'+smiio_name][i]):\n",
    "        continue\n",
    "\n",
    "    ind_diff = df_smiio['SMIo'+smiio_name][i] - df_smiio['SMIo'+smiio_name][i-1]\n",
    "\n",
    "    last_val = df_smiio['SMIo'+smiio_name][i-1]\n",
    "    \n",
    "    #print(df_smiio['SMIo'+smiio_name][i])\n",
    "    #print(ind_diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5fef8b-5995-4a66-b64c-6b26df302711",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Here we will write basic functions to 'purchase' and 'sell' stocks, and a function to sum up the transactions for a profit/loss measurement\n",
    "histCols = ['Date','Ticker','Volume','Value','TimingState']\n",
    "global dfTradeHist\n",
    "dfTradeHist = pd.DataFrame(columns = histCols)\n",
    "\n",
    "#######\n",
    "def PrepDataFrame(dataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Make all necessary preparation to use the dataframe in model (NOT FULLY COMPLETE). Currently this is:\n",
    "    - Moving any date/time outside of dataframe index\n",
    "    - Changing date/time to a full date-time style (i.e. dates-only will also include time (00:00:00))\n",
    "    \"\"\"\n",
    "    # Guard function to get indexData (usually date) as a non index col to use (if as index) \n",
    "    if 'Date' not in dataFrame.columns: \n",
    "        dataFrame = dataFrame.reset_index()\n",
    "    \n",
    "    # Converts possible date-only to date-time\n",
    "    dataFrame['Date'] = pd.to_datetime(dataFrame.Date, format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # Convert date-time to string (seems to create more bugs)\n",
    "    #dataFrame['Date'] = dataFrame['Date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    return dataFrame\n",
    "#######\n",
    "\n",
    "#######\n",
    "## A FUNCTION TO REMOVE DATA? PROB NOT NEEDED\n",
    "#######\n",
    "\n",
    "\n",
    "\n",
    "#######\n",
    "## A SET OF FUNCTIONS TO ANALYSE STOCK DATA, TRAJECTORY, VOLATILITY ETC.\n",
    "#######\n",
    "\n",
    "\n",
    "#######\n",
    "## FUNCTION TO MODIFY AN ACCURACY/ANALYTICS MATRIX DIRECTLY (INSTEAD OF DOING IT WITHIN THE MODEL ITSELF) FOR INTEROPERABILITY WITH MODEL EVALUATOR\n",
    "#######\n",
    "\n",
    "\n",
    "\n",
    "#######\n",
    "def ResetTradeHist(name = 'dfTradeHist', cols = ['DateTime','Ticker','Volume','Value','TimingState']):\n",
    "    \"\"\" Reset the trade history dataframe and remakes its columns (cols, if specified). \"\"\"    \n",
    "    globals()[name] = pd.DataFrame(columns = cols)\n",
    "    return\n",
    "#######\n",
    "\n",
    "#######\n",
    "def MakeTrade (dateTime, volume, ticker, history = None, buy = True, timingState = 'Close', declare = False) -> [[float, str, float, float, str], pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Simulates making a specified trade and records it (using a sub-routine). Ticker price dataframe must have 'df_' prefix\n",
    "    \"\"\"\n",
    "    # Default history input save status (take dfTradeHist as default storage dataframe)\n",
    "    isDefaultHist = False\n",
    "    if history is None:\n",
    "        global dfTradeHist\n",
    "        history = dfTradeHist.copy()\n",
    "        isDefaultHist = True\n",
    "    \n",
    "    \n",
    "    datasetName = 'df_'+ticker\n",
    "\n",
    "    # The 1st part gets the global dataset, 2nd searches the datetime and 3rd gets the timings state price\n",
    "    unitValue = globals()[datasetName][ globals()[datasetName]['Date'] == dateTime ][timingState].values[0]\n",
    "    \n",
    "    totalValue = -unitValue * volume\n",
    "    transaction = 'bought'\n",
    "    \n",
    "    if not buy:\n",
    "        transaction = 'sold'\n",
    "\n",
    "    if declare == True:\n",
    "        print(f\"{volume} {ticker} stock(s) {transaction} at total price {-totalValue} (unit price {unitValue}) at {dateTime}\")\n",
    "\n",
    "    if not buy:\n",
    "        totalValue = -totalValue\n",
    "\n",
    "    result = [dateTime, ticker, volume, totalValue, timingState]\n",
    "    \n",
    "    ## Store in history\n",
    "    history = RecordAction(result, history)\n",
    "\n",
    "    # Default history location (if None then it won't update the original dataframe otherwise)\n",
    "    if isDefaultHist:\n",
    "        dfTradeHist = history.copy()\n",
    "    \n",
    "    return [[dateTime, ticker, volume, totalValue, timingState], history]\n",
    "#######\n",
    "#######\n",
    "def RecordAction(action, dataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Records action taken into a history dataframe.\n",
    "    Note: Make sure the action row size is the same as the column size in the dataFrame.\n",
    "    For each column take action indices and add to the existing history dataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    # A slightly different process for first action recorded to avoid python warning\n",
    "    if dataFrame.size == 0: # If no data\n",
    "        \n",
    "        print('Recording first input into trade history')\n",
    "        dataFrame = pd.DataFrame(columns = dataFrame.columns)\n",
    "\n",
    "    # Adding all actions into a dictionary to append to dataFrame\n",
    "    appendDict = {}\n",
    "    \n",
    "    for i in range(0,len(action)):\n",
    "        appendDict[dataFrame.columns[i]] = action[i]\n",
    "    appendSeries = pd.Series(appendDict)    \n",
    "    \n",
    "    return pd.concat([dataFrame,appendSeries.to_frame().T], ignore_index=True)\n",
    "#######\n",
    "#######\n",
    "def EvaluateModel(dataFrame, *arguments, **keywords) -> float:\n",
    "    \"\"\"\n",
    "    This function is supposed to take the trade/call history and evaluate its properties,\n",
    "    such as success (profit/loss) and other properties (e.g. profit vs. loss freq. etc.).\n",
    "    \"\"\"\n",
    "    # Additional things to implement:\n",
    "    # Exchange rate of currency effects\n",
    "    # Trading costs\n",
    "    \n",
    "    \n",
    "    print('------------------')\n",
    "    print('Evaluating current algo model...')\n",
    "    \n",
    "    # Stratifying data based on ticker\n",
    "    stratDF = dataFrame.groupby('Ticker').apply(lambda x: x)\n",
    "    stratDF = stratDF.drop(columns=['Ticker'])\n",
    "\n",
    "    # Extended output\n",
    "    if any(val == 'Extended' for val in keywords.values()):\n",
    "        print('List of trades to be evaluated:')\n",
    "        print(stratDF)\n",
    "\n",
    "\n",
    "    # Make a DF of compiled stats\n",
    "    basicStatsCols = ['LongVol','ShortVol','RemainVol','Cost','Income','Profit']\n",
    "    compileDF = pd.DataFrame(columns = basicStatsCols)\n",
    "    \n",
    "\n",
    "    # Takes list of the (multi-level) index of DF (tuple) and converts to\n",
    "    # dictionary for unique 'Ticker' key \n",
    "    print('List of tickers traded:')\n",
    "    for key in dict(stratDF.index.tolist()).keys():\n",
    "        print(key)\n",
    "        # key is the different ticker names\n",
    "\n",
    "    # Implement the evaluation for each ticker traded here\n",
    "    for key in dict(stratDF.index.tolist()).keys():\n",
    "        # stratDF.loc[key] is the different ticker trades each in own DF\n",
    "        tickerDF = stratDF.loc[key]\n",
    "\n",
    "        # Extended output\n",
    "        for value in keywords.values():\n",
    "            if value == 'Verbose':\n",
    "                print('Trade history of ticker ' + key +':')\n",
    "                print(tickerDF)\n",
    "\n",
    "        sumVal = tickerDF['Value'].sum()\n",
    "        \n",
    "        ## Basic stats\n",
    "        sumShortVol = 0\n",
    "        sumCost = 0 # Cost is for purchasing the share etc.\n",
    "        sumLongVol = 0\n",
    "        sumIncome = 0 # Income is obtained by selling\n",
    "        for i in range(0,len(tickerDF['Value'])):\n",
    "            if tickerDF['Value'].iloc[i] > 0:\n",
    "                sumShortVol = sumShortVol + tickerDF['Volume'].iloc[i]\n",
    "                sumIncome = sumIncome + tickerDF['Value'].iloc[i]\n",
    "            else:\n",
    "                sumLongVol = sumLongVol + tickerDF['Volume'].iloc[i]\n",
    "                sumCost = sumCost - tickerDF['Value'].iloc[i]\n",
    "        profit = sumIncome - sumCost\n",
    "        specProfit = profit*2/(sumShortVol+sumLongVol)\n",
    "    \n",
    "        # To implement a existing position closing system (to evaluate the model more accurately)\n",
    "        remainVol = sumLongVol - sumShortVol\n",
    "        if remainVol != 0:\n",
    "            print(Fore.RED)\n",
    "            print(' !!!!!!!!!!')\n",
    "            print('There is an existing open position in ' + key + '! This may impact the accuracy of the model evaluation.')\n",
    "            if value == 'Extended':\n",
    "                if remainVol > 0:\n",
    "                    print('Open position size: ' + str(remainVol) + ' shares long.')\n",
    "                else:\n",
    "                    print('Open position size: ' + str(-remainVol) + ' shares short.')\n",
    "            \n",
    "            print('Existing positions will be closed.')\n",
    "            print(' !!!!!!!!!!')\n",
    "            print(Fore.BLACK)\n",
    "        \n",
    "            # Closing existing positions using last unit price\n",
    "            datasetName = 'df_' + key\n",
    "            \n",
    "            # Portfolio value is positive if long (remain vol > 0)\n",
    "            portValue = remainVol * globals()[datasetName]['Close'][len(globals()[datasetName]['Close'])-1]\n",
    "            \n",
    "            finalProfit = profit + portValue\n",
    "            if portValue > 0:\n",
    "                finalCost = sumCost\n",
    "                finalIncome = sumIncome + portValue\n",
    "            else:\n",
    "                finalCost = sumCost + portValue\n",
    "                finalIncome = sumIncome\n",
    "    \n",
    "        ## Advanced stats \n",
    "\n",
    "\n",
    "\n",
    "        # Adding all basic stats for each ticker into an array then converting into dictionary to append to dataFrame\n",
    "        # basicStats uses 'final' position if unclosed     \n",
    "        basicStats = []        \n",
    "        if remainVol != 0:\n",
    "            basicStats = [sumLongVol, sumShortVol,remainVol,finalCost,finalIncome,finalProfit]\n",
    "        else:\n",
    "            basicStats = [sumLongVol, sumShortVol,remainVol,sumCost,sumIncome,profit]\n",
    "\n",
    "        # Append dictionary for dataFrame\n",
    "        appendDict = {}\n",
    "        for i in range(0,len(basicStats)):\n",
    "            appendDict[compileDF.columns[i]] = basicStats[i]\n",
    "\n",
    "        compileDF = compileDF._append(appendDict, ignore_index=True)\n",
    "    \n",
    "        # Currently only evaluates the profit/loss levels (not including closing existing positions)\n",
    "        print('===================\\n\\nCalculating basic evaluation stats in ticker '+key+':')\n",
    "        \n",
    "        print('Current profit/loss stats (not closing existing positions):')\n",
    "        print('Total profit from all trades: ' + str(sumVal))\n",
    "        print('Total cost: ' + str(sumCost))\n",
    "        print('Total income: ' + str(sumIncome))\n",
    "        print('Total profit: ' + str(profit))\n",
    "        \n",
    "        print('Profit Margin: ' + str(profit/sumCost))\n",
    "        print('Specific Profit (per volume traded): ' + str(specProfit)) # Calculated as avg. of long and short vol. (if shares outstanding)\n",
    "    \n",
    "        if remainVol != 0:\n",
    "            print(Fore.BLUE)\n",
    "            print('Current profit/loss stats (after closing existing positions):')\n",
    "            print('Remaining portfolio value (before close): ' + str(portValue))\n",
    "            print('Final cost: ' + str(compileDF['Cost'].sum()))\n",
    "            print('Final income: ' + str(compileDF['Income'].sum()))\n",
    "            print('Final profit from all trades: ' + str(compileDF['Profit'].sum()))\n",
    "            \n",
    "            print('Final Profit Margin: ' + str(compileDF['Profit'].sum()/compileDF['Cost'].sum()))\n",
    "            print(Fore.BLACK)\n",
    "        \n",
    "        print('===================')\n",
    "\n",
    "\n",
    "    if any(val == 'AddEval' for val in keywords.values()):\n",
    "        for input in arguments: # Do not need to use *args this way but I chose to, its fine until I want to use more than 1 args dataframe\n",
    "            if isinstance(input, pd.DataFrame):\n",
    "\n",
    "                ## TO ADD TRADING ANALYTICS HERE, THIS SHOULD ONLY TAKE IN OUTPUT FROM ELSEWHERE\n",
    "                ## SHOULD CREATE FUNCTIONS TO DIRECTLY MODIFY ACCURACY MATRIX INSTEAD OF MAKING WITHIN EACH MODEL\n",
    "                ## SHOULD ALSO CREATE STOCK ANALYSER METHOD SEPARATELY\n",
    "\n",
    "            \n",
    "                print(Fore.GREEN)\n",
    "                print('#################################\\nCalculating additional model evaluations from given accuracy dataFrame:')\n",
    "                print('Model efficiency stats:')\n",
    "                print('Trade accuracy (trades being incorrect by instance, not volume?): (NOT IMPLEMENTED YET)')\n",
    "                print('Trade efficiency (how many trades are not correct?): (NOT IMPLEMENTED YET)')\n",
    "                print('Model Loss (how far are trades from actual optimal points): (NOT IMPLEMENTED YET)')\n",
    "                print('#################################')\n",
    "                print(Fore.BLACK)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print('------------------')\n",
    "    return 1\n",
    "\n",
    "df_NVDA = PrepDataFrame(df_NVDA)\n",
    "df_MSFT = PrepDataFrame(df_MSFT)\n",
    "df_KO = PrepDataFrame(df_KO)\n",
    "\n",
    "[NVDA_L1,_] = MakeTrade('2019-07-01 00:00:00', 10.5, 'NVDA')\n",
    "[MSFT_L1,_] = MakeTrade('2019-10-01 00:00:00', 6, 'MSFT')\n",
    "[NVDA_S1,_] = MakeTrade('2022-07-01 00:00:00', 10.5, 'NVDA', buy = False)\n",
    "[NVDA_S2,_] = MakeTrade('2023-06-01 00:00:00', 100, 'NVDA', buy = False)\n",
    "[NVDA_L2,_] = MakeTrade('2023-08-01 00:00:00', 99, 'NVDA', buy = True)\n",
    "[MSFT_S1,_] = MakeTrade('2022-07-01 00:00:00', 6, 'MSFT', history = None, buy = False)\n",
    "\n",
    "num = EvaluateModel(dfTradeHist, depth = 'AddEval', debug = 'Simple')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b4f61f-d884-45ea-bf1d-8431a33ba96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating sub-routines for models here\n",
    "\n",
    "# Here we will create a sub-routine that provides conditions for specified trades when a condition is met.\n",
    "# The basic one will be when a set of values becomes positive (from a negative/zero value).\n",
    "\n",
    "\n",
    "def WhenPositive(dataset,searchData,indexData):\n",
    "    \"\"\"\n",
    "    This function returns set of independent variables (outData, e.g. date/time) in the selected data\n",
    "    to be searched (indexData) from the dataset, when searchData (e.g. price) turns positive from negative.\n",
    "    searchData and indexData are strings of the column names.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Indexed (observed) dataset (independent var.)\n",
    "    obsDataset = dataset[indexData]\n",
    "\n",
    "    # Searched dataset (dependent var.)\n",
    "    searchDataset = dataset[searchData]\n",
    "\n",
    "    # Outputted dataset array (indep. var.)\n",
    "    outDataset = []\n",
    "\n",
    "    # Goes through all points to get when turn positive\n",
    "    for i in range(1,len(obsDataset)):\n",
    "        # range starts from 1 because need the i-1'th datapoint\n",
    "        # If negative, irrelevant so skip step\n",
    "        if searchDataset.iloc[i] <= 0:\n",
    "            continue\n",
    "\n",
    "        # If positive but last step negative, record\n",
    "        if searchDataset.iloc[i-1] < 0:\n",
    "            outDataset = outDataset + [obsDataset.iloc[i]]\n",
    "\n",
    "    # Note output is an array for efficiency\n",
    "    return outDataset\n",
    "\n",
    "def WhenNegative(dataset,searchData,indexData):\n",
    "    \"\"\"\n",
    "    This function returns set of independent variables (outData, e.g. date/time) in the selected data\n",
    "    to be searched (indexData) from the dataset, when searchData (e.g. price) turns negative from positive.\n",
    "    searchData and indexData are strings of the column names.\n",
    "    Same as WhenPositive but reverse as you cant just use -dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Indexed (observed) dataset (independent var.)\n",
    "    obsDataset = dataset[indexData]\n",
    "\n",
    "    # Searched dataset (dependent var.)\n",
    "    searchDataset = dataset[searchData]\n",
    "\n",
    "    # Outputted dataset array (indep. var.)\n",
    "    outDataset = []\n",
    "\n",
    "    # Goes through all points to get when turn negative\n",
    "    for i in range(1,len(obsDataset)-1):\n",
    "        # range starts from 1 because need the i-1'th datapoint\n",
    "        # range ends at len(obsDataset)-1 if last datapoint is 'live' and pending update\n",
    "        # If positive, irrelevant so skip step\n",
    "        if searchDataset.iloc[i] >= 0:\n",
    "            continue\n",
    "\n",
    "        # If negative but last step positive, record\n",
    "        if searchDataset.iloc[i-1] > 0:\n",
    "            outDataset = outDataset + [obsDataset.iloc[i]]\n",
    "\n",
    "    # Note output is an array for efficiency\n",
    "    return outDataset\n",
    "\n",
    "def JumpChecker(dataset, searchData, indexData, jumpThresh, jumpPeriod = 1, signalPeriod = 1):\n",
    "    \"\"\"\n",
    "    This function returns set of independent variables (outData, e.g. date/time) in the selected data\n",
    "    to be searched (indexData) from the dataset, when searchData (e.g. price) jumps (or changes) a\n",
    "    certain percentage (jumpThresh) on average of a (signalPeriod) time period, from its last (jumpPeriod)\n",
    "    time periods ago.\n",
    "    searchData and indexData are strings of the column names.\n",
    "    \"\"\"\n",
    "\n",
    "    # Error input guard (fixes negative input values for real numbers)\n",
    "    jumpThresh = abs(jumpThresh)\n",
    "    jumpPeriod = abs(jumpPeriod)\n",
    "    \n",
    "    # Indexed (observed) dataset (independent var.) - dataset[indexData]\n",
    "    # Searched dataset (dependent var.) - dataset[searchData]\n",
    "\n",
    "    # Outputted dataset array pair (independent var., and bool) [['Time'],['Jump?']]\n",
    "    outDataset = []\n",
    "    jumpTime = []\n",
    "    isJump = []\n",
    "    \n",
    "    # Get rolling simple moving average of dataset (SMA as signal safety vs instant drop/jumps or spread drop/jumps)\n",
    "    copyDataset = dataset.copy() # Make shallow copy to not bloat original dataset\n",
    "    copyDataset['SMA ' + searchData] = copyDataset[searchData].rolling(window = signalPeriod).mean()\n",
    "    \n",
    "    # Goes through all points to get when turn negative\n",
    "    for i in range(jumpPeriod,len(dataset[indexData])-1):\n",
    "        \n",
    "        # Range starts from jumpPeriod because SMA starts from jumpPeriod-1'th datapoint (as index starts at 0),\n",
    "        # but need the value before jumpPeriod (so +1) since that is the drop signal datapoint. \n",
    "        # Range ends at len(obsDataset)-1 if last datapoint is 'live' and pending update\n",
    "\n",
    "\n",
    "        # Error guard function (0 val input), skip step\n",
    "        if dataset[searchData][i-jumpPeriod] == 0:\n",
    "            continue\n",
    "\n",
    "        # Jump ratio is the [amount at time-index i] - [amount at i-jumpPeriod (pre-jump) time] / pre-jump value (normalisation) \n",
    "        jumpRatio = (copyDataset['SMA ' + searchData][i] - copyDataset['SMA ' + searchData][i-jumpPeriod])/copyDataset['SMA ' + searchData][i-jumpPeriod]\n",
    "        #jumpRatio = (dataset[searchData][i] - dataset[searchData][i-jumpPeriod])/dataset[searchData][i-jumpPeriod]\n",
    "        \n",
    "        # Check for value jump (above jumpThresh) or drop (below -jumpThresh), if so, record point\n",
    "        if jumpRatio > jumpThresh:\n",
    "            #print('Value jump detected: ' + str(jumpRatio*100) + '%.')\n",
    "            # Record data (as jump)\n",
    "            jumpTime = jumpTime + [dataset[indexData][i]]\n",
    "            isJump = isJump + [True]\n",
    "\n",
    "            #outDataset = outDataset + [[dataset[indexData][i], True]]\n",
    "\n",
    "        elif jumpRatio < -jumpThresh:\n",
    "            #print('Value drop detected: ' + str(-jumpRatio*100) + '%.')\n",
    "            # Record data (as drop)\n",
    "            jumpTime = jumpTime + [dataset[indexData][i]]\n",
    "            isJump = isJump + [False]\n",
    "            \n",
    "            #outDataset = outDataset + [[dataset[indexData][i], False]]\n",
    "\n",
    "    outDataset = [jumpTime, isJump]\n",
    "    # Note output is an array for efficiency\n",
    "    return outDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0db583-0d19-41a3-aee1-5ca37928f374",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing playground to add into the model evaluator\n",
    "\n",
    "\n",
    "# Note: using frozenset() for checks can make it faster for big datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be29d90-8b86-41ca-b435-1ffabfd07d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def SMIFlipTradeModel(ticker, fast_period, slow_period, signal_period, timingState = 'Close'):\n",
    "    \"\"\"This trade model trades when SMIIO of ticker becomes ('flips') to positive (long) and negative (short).\"\"\"\n",
    "\n",
    "    global histCols\n",
    "    # Create trade history dataframe\n",
    "    tradeHist = pd.DataFrame(columns = histCols)\n",
    "    \n",
    "    # Get and prep relevant dataFrame\n",
    "    df_ticker = PrepDataFrame(globals()['df_'+ticker])\n",
    "\n",
    "    # Apply SMI model to get result dataframe (and add date column)\n",
    "    df_smi = ta.momentum.smi(df_ticker['Close'],fast_period,slow_period,signal_period)\n",
    "    df_smi['Date'] = df_ticker['Date'].copy()\n",
    "    \n",
    "    smiConfigName = \"_{}_{}_{}\".format(fast_period,slow_period,signal_period)\n",
    "    # SMI + smi_name is the SMI of the stock\n",
    "    # SMIs + smi_name is the indicator made from signal line\n",
    "    # SMIo + smi_name is the oscillator made by SMI - SMIs\n",
    "    \n",
    "    smiTypeNames = [\"SMI\"+smiConfigName,\"SMIs\"+smiConfigName,\"SMIo\"+smiConfigName]\n",
    "    \n",
    "    # Removing NaN values\n",
    "    df_smi.fillna(0, inplace = True) \n",
    "    \n",
    "    buyFlip = WhenPositive(df_smi,smiTypeNames[2],'Date')\n",
    "    sellFlip = WhenNegative(df_smi,smiTypeNames[2],'Date')\n",
    "\n",
    "    # Currently only trade one stock per instance, can be made variable.\n",
    "    for i in range(0,len(buyFlip)):\n",
    "        [maketrade, tradeHist] = MakeTrade(buyFlip[i].strftime('%Y-%m-%d %H:%M:%S'), 1, ticker, history = tradeHist)\n",
    "    \n",
    "    for i in range(0,len(sellFlip)):\n",
    "        [maketrade, tradeHist] = MakeTrade(sellFlip[i].strftime('%Y-%m-%d %H:%M:%S'), 1, ticker, history = tradeHist, buy = False)\n",
    "    \n",
    "    eval = EvaluateModel(tradeHist, depth = 'AddEval', debug = 'Extended')\n",
    "    \n",
    "    return tradeHist\n",
    "\n",
    "def SuddenChangeTradeModel(ticker, changeThresh = 0.05, changePeriod = 1, revertRatio = 0.9, safetyPeriod = 0, timingState = 'Close'):\n",
    "    \"\"\"\n",
    "    This model trades when a ticker suddenly changes up (short) or down (long) by a certain amount\n",
    "    (Thresh), and 'reverts' position when it goes back up.\n",
    "    \n",
    "    Details: Can revert the up or down jumps partially using a setting (revertRatio), and can add safety factors\n",
    "    to the initation signal (safetyPeriod). Currently only takes one ticker.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create trade history (global, local seems to make issues)\n",
    "    global histCols\n",
    "    tradeHist = pd.DataFrame(columns = histCols)\n",
    "    \n",
    "    # Get relevant dataFrame and pre-process it\n",
    "    df_ticker = globals()['df_'+ticker].copy()\n",
    "    df_ticker = PrepDataFrame(df_ticker)\n",
    "\n",
    "    # Obtain timings of significant price changes ([Date of change ending, bool if its jump (up)])\n",
    "    changeTimings = JumpChecker(df_ticker, timingState, 'Date', changeThresh, jumpPeriod = changePeriod, signalPeriod = safetyPeriod)\n",
    "    # Make dummy ticker to establish repurchase date\n",
    "    df_dummy = copy.deepcopy(df_ticker)\n",
    "    \n",
    "    # Currently only trade one stock per instance, can be made variable.\n",
    "    for i in range(0,len(changeTimings[0])):\n",
    "        \n",
    "        # If jump\n",
    "        if changeTimings[1][i]:\n",
    "            # Sell now, buy later\n",
    "            [makeTrade, tradeHist] = MakeTrade(changeTimings[0][i].strftime('%Y-%m-%d %H:%M:%S'), 1, ticker, history = tradeHist, buy = False)\n",
    "\n",
    "        else: # If drop\n",
    "            # Buy now, sell later\n",
    "            [makeTrade, tradeHist] = MakeTrade(changeTimings[0][i].strftime('%Y-%m-%d %H:%M:%S'), 1, ticker, history = tradeHist)\n",
    "\n",
    "    \n",
    "    # Get the time and value of stock pre-jump after all 'procedure starting' trades are done (to be used for revert trades)\n",
    "    # Sudden change is generally referred to as Jump here for simplicity\n",
    "    # This is done at end to not enlarge list during iterations\n",
    "\n",
    "    # This is a list comprehension; result (preJumpIndex) is left (i - changePeriod) for output (i, _)\n",
    "    # (i is index num, x/t is value itself) in input variable. Can also add boolean\n",
    "    # condition after (i.e. if tradeHist['Date'] = 01-01-2023)\n",
    "    jumpTime = [x for _, x in enumerate(tradeHist['Date'])] # Note x is in str not timestamp\n",
    "    jumpIndex = [i for i, t in enumerate(df_ticker['Date']) if t.strftime('%Y-%m-%d %H:%M:%S') in jumpTime]\n",
    "    jumpValue = [df_ticker[timingState][i] for i in jumpIndex]\n",
    "    preJumpValue = [df_ticker[timingState][i - changePeriod] for i in jumpIndex]\n",
    "\n",
    "    revertValue = [jumpValue[i] + revertRatio*(preJumpValue[i] - jumpValue[i]) for i in range(0,len(jumpValue))]\n",
    "\n",
    "    # Get (spot - revert) values and check for negative/positive first swap point for each initiation trade and\n",
    "    # make a reversion trade at that point.\n",
    "    # We will put each reversion point of a trade on the initiation trade time in the dataframe\n",
    "    df_dummy['ReversionVal'] = None\n",
    "    df_dummy['ReversionDate'] = None\n",
    "    df_dummy['UnclosedTrade'] = None # To highlight unclosed trades\n",
    "    \n",
    "    for i in range(0,len(jumpIndex)):\n",
    "        # i is out of the number of jumps traded, j is the index (in the dataframe) where the trade is done.\n",
    "        j = jumpIndex[i]\n",
    "\n",
    "        # We use .loc to get j'th point in 'Reversion' column as it takes the variable in memory and not its mirror/copy.\n",
    "        df_dummy.loc[j,('ReversionVal')] = revertValue[i]\n",
    "        # df_dummy['Reversion'][i] is CHAINED INDEXING and won't work because it calls\n",
    "        # df_dummy.__getitem__('Reversion).__setitem__(i) = ... which may not be applied to df_dummy location\n",
    "        # in memory layout (as getitem) and be thrown out immediately. But, .loc dodges this by having __setitem__ only.\n",
    "        # Note: this wouldnt be an issue if the chained indexing happened on the other side (unless doing assignment?).\n",
    "\n",
    "        # The spot - reversion values (i'th value happens at index j within dataframe)\n",
    "        df_dummy['DistToRev'] = None\n",
    "        df_dummy['DistToRev'] = df_dummy['Close'][j:] - revertValue[i]\n",
    "        \n",
    "    \n",
    "        # Distance (of value at first trade) to reversion point is positive if jump (as immediately greater than\n",
    "        # reversion point), and negative if drop (immediately below reversion point).\n",
    "        # Thus, depending on first value we know what if buy or sell first, then select if WhenPositive or WhenNegative\n",
    "        if df_dummy['DistToRev'][j] > 0:\n",
    "            # Buy back now\n",
    "            # To scan and find the first point distance-to-reversion value changes sign (becomes negative)\n",
    "            scanRange = range( j, len(df_dummy['DistToRev']) )\n",
    "            reverseTrade = WhenNegative(df_dummy.iloc[scanRange],'DistToRev','Date')\n",
    "            if len(reverseTrade) != 0:\n",
    "                [makeTrade, tradeHist] = MakeTrade(reverseTrade[0].strftime('%Y-%m-%d %H:%M:%S'), 1, ticker, history = tradeHist)\n",
    "            else:\n",
    "                print('Unclosed short detected at time: ' + df_dummy['Date'][j].strftime('%Y-%m-%d %H:%M:%S'))\n",
    "                print('Unclosed trades can cause significant losses! Initial trade elected to be cancelled.')\n",
    "                df_dummy.loc[j,('UnclosedTrade')] = df_dummy['Close'][j]\n",
    "                [makeTrade, tradeHist] = MakeTrade(changeTimings[0][i].strftime('%Y-%m-%d %H:%M:%S'), 1, ticker, history = tradeHist)\n",
    "\n",
    "\n",
    "        elif df_dummy['DistToRev'][j] < 0:\n",
    "            # Sell back now\n",
    "            # To scan and find the first point distance-to-reversion value changes sign (becomes positive)\n",
    "            scanRange = range( j, len(df_dummy['DistToRev']) )\n",
    "            reverseTrade = WhenPositive(df_dummy.loc[scanRange],'DistToRev','Date')\n",
    "            if len(reverseTrade) != 0:\n",
    "                [makeTrade, tradeHist] = MakeTrade(reverseTrade[0].strftime('%Y-%m-%d %H:%M:%S'), 1, ticker, history = tradeHist, buy = False)\n",
    "            else:\n",
    "                print('Unclosed long detected at time: ' + df_dummy['Date'][j].strftime('%Y-%m-%d %H:%M:%S'))\n",
    "                print('Unclosed trades can cause significant losses! Initial trade elected to be cancelled.')\n",
    "                df_dummy.iloc[j,('UnclosedTrade')] = df_dummy['Close'][j]\n",
    "                [makeTrade, tradeHist] = MakeTrade(changeTimings[0][i].strftime('%Y-%m-%d %H:%M:%S'), 1, ticker, history = tradeHist, buy = False)\n",
    "\n",
    "        else:\n",
    "            raise ValueError('The immediate distance of price to reversion trade point should not be 0 or NaN.')\n",
    "\n",
    "    # Display graphical data of the model (when trades were done, size of trades). (Also try to display the reversion datasets.)\n",
    "    print(df_dummy.to_string())\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(df_dummy['Date'],df_dummy['ReversionVal'])\n",
    "    plt.plot(df_dummy['Date'],df_dummy['Close'])\n",
    "    plt.plot(df_dummy['Date'],df_dummy['UnclosedTrade'])\n",
    "    \n",
    "\n",
    "    eval = EvaluateModel(tradeHist, df_dummy, depth = 'AddEval', debug = 'Extended')\n",
    "\n",
    "    # AFTER THIS CREATE A MODEL CLASS AND USE IT TO CREATE A CUSTOM MODEL TYPE VARIABLE WITH CALLABLE OUTPUT VALUES\n",
    "    # AND CUSTOMISABLE INPUTS\n",
    "    \n",
    "    return tradeHist\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6fb804-56fa-498b-8ab1-75ccafdd95cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = SuddenChangeTradeModel('NVDA', 0.1, 10, 0.75, 2)\n",
    "\n",
    "#data = SMIFlipTradeModel('NVDA', 3, 10, 5)\n",
    "\n",
    "\n",
    "###\n",
    "###\n",
    "# The code seems to work as intended but I will check again, but the method seems to not work. There could be at least 3-5 reasons.\n",
    "# 1) Selling the buying (and vice versa) does not work if the stock trajectory long-term is upward as there will be cases where you cannot sell again\n",
    "# (and when forced to close position, are at a loss).\n",
    "# 2) Linked to 1) the periodicity of the model application means if applied at the wrong timeframe (granularity) i.e. each datapoint is day and\n",
    "# not hour or 15 mins, it causes loss as the trajectory of the model is more refined and there will be more dips and peaks.\n",
    "# 3) The code should be also tested for models of different trajectories (long-term, not related to granularity), as perhaps that influences the\n",
    "# outcome more than the effect of granularity.\n",
    "# 4) A general loss-stop missing, maybe the biggest losses are due to a lack of loss-stop method and a bleed in the earlier trades.\n",
    "# 5) Perhaps the method itself is statistically bad/incorrect (i.e. when a jump happens the stock is likelier than not to keep going up and not\n",
    "# reverting)\n",
    "###\n",
    "# Note that removing unclosed trades fixes all issues as hypothesized. Max. profit margin is approx. jumpRatio*reversionRatio. But issue is we cannot\n",
    "# know in advance if there will be unclosed trades unless we know the trajectory of the stock or if we use a stop-loss\n",
    "# need to record if trade is incomplete, add statistical modelling as well to assess shortfall?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ace4030-e4db-41a8-982a-97e04e65bb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "## THIS CODE IS SHELVED, NO NEED TO USE (CODE DISABLED, CAN REENABLE WITH THE BOOL SETTINGS BELOW)\n",
    "\n",
    "\n",
    "## Here we scrape data from websites or from yahoo finance api\n",
    "\n",
    "# Have them here at the end as they are not being used currently but may have some use\n",
    "# Examples of future use are: sentiment analysis, or low temporal resolution results\n",
    "# Also raw code so I do not need to immediately go through using data manifest class\n",
    "\n",
    "#############\n",
    "# User settings (to avoid looking for lines and changing manually)\n",
    "# Enable various scraping mechanisms\n",
    "enableSelenium = False\n",
    "enableyFinance = False\n",
    "enableTVS = False\n",
    "enableAlphavantage = False\n",
    "\n",
    "# Quit chrome after selenium use complete\n",
    "chromeQuit = True\n",
    "\n",
    "# Export data to a file?\n",
    "export = True\n",
    "\n",
    "###########\n",
    "# Taking direct data using Alphavantage's API of intraday and daily values\n",
    "if enableAlphavantage:\n",
    "    # Alphavantage API key (intraday upto a month length each time, limited daily request, free), no scraping\n",
    "    # Something like '69SFCX93J1H8V9K0'\n",
    "\n",
    "    # Documentation for API here: https://www.alphavantage.co/documentation/\n",
    "\n",
    "    ### Extract data manifest from folder and ignore existing datasets in API request\n",
    "    # Load manifest file (if exists, if not, create empty file)\n",
    "    try:\n",
    "        dataManifestFile = open(r'StockHistData\\dataManifest.txt',\"r\")\n",
    "    except FileNotFoundError:\n",
    "        dataManifestFile = open(r'StockHistData\\dataManifest.txt',\"w\")\n",
    "\n",
    "    # EXTRACT JSON AND \n",
    "    #textdata = dataManifestFile.read()\n",
    "\n",
    "    \n",
    "    #print(textdata) # NEED TO HAVE CODE TO CONVERT OUTPUT STRAIGHT INTO A DF OR\n",
    "    dataManifestFile.close()\n",
    "    \n",
    "    # Here we scrape past intraday stock data (not interested in testing current prices as can test it later by making it past :D )\n",
    "    dataManifestFile = open(r'StockHistData\\dataManifest.txt',\"a\")\n",
    "    \n",
    "    for symbol in dltickers:\n",
    "        for month in dlmonths:\n",
    "            for interval in dlintervals:\n",
    "                # Check data file here\n",
    "                ## NEED TO CHECK FILE DATA HERE, IF NO FILE, NEED TO CATCH NOFILEERROR AND CREATE NEW ONE AND BLANK MIDF\n",
    "                # READ FILE, IF DOESN'T EXIST, DOWNLOAD DATA AND SAVE AND ADD TO MANIFEST\n",
    "                \n",
    "                alphaURL = rf\"https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY&symbol={symbol}&interval={interval}min&month={month}&outputsize=full&apikey={alphaAPIkey} \"\n",
    "                print(alphaURL)\n",
    "                r = requests.get(alphaURL)\n",
    "                data = r.json()\n",
    "\n",
    "                # We add this dataset to the manifest, and save the data itself\n",
    "                #dataManifestFile.write()\n",
    "    \n",
    "    dataManifestFile.close()\n",
    "                \n",
    "    scrapeDF = pd.DataFrame.from_dict(data, orient='columns')\n",
    "    print(scrapeDF)\n",
    "    # Check if dataset exists before making request (to avoid wasting limited daily calls)\n",
    "\n",
    "    \n",
    "    #dataManifestFile.close()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "##########\n",
    "# Selenium scraper to get intra-day prices (not completed so doesn't get intraday prices)\n",
    "# May need higher level expertise to extract data from jscript objects \n",
    "if enableSelenium:\n",
    "    DRIVERPATH = r\"D:\\Finance Study\\chromedriver-win64\\chromedriver-win64\\ \"\n",
    "    \n",
    "    # Set Chrome options\n",
    "    options = Options()\n",
    "    options.headless = False #True # Enable headless mode (no GUI)\n",
    "    options.add_argument(\"--window-size=1920,1200\")  # Set the window size\n",
    "    \n",
    "    \n",
    "    # Init Chrome driver (I guess it's a semi-manual task?)\n",
    "    driver = webdriver.Chrome()#executable_path = DRIVERPATH)\n",
    "    \n",
    "    # Navigate to the desired page\n",
    "    for url in yUrls:\n",
    "        print(\"==================\")\n",
    "        driver.get(r''+url)\n",
    "        time.sleep(5)\n",
    "    \n",
    "    \n",
    "    # Testing here (to develop interaction code here)\n",
    "    #print(driver.page_source)\n",
    "    \n",
    "    \n",
    "    # Good practice to quit when done\n",
    "    if chromeQuit: driver.quit()\n",
    "\n",
    "##########\n",
    "# yFinance to scrape Yahoo Finance\n",
    "if enableyFinance:\n",
    "    # yFinance (Yahoo Finance Historical Data (daily))\n",
    "    # Ticker object array\n",
    "    tickObjArr = [yf.Ticker(ticker) for ticker in dltickers]\n",
    "    \n",
    "    # Fetch historical data\n",
    "    tframe = \"5d\"#\"1mo\"#\"1y\"\n",
    "    histData = [tickObj.history(period = tframe) for tickObj in tickObjArr]\n",
    "    for i in range(len(histData)):\n",
    "        print(\"Historical data for \" + tickObjArr[i].ticker + \":\")\n",
    "        print(histData[i])\n",
    "    \n",
    "    \n",
    "    # Fetch basic financial data\n",
    "    finData = [tickObj.financials for tickObj in tickObjArr]\n",
    "    for i in range(len(finData)):\n",
    "        print(\"Basic Financial data for \" + tickObjArr[i].ticker + \":\")\n",
    "        print(finData[i])\n",
    "    \n",
    "    # Fetch stock actions like dividends and splits\n",
    "    actionData = [tickObj.actions for tickObj in tickObjArr]\n",
    "    for i in range(len(actionData)):\n",
    "        print(\"\\nStock Actions for \" + tickObjArr[i].ticker +  \":\")\n",
    "        print(actionData[i])\n",
    "    \n",
    "    # Using soup\n",
    "    yUrls = [ f'https://finance.yahoo.com/quote/{ticker}/' for ticker in dltickers\n",
    "    ]\n",
    "    #print(urls)\n",
    "    r = requests.get(url=yUrls[0], headers=user_header)\n",
    "    #print(r.content)\n",
    "    \n",
    "    soup = BeautifulSoup(r.content, 'html5lib') # If this line causes an error, run 'pip install html5lib' or install html5lib\n",
    "    #print(soup.prettify())\n",
    "    #table = soup.find('div',)\n",
    "    \n",
    "    company = soup.find('h1', {'class': 'yf-xxbei9'}).text\n",
    "    #print(company)\n",
    "    closePrice = soup.find('div', {'class': 'stx-btn-panel stx-show'})\n",
    "    print(closePrice)\n",
    "    closePrice = soup.find('span', {'class': 'stx-ico-close'})\n",
    "    print(closePrice)\n",
    "\n",
    "\n",
    "\n",
    "##########\n",
    "# Tradingview Scraper\n",
    "if enableTVS:\n",
    "    \n",
    "    # Ideas scraper\n",
    "    # Ideas are the tab in the webpage with articles of sorts\n",
    "    # Initialize the Ideas scraper with default parameters\n",
    "    \n",
    "    # Default: export_result=False, export_type='json'\n",
    "    ideas_scraper = Ideas(\n",
    "      export_result=True,  # Set to True to save the results\n",
    "      export_type='csv'    # Specify the export type (json or csv)\n",
    "    )\n",
    "    \n",
    "    # Default symbol: 'BTCUSD'\n",
    "    # Scrape ideas for the NVDA symbol, from page 1 to page 1\n",
    "    ideas = ideas_scraper.scrape(\n",
    "      symbol=\"NVDA\",\n",
    "      startPage=1,\n",
    "      endPage=1,\n",
    "      sort=\"popular\"  #  Could be 'popular' or 'recent'\n",
    "    )\n",
    "    \n",
    "    #print(\"Ideas:\", ideas)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##########\n",
    "    # Indicators\n",
    "    from tradingview_scraper.symbols.technicals import Indicators\n",
    "    \n",
    "    # Scrape all indicators for the BTCUSD symbol\n",
    "    indicators_scraper = Indicators(export_result=True, export_type='json')\n",
    "    indicators = indicators_scraper.scrape(\n",
    "        symbol=\"BTCUSD\",\n",
    "        timeframe=\"4h\",\n",
    "        allIndicators=True\n",
    "    )\n",
    "    #print(\"All Indicators:\", indicators)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
