{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc6fdd67-383f-4bc2-bbca-2bfb8e819e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile __init__.py\n",
    "###### -- Python Script in Jupyter to access, create and test financial data and models\n",
    "###### -- By Ahmed Asiliskender, initial write date 25 June 2024\n",
    "###### -- May also access MATLAB scripts through here and .py files.\n",
    "\n",
    "### Here we initialise important libraries and variables.\n",
    "\n",
    "## To download packages using pip\n",
    "import sys #! allows to use command terminal code in here\n",
    "#!{sys.executable} --version\n",
    "#!pip install html5lib\n",
    "#!pip install bs4\n",
    "#!pip install yfinance\n",
    "#!pip install tradingview-scraper\n",
    "#!pip install --upgrade --no-cache tradingview-scraper\n",
    "#!pip install selenium\n",
    "#!pip install sqlalchemy\n",
    "#!pip install python-dotenv\n",
    "#!pip install pandas-ta\n",
    "#!pip install pytest\n",
    "#!pip install python-on-whales\n",
    "# Security testing\n",
    "#!pip install bandit\n",
    "\n",
    "# Cmd terminal environment install (psycopg2)\n",
    "#!pip install psycopg2-binary \n",
    "# Conda environment install\n",
    "#!conda install -c anaconda psycopg2 \n",
    "\n",
    "\n",
    "\n",
    "# Import pandas (python data analysis lib) and data analysis packages\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "#user_header = {'user-agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) \\\n",
    "#                                AppleWebKit/537.36 (KHTML, like Gecko) \\\n",
    "#                                Chrome/122.0.0.0 Safari/537.36'}\n",
    "\n",
    "# Webscrape libs\n",
    "from bs4 import BeautifulSoup\n",
    "import yfinance as yf\n",
    "import tradingview_scraper as tvs\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from tradingview_scraper.symbols.ideas import Ideas\n",
    "\n",
    "# Other libs (system, graphical or time-compute analysis)\n",
    "import os\n",
    "from io import StringIO\n",
    "import psycopg2\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from dotenv.main import set_key\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy import types as sqltype\n",
    "import sqlalchemy.exc as sqlexc\n",
    "from colorama import Fore, Back, Style\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import time\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from ast import literal_eval\n",
    "import json\n",
    "import unittest\n",
    "import pytest\n",
    "\n",
    "#import warnings\n",
    "\n",
    "\n",
    "# My libs\n",
    "from arcanequant.quantlib.DataManifestManager import DataManifest\n",
    "from arcanequant.quantlib.DataManifestManager import DownloadIntraday # NEED TO MOVE THIS TO DATA MANAGER\n",
    "from arcanequant.quantlib.SQLManager import SetKeysQuery, DropKeysQuery, ExecuteSQL\n",
    "\n",
    "# Paid APIs, (not used, left here)\n",
    "\n",
    "# Bloomberg (not free)\n",
    "#!pip install blpapi --index-url=https://blpapi.bloomberg.com/repository/releases/python/simple/ blpapi\n",
    "#!pip install xbbg\n",
    "#from xbbg import blp\n",
    "#blp.bdh( tickers='SPX Index', flds=['High', 'Low', 'Last_Price'], start_date='2018-10-10', end_date='2018-10-20')\n",
    "#blp.bdp('AAPL US Equity', 'Eqy_Weighted_Avg_Px', VWAP_Dt='20181224')\n",
    "#blp.bdp(tickers='NVDA US Equity', flds=['Security_Name', 'GICS_Sector_Name'])\n",
    "\n",
    "\n",
    "### RAPID API FOR INTRADAYS (not free)\n",
    "#import http.client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a8de56-48a1-4076-a32d-4502bacb6379",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77f18e65-bb22-43e3-b8da-4b6e68aff79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#conn = http.client.HTTPSConnection(\"yahoo-finance127.p.rapidapi.com\")\n",
    "\n",
    "#headers = {\n",
    "#    'x-rapidapi-key': \"2e7bf1e71cmsh8f7a5babc8f5197p1f02bejsn72107f046a86\",\n",
    "#    'x-rapidapi-host': \"yahoo-finance127.p.rapidapi.com\"\n",
    "#}\n",
    "\n",
    "#conn.request(\"GET\", \"/finance-analytics/nvda\", headers=headers)\n",
    "\n",
    "#res = conn.getresponse()\n",
    "#data = res.read()\n",
    "\n",
    "#print(data.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f16ac43-8712-42fd-8907-04f335f8ef4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main\n"
     ]
    }
   ],
   "source": [
    "#%%writefile main.py\n",
    "\n",
    "def main():\n",
    "    print('main')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00f6a8f6-0f27-450c-b312-8ae6e689abde",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#%%writefile DataManifestManager_new.py\n",
    "# CREATE A COMPRESSED MANIFEST (YEARS AND STOCKS ONLY, 2 INDICATES AN INCOMPLETE POINT (I.E. NOT ALL MONTHS OR NOT ALL INTERVALS)\n",
    "\n",
    "# VALIDATION IS STILL DONE ONLY VIA CSV FILES\n",
    "# MAYBE CAN EXTEND VALIDATION TO RELY ALSO ON DATABASE (AND MAYBE HAVE A MANIFEST TABLE IN DATABASE?)\n",
    "\n",
    "# MODIFY DOWNLOADINTRADAY TO ADD SAVE SETTING (SAVE AS SQL OR JSON OPTIONS, AND MAYBE ADD CSV OPTION)\n",
    "\n",
    "# CONSIDERING TIMEZONE CASTING (FOR LATER) - FUNCTION NAMED CONTEXTUALISE (I.E. USING THE META DATA)?\n",
    "\n",
    "# CREATE DATAMANAGER FILE/CLASS TO MANAGE DATA SPECIFICALLY (RATHER THAN JUST MANIFEST OR MANIFEST-RELATED STUFF)\n",
    "# SHOULD CONTAIN DOWNLOAD INTRADAY AND EXTRACT DATA (CAN ALSO HAVE SAVE DATA WHICH POINTS TO SQLSAVE OR SAVECSV)\n",
    "# MAKE A FUNCTION TO SAVE A FILE TO CSV? (WITH ADDITIONAL PARAMETERS? CURRENTLY NOTHING TO CALL TO SAVE TO CSV)\n",
    "\n",
    "# ADD CODE TO STITCH TOGETHER SPECIFIC DATA PARTS FROM DIFFERENT SETS FOR ANALYSIS (CORRELATION ETC.)\n",
    "# ADD CODE TO DIRECTLY API CALL POST-PROCESS DATA (TECH INDICATORS) (ALSO MAKE CODE TO PROCESS IN HOUSE IF DESIRED)\n",
    "# WILL NEED CODE TO ASSESS ANY STOCKSPLIT INFORMATION AND EITHER MARK FOR RENEW DATA FROM API OR EDIT EXISTING DATA AS NEEDED\n",
    "\n",
    "# Placeholder class for file name and package importing\n",
    "class DataManifestManager():\n",
    "    \"\"\"Placeholder class for package-level structure (or possibly also future use).\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "##################################\n",
    "##################################\n",
    "# Data Manifest Class\n",
    "class DataManifest():\n",
    "    \"\"\"\n",
    "    Data Manifests show in a readable format the set of data that exists in the directory the file is in.\n",
    "    This allows for easier downloading of missing data as necessary.\n",
    "    This class also has functions and methods to easily modify values as needed as well as verifying the presence of files.\n",
    "    The data represented by the manifest can be loaded from its .csv form or from its database (SQL) form.\n",
    "    The data manifest dataframe itself can be saved or loaded from .json form or from its database (SQL) form.\n",
    "    Note: Consider manifest size and data size (of the range of data you use), SQL is better at working with very large datasets.\n",
    "    ---\n",
    "    Value list:\n",
    "    0 - No file exists\n",
    "    1 - File exists\n",
    "    2 - File exists but incomplete (as file covers current time or not updated after month ended)\n",
    "    Note: 2 is currently marked by you manually, and if validation finds an unmarked file, it assumes it is complete unless stated otherwise.\n",
    "    ---\n",
    "    _____\n",
    "    Method List:\n",
    "    - loadManifest - Loads manifest from a .json file into the Data Frame in this class\n",
    "    - saveManifest - Saves manifest in this class from DataFrame into a .json file \n",
    "    - setValue - Sets (or adds) a given value in the manifest\n",
    "    - validateManifest - Checks the files (or lack thereof) indicated by the manifest\n",
    "    - reduceManifest - Culls and rows and columns full of zeroes\n",
    "    - loadData_fromcsv - Loads actual data (of point indicated in manifest) from .csv file\n",
    "    - loadData_fromsql - Loads actual data (of point indicated in manifest) from database\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self):\n",
    "        self.DF = pd.DataFrame(index = pd.MultiIndex(levels = [[],[]], codes = [[],[]], names=['Ticker','Interval']), columns = pd.Index(data = [],name = 'Month'))\n",
    "         # TODO: CHECK VALIDITY OF SHORTFORM MANIFEST\n",
    "        self.DFshort = pd.DataFrame(index = pd.Index(data = [],name = 'Ticker'), columns = pd.Index(data = [],name = 'Month'))\n",
    "        self.directory = None\n",
    "        self.fileName = 'dataManifest'\n",
    "        self.SQLengine = None\n",
    "        \n",
    "        print('Data Manifest Initialised')\n",
    "\n",
    "\n",
    "    # Validate Manifest Data (check if file exists, add to manifest or set to 1, else set to 0 or remove)\n",
    "    def validateManifest(self, fastValidate = True, echo = True):\n",
    "        \"\"\"\n",
    "        This method validates the DataManifest's DataFrame.\n",
    "        This is done by comparing the stated intraday data file's presence (or lack thereof) in the DataManifest's indicated directory path.\n",
    "        The files are expected to be named as \"{ticker}_{interval}_{month}.csv\" where month is YYYY-MM (\"2025-01\")\n",
    "        \"\"\"\n",
    "        print('Validating data manifest DataFrame.')\n",
    "        \n",
    "        if fastValidate: print('Conducting fast validation.')\n",
    "        invalidpoints=[0,0] # Left is invalid, right is total\n",
    "        # Here we need to separate the stock values, and for each, separate the interval value.\n",
    "        # For each of these, check if the file exists:\n",
    "        # If fastValidate, for only the month entries which are stated as 1 (exists).\n",
    "        # This is because ignored files (set to 0) will be redownloaded (or searched if exists) later anyways.\n",
    "        # Otherwise, for every month entry, and set the value accordingly.\n",
    "        \n",
    "        # Unique values of symbols (ticker)\n",
    "        symbolComponents = list(self.DF.index.get_level_values(0).unique().values)\n",
    "        # We get these now since it doesn't change across tickers or intervals\n",
    "        monthComponents = list(self.DF.columns.values)\n",
    "        \n",
    "        for symbol in symbolComponents:\n",
    "            symbolSection = self.DF[(self.DF.index.get_level_values('Ticker') == symbol)]\n",
    "            # Will try for each interval list in each ticker set (to avoid having to catch errors)\n",
    "            intervalComponents = list(symbolSection.index.get_level_values(1).unique().values) \n",
    "            \n",
    "            for interval in intervalComponents:\n",
    "                for month in monthComponents:\n",
    "                    # Validation operation here:\n",
    "                    # 0 - No file exists\n",
    "                    # 1 - File exists\n",
    "                    # 2 - File exists but incomplete (as file covers current time or not updated after month ended)\n",
    "                    # Note: 2 is only marked as so elsewhere and assume all files are complete unless stated otherwise\n",
    "    \n",
    "                    fileValue = self.DF.loc[((symbol,interval),month)]\n",
    "                    if not (fileValue == 0 or fileValue == 1 or fileValue == 2):\n",
    "                        raise ValueError('Manifest has a value that is not 0, 1 or 2. Values are Ticker: ' + symbol + \", Interval: \" + str(interval) + \", Month: \" + month)\n",
    "                    \n",
    "                    if fileValue == 1 or fileValue == 2:\n",
    "                        # Check if file exists, if so, leave value, otherwise (error), set to 0.\n",
    "                        invalidpoints[1] += 1 # Adding to no. of tested datapoints\n",
    "                        try:\n",
    "                            fileString = r'' + symbol + \"_\" + str(interval) + \"_\" + month\n",
    "                            if echo: print('Searching for file ' + fileString)\n",
    "                            \n",
    "                            fileRead = self.loadData_fromcsv(symbol, interval, month, echo = echo)\n",
    "                        except FileNotFoundError:\n",
    "                            print('File ' + fileString + ' not found when stated to exist.')\n",
    "                            self.DF.loc[((symbol,interval),month)] = 0\n",
    "                            invalidpoints[0] += 1 # Adding to no. of invalid datapoints\n",
    "                        else:\n",
    "                            if echo: print('File ' + fileString + ' found.')\n",
    "                        \n",
    "                    elif fileValue == 0 and not fastValidate:\n",
    "                        # Check if file does NOT exist if value zero AND full validate is on.\n",
    "                        # Extra lines but more efficient\n",
    "                        invalidpoints[1] += 1 # Adding to no. of tested datapoints\n",
    "                        try:\n",
    "                            fileString = r'' + symbol + \"_\" + str(interval) + \"_\" + month\n",
    "                            if echo: print('Searching for file ' + fileString)\n",
    "                            \n",
    "                            fileRead = self.loadData_fromcsv(symbol, interval, month, echo = echo)\n",
    "                        except FileNotFoundError:\n",
    "                            if echo: print('File ' + fileString + ' not found.')\n",
    "                        else:\n",
    "                            print('File ' + fileString + ' found when stated to not exist.')\n",
    "                            self.DF.loc[((symbol,interval),month)] = 1 # TODO: REFACTOR THIS WHAT ABOUT 2, AND WHY NOT USING setValue???\n",
    "                            invalidpoints[0] += 1 # Adding to no. of invalid datapoints\n",
    "\n",
    "        print(f\"Number of invalid/tested datapoints in manifest: {invalidpoints[0]}/{invalidpoints[1]}\")\n",
    "        return\n",
    "\n",
    "    # Method to create and link an SQL connection engine and link to class instance\n",
    "    def connectSQL(self, dbcred = 'SQLlogin'):\n",
    "        \"\"\"\n",
    "        Creates connection engine and links to class instance (self.SQLengine) for the database given the requisite details.\n",
    "        You must have already set up SQL and a database to use this functionality.\n",
    "\n",
    "        Input:\n",
    "        - dbcred is the name of the file (string, not including file extension) containing the connection details.\n",
    "\n",
    "        The file must be an .env file with the following keys:\n",
    "        DRIVER - the software dealing with the database\n",
    "        DIALECT - the specific language specification for SQL (i.e. MySQL or PostgreSQL)\n",
    "        DB_USER - username to access database\n",
    "        PASSWORD - password linked to username\n",
    "        HOST_MACHINE - the machine to connect to (containing the database)\n",
    "        DBNAME - name of the database (must already exist)\n",
    "\n",
    "        The .env is normally readily creatable/editable if file extensions can be changed manually by the user.\n",
    "\n",
    "        Example:\n",
    "        DRIVER=psycopg2:\n",
    "        DIALECT=postgresql\n",
    "        DB_USER=myuser\n",
    "        PASSWORD=mypass\n",
    "        HOST_MACHINE=localhost\n",
    "        PORT=5432\n",
    "        DBNAME=databasename\n",
    "        \"\"\"\n",
    "\n",
    "        env_path = Path(\".\") / f\"{dbcred}.env\" # Environment variables file must be same folder as this code\n",
    "        load_dotenv(dotenv_path=env_path, override=True)\n",
    "        if not (os.path.isfile(env_path)):\n",
    "            raise FileNotFoundError(f\"The environment file ({env_path}) does not exist\")\n",
    "\n",
    "        driver = os.getenv(\"DRIVER\")\n",
    "        dialect = os.getenv(\"DIALECT\")\n",
    "        username = os.getenv(\"DB_USER\") # Not USERNAME as that aLready exists in OS environment variables and it is not wise to overwrite it\n",
    "        password = os.getenv(\"PASSWORD\")\n",
    "        host_machine = os.getenv(\"HOST_MACHINE\")\n",
    "        port = os.getenv(\"PORT\")\n",
    "        dbname = os.getenv(\"DBNAME\")\n",
    "    \n",
    "        if not all([driver, dialect, username, password, host_machine, port, dbname]):\n",
    "            raise EnvError('Environment variables are not all provided.')\n",
    "\n",
    "        connstring = f\"{dialect}+{driver}//{username}:{password}@{host_machine}:{port}/{dbname}\"\n",
    "        \n",
    "                                     #\"dialect+driver//username:password@hostname:portnumber/databasename\") \n",
    "        self.SQLengine = create_engine(connstring)\n",
    "        print(f\"Connecting to engine: {self.SQLengine}\")\n",
    "        # Note: Code fails if no/wrong database (OperationalError)\n",
    "        return\n",
    "    \n",
    "    # Method to reduce manifest (remove completely 0 rows and columns)\n",
    "    def reduceManifest(self):\n",
    "        \"\"\"This method culls any rows and columns full of zeroes.\"\"\"\n",
    "        print('Culling manifest size')\n",
    "\n",
    "        # Convert all zeroes to NaNs, and use dropna method, then re-fill with fillna(0)\n",
    "        self.DF[self.DF == 0] = None\n",
    "\n",
    "        self.DF.dropna(how='all', inplace=True)\n",
    "        self.DF.dropna(axis=1, how='all', inplace=True)\n",
    "        \n",
    "        self.DF.fillna(0, inplace=True)\n",
    "        return\n",
    "\n",
    "    # Method to update manifest (adds columns/index rows as necessary)\n",
    "    def setValue(self, ticker, interval, month, value, sort = True):\n",
    "        \"\"\" This method updates a value in the DataManifest's DataFrame.\n",
    "        The method adds columns/indices as necessary.\"\"\"\n",
    "        if value != 0 and value != 1 and value != 2: raise ValueError('Manifest values must be set to 0, 1 or 2')\n",
    "\n",
    "        # If any of the symbol, interval (for the symbol) and month values are new, fill all NaNs as 0 in the new rows/cols\n",
    "        uniqueSymbols = list(self.DF.index.get_level_values(0).unique().values)\n",
    "        uniqueMonths = list(self.DF.columns.values)\n",
    "        \n",
    "        symbolSection = self.DF[(self.DF.index.get_level_values('Ticker') == ticker)]\n",
    "        uniqueIntervals = list(symbolSection.index.get_level_values(1).unique().values) \n",
    "\n",
    "        isnewRowCol = False\n",
    "        if (ticker not in uniqueSymbols) or (month not in uniqueMonths) or (interval not in uniqueIntervals): isnewRowCol = True \n",
    "        \n",
    "        \n",
    "        self.DF.loc[((ticker,interval),month)] = int(value)\n",
    "\n",
    "        if isnewRowCol: self.DF.fillna(int(0), inplace=True)\n",
    "\n",
    "        if sort: # Sort before updating\n",
    "            self.DF.sort_values(by=['Ticker','Interval'], inplace=True)\n",
    "            self.DF.sort_values(by=['Month'], axis=1, inplace=True)\n",
    "        \n",
    "        return\n",
    "\n",
    "    # Method to load .csv market data based on the path of the class, and inputted parameters (ticker, interval, month).\n",
    "    def loadData_fromcsv(self, ticker, interval, month, convert_DateTime = False, echo = True):\n",
    "        \"\"\" This method loads a .csv file of stock data, based on the path of the class, and inputted parameters (ticker, interval, month)\n",
    "        The method assumes the file naming format \"{ticker}_{interval}_{month}.csv\" where month is YYYY-MM (\"2025-01\") on the .csv files \n",
    "        The method returns the data frame of stock data.\n",
    "\n",
    "        Input:\n",
    "        - ticker - string of ticker name (i.e. \"NVDA\")\n",
    "        - interval - int of interval value (time gap between datapoints)\n",
    "        - month - string of the year and month of the period (in YYYY-MM format, i.e. \"2025-01\")\n",
    "\n",
    "        Optional inputs:\n",
    "        - convert_DateTime - Boolean indicating to read the DateTime column as datetime64 type (or as a string)\n",
    "        - echo - Boolean that provides more output during execution.\n",
    "        \"\"\"\n",
    "        if not isinstance(self.directory, str): raise TypeError('The data manifest directory pointer must be a string pointing to a valid path/folder.')\n",
    "        \n",
    "        fileString = r'' + ticker + \"_\" + str(interval) + \"_\" + month\n",
    "        if echo: print(rf\"Loading file data: {fileString}.csv\")\n",
    "        \n",
    "        fileRead = pd.read_csv(rf\"{self.directory}{ticker}/{fileString}.csv\")\n",
    "        # Convert datetime column from string to datetime64\n",
    "        if convert_DateTime:\n",
    "            fileRead['DateTime'] = pd.to_datetime(fileRead['DateTime'], format = \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        return fileRead\n",
    "\n",
    "    # Method to load database market data based on the path of the class, and inputted parameters (ticker, interval, month).\n",
    "    def loadData_fromsql(self, ticker, interval, month, echo = True):\n",
    "        \"\"\" This method loads a part of the database of stock data, based on the path of the class, and inputted parameters (ticker, interval, month)\n",
    "        The method assumes the postgreSQL information from []\n",
    "        The method returns the data frame of stock data.\n",
    "        \"\"\"\n",
    "        \n",
    "        fileRead=0\n",
    "\n",
    "        return fileRead\n",
    "    \n",
    "    # Method to save manifest data into file\n",
    "    def saveManifest(self, path, save = 'both', echo = True):\n",
    "        \"\"\"This method saves the DataFrame in the DataManifest class into a file with the given directory.\n",
    "        The save format is .json\"\"\"\n",
    "        \n",
    "        if not isinstance(path, str): raise TypeError('You must provide a valid path to save or load.')\n",
    "        \n",
    "        if echo: print('Saving Manifest Data')\n",
    "        \n",
    "\n",
    "        # Update class-file link details\n",
    "        self.directory = path\n",
    "        filepath = rf\"{path}{self.fileName}.json\"\n",
    "        if echo: print('Save path/name: ' + filepath)\n",
    "        \n",
    "        # Sort before saving\n",
    "        self.DF.sort_values(by=['Ticker','Interval'], inplace=True)\n",
    "        self.DF.sort_values(by=['Month'], axis=1, inplace=True)\n",
    "        \n",
    "        manifestJSON = self.DF.to_json()\n",
    "        \n",
    "        ### Saving is done after we have read the manifest so we don't lose any data\n",
    "        # Saving to .json file (always)\n",
    "        \n",
    "        # Using the with statement to avoid file close errors (though it shouldn't occur) for one-step changes\n",
    "        with open(filepath,\"w\") as manifestSave:\n",
    "            # Indent works if we use json.loads to change into dict as dump indents properly with that\n",
    "            # Works without indent but is not human-readable\n",
    "            readableJSON = json.loads(manifestJSON)\n",
    "            json.dump(readableJSON, manifestSave, indent = 4)\n",
    "            if echo: print(f\"JSON saved successfully to {filepath}\")\n",
    "\n",
    "        # Saving to SQL database (if possible)\n",
    "        # Save if the SQLengine is already established, if not try connect to SQL first\n",
    "        # If run into errors, cancel this operation\n",
    "        skipEngine = False\n",
    "        if self.SQLengine is None:\n",
    "            try:\n",
    "                if echo: print('No SQL connection, attempting to create connection engine...')\n",
    "                self.connectSQL()\n",
    "                if echo: print('Connection engine created')\n",
    "            except Exception as e:\n",
    "                skipEngine = True\n",
    "                print(f\"An error occurred, skipping connection engine creation: {e}\")\n",
    "                \n",
    "        if not skipEngine: # Run if engine already exists or just created \n",
    "            self.DF.to_sql('manifest', self.SQLengine, if_exists='replace')\n",
    "        \n",
    "        return\n",
    "\n",
    "\n",
    "    # Method to load manifest data and convert into Multi-Index DataFrame\n",
    "    def loadManifest(self, path, echo = True):\n",
    "        \"\"\"This method loads up a manifest file into the DataManifest class' DataFrame attribute.\"\"\"\n",
    "    \n",
    "        if not isinstance(path, str): raise TypeError('You must provide a valid path to save or load.')\n",
    "        \n",
    "        if echo: print('Loading Manifest Data')\n",
    "\n",
    "        # Update class-file link details\n",
    "        self.directory = path\n",
    "        filepath = path + self.fileName + '.json'\n",
    "        if echo: print('Load path/name: ' + filepath) \n",
    "        \n",
    "        # Here we load the .json file before converting into MIDF\n",
    "        with open(filepath,) as manifestLoad: # Default is read mode\n",
    "            # Note: manifestJSON is the string version of loadJSON (dict form), difference is None is null in JSON form\n",
    "            loadJSON = json.load(manifestLoad)\n",
    "        \n",
    "        # After loading json (dict form and None) need to convert to json form (string form and null) to use in pandas\n",
    "        stringJSON = json.dumps(loadJSON)\n",
    "        \n",
    "        # Here we read the JSON form to convert into MIDF and process it to return to original form\n",
    "        # We don't use json.load as we parse into DF not dict\n",
    "        self.DF = pd.read_json(StringIO(stringJSON))\n",
    "        \n",
    "        # Convert the strings of the 'tuples' in the index into a tuple and put into list to recreate the MultiIndex\n",
    "        indexlist=[literal_eval(x) for x in self.DF.index]\n",
    "        \n",
    "        # Change index from fake tuple into MultiIndex (and rename index axes) \n",
    "        self.DF.index = pd.MultiIndex.from_tuples(indexlist,names=['Ticker','Interval'])\n",
    "        \n",
    "        # Rename the column axis to Month\n",
    "        self.DF.rename_axis(\"Month\",axis=1,inplace = True)\n",
    "    \n",
    "        # Convert the full datetime month representation to only Year-Month (while keeping it string)\n",
    "        # Code works if column is both originally string or datetime representation\n",
    "        if isinstance(self.DF.columns, pd.Index):\n",
    "            self.DF.columns = pd.to_datetime(self.DF.columns,format=\"%Y-%m\")\n",
    "            self.DF.columns = self.DF.columns.strftime(\"%Y-%m\")\n",
    "        elif isinstance(self.DF.columns, pd.DatetimeIndex):\n",
    "            self.DF.columns = self.DF.columns.strftime(\"%Y-%m\")\n",
    "        \n",
    "        # Replace any NaNs with zeroes\n",
    "        self.DF.fillna(0, inplace=True)\n",
    "    \n",
    "        # Convert all values to int (1 or 0)\n",
    "        for column in self.DF.columns:\n",
    "            self.DF[column] = self.DF[column].astype(int)\n",
    "        \n",
    "        return\n",
    "\n",
    "##################################\n",
    "##################################\n",
    "\n",
    "##################################\n",
    "##################################\n",
    "# Direct Functions (may incorporate them into some other library or class later)\n",
    "def DownloadIntraday(path, tickers, intervals, months, APIkey, verbose = False):\n",
    "    \"\"\"This function downloads monthly intraday stock data for a given stock, interval and month.\n",
    "    The requested data is saved in a directory, and the manifest file in the directory is updated/created.\n",
    "    The data is directly requested from Alphavantage's API, and requires an API key (free one obtainable).\n",
    "    Documentation for API here: https://www.alphavantage.co/documentation/\n",
    "    The primary data file is saved in a .csv format with a naming format \"{ticker}_{interval}_{month}.csv\".\n",
    "    The meta data of the file is saved in a .csv format similar to the primary file, with the added suffix '_meta'\n",
    "    The intraday data is only available for equities listed on US exchanges.\n",
    "    \n",
    "    The input is as follows:\n",
    "    - path is the directory into which the intraday data is saved and the data manifest is created or updated.\n",
    "    - manifestFile is the name of the manifest file to be updated/created \n",
    "    - tickers are the list of typical stock string (normally 4 characters) i.e. Microsoft is \"MSFT\"\n",
    "    - intervals is a list of int with time resolution options 1, 5, 15, 30 and 60 mins.\n",
    "    - months to request are in a list of strings in \"YYYY-MM\" format, i.e. \"2025-01\"\n",
    "    - verbose is a bool setting to detail the input, process and/or outputs\n",
    "    \"\"\"\n",
    "\n",
    "    dataManifest = DataManifest()\n",
    "    \n",
    "    try:\n",
    "        dataManifest.loadManifest(path)\n",
    "    except FileNotFoundError:\n",
    "        pass # Use the empty MIDF to start with if no file\n",
    "    \n",
    "    # Here we scrape past intraday stock data\n",
    "    # For now, not interested in testing current prices as can test it later by making it past :D\n",
    "    # But need to have a way of retrieving live data to guide my trading decisions\n",
    "\n",
    "    for month in months:\n",
    "        # If current month (today) is same as month requested the dataset will always be updated from API\n",
    "        # If not, then if data is incomplete (value 2 in manifest) or missing (value 0), request, otherwise don't (as we already have it)\n",
    "        currentMonth = pd.to_datetime(datetime.now(),format=\"%Y-%m\")\n",
    "        currentMonth = currentMonth.strftime(\"%Y-%m\")\n",
    "        update = False\n",
    "            \n",
    "        for symbol in tickers:\n",
    "            for interval in intervals:\n",
    "                \n",
    "                # Check if dataset exists before making request (to avoid wasting limited daily calls)\n",
    "                try:\n",
    "                    # Check data file here (can either try load directly or check manifest)\n",
    "                    dataVal = dataManifest.DF.loc[((symbol,interval),month)] # KeyError\n",
    "                except KeyError:\n",
    "                    print('Data file not indicated in manifest, requesting from API and saving')\n",
    "                    update = True\n",
    "\n",
    "                else:\n",
    "                    # Since we have a value in the manifest (no KeyError), we check what the number is (0, 1, or 2)\n",
    "                    if dataVal == 0:\n",
    "                        update = True\n",
    "                        print('Data file indicated missing in manifest, requesting from API and saving')\n",
    "\n",
    "                    # Even if dataVal is 1, if we request current month, it should always update (shouldn't be 1 in the first place)\n",
    "                    elif dataVal == 1 and month != currentMonth:\n",
    "                        print('Data file existence indicated in manifest, skipping.')\n",
    "                        update = False\n",
    "                    else: # Only remaining option is dataVal is 2 and/or requested month is current month\n",
    "                        print('Data file indicated incomplete, updating from API.')\n",
    "                        update = True\n",
    "            \n",
    "                finally:\n",
    "                    # Now we decide to request/update the data from the API (or not)\n",
    "                    if update:\n",
    "                        # Taking direct data using Alphavantage's API of intraday (and can even do daily values)\n",
    "                        alphaURL = rf\"https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY&symbol={symbol}&interval={interval}min&month={month}&outputsize=full&apikey={APIkey} \"\n",
    "                        if verbose: print(alphaURL)\n",
    "                        \n",
    "                        # Requesting data from url\n",
    "                        r = requests.get(alphaURL)\n",
    "        \n",
    "                        # Reading data as json format (dictionary)\n",
    "                        data = r.json()\n",
    "\n",
    "                        # Raise error if API call limit reached or if another error message:\n",
    "                        if \"Information\" in data:\n",
    "                            errorStr = data[\"Information\"]\n",
    "                            raise APIError(f\"API call limit reached (call: {symbol}, {interval}min, {month}). Statement from API: {errorStr}\")\n",
    "                        elif \"Error Message\" in data:\n",
    "                            errorStr = data[\"Error Message\"]\n",
    "                            raise APIError(f\"Invalid API call ({symbol}, {interval}min, {month}), check IPO of ticker. Statement from API: {errorStr}\")\n",
    "                        elif \"Note\" in data:\n",
    "                            errorStr = data[\"Note\"]\n",
    "                            raise APIError(f\"Unintended response ({symbol}, {interval}min, {month}). Statement from API: {errorStr}\")\n",
    "                        elif \"Meta Data\" not in data:\n",
    "                            raise APIError(f\"The API response does not contain data. Check the API output here: {data}\")\n",
    "                        \n",
    "                        # Converting the dictionary form to DataFrame\n",
    "                        scrapeDF = pd.DataFrame.from_dict(data, orient='columns')\n",
    "                        print(scrapeDF)\n",
    "                        \n",
    "                        # This DF contains both meta data and actual data, must split them up first\n",
    "                        datalabel = f'Time Series ({interval}min)' # To get the header of the actual data\n",
    "                        \n",
    "                        metaDF = scrapeDF[['Meta Data']].dropna(subset = ['Meta Data'])\n",
    "                        dataDF = scrapeDF[[datalabel]].dropna(subset = [datalabel])\n",
    "        \n",
    "                        # Actual data post processing (renaming columns etc.)\n",
    "                        dateTimeCol = dataDF.index # Save datetime index to add later as a column\n",
    "                        dataDF = pd.DataFrame(list(dataDF[datalabel]))\n",
    "                        dataDF['DateTime'] = dateTimeCol\n",
    "        \n",
    "                        dataDF.rename(columns={'1. open' : 'Open', '2. high' : 'High', '3. low' : 'Low', '4. close' : 'Close', '5. volume' : 'Volume'}, inplace=True)\n",
    "                        dataDF = dataDF[['DateTime','Open','High','Low','Close','Volume']] # Reordering columns\n",
    "        \n",
    "                        # Show meta and actual data (if show enabled)\n",
    "                        if verbose:              \n",
    "                            print('Meta Data:')\n",
    "                            print(metaDF)\n",
    "                            print('Actual Data DataFrame')\n",
    "                            print(dataDF)\n",
    "                        \n",
    "                        # Save Meta Data with index (and excepting a possible lack of folder)\n",
    "                        try:\n",
    "                            metaDF.to_csv(rf\"{path}{symbol}/{symbol}_{interval}_{month}_meta.csv\",index=True)\n",
    "                        except OSError: # If no folder to save into, create it\n",
    "                            # Specify the directory\n",
    "                            new_directory_path = Path(rf\"{path}{symbol}\")\n",
    "                            # Create the directory\n",
    "                            try:\n",
    "                                new_directory_path.mkdir()\n",
    "                                print(f\"New folder '{new_directory_path}' created successfully.\")\n",
    "                            except FileExistsError: # This error should not occur\n",
    "                                pass\n",
    "                            except PermissionError:\n",
    "                                print(f\"Permission denied: Unable to create new directory '{new_directory_path}'.\")\n",
    "                            except Exception as e:\n",
    "                                print(f\"An error occurred: {e}\")\n",
    "                            finally: # Try again (even if other errors)\n",
    "                                metaDF.to_csv(rf\"{path}{symbol}/{symbol}_{interval}_{month}_meta.csv\",index=True) \n",
    "                        \n",
    "                        # No need to save index numbering with intraday data\n",
    "                        dataDF.to_csv(rf\"{path}{symbol}/{symbol}_{interval}_{month}.csv\",index=False)\n",
    "\n",
    "                        # Decide to put 1 or 2 in manifest (based on if requested month is the present month) \n",
    "                        newValue = 1\n",
    "                        if month == currentMonth: newValue = 2\n",
    "                            \n",
    "                        dataManifest.setValue(symbol, interval, month, newValue, sort = False) # No sort to save time\n",
    "                        dataManifest.saveManifest(path, echo = verbose) # Run quietly\n",
    "                    \n",
    "    print('Current Manifest:')\n",
    "    print(dataManifest.DF)\n",
    "    return\n",
    "##################################\n",
    "##################################\n",
    "\n",
    "# ADD CODE TO STITCH TOGETHER SPECIFIC DATA PARTS FROM DIFFERENT SETS FOR ANALYSIS (CORRELATION ETC.)\n",
    "# ADD CODE TO DIRECTLY API CALL POST-PROCESS DATA (TECH INDICATORS) (ALSO MAKE CODE TO PROCESS IN HOUSE IF DESIRED)\n",
    "# WILL NEED CODE TO ASSESS ANY STOCKSPLIT INFORMATION AND EITHER MARK FOR RENEW DATA FROM API OR EDIT EXISTING DATA AS NEEDED\n",
    "\n",
    "##################################\n",
    "##################################\n",
    "# Auxiliary code\n",
    "class APIError(Exception): pass # Error for a API error response (invalid call or API call limit)\n",
    "class EnvError(Exception): pass # Error for missing environment variables\n",
    "##################################\n",
    "##################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4cf7b04-0dd2-44d7-925f-746fa44e2c6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Manifest Initialised\n",
      "Loading Manifest Data\n",
      "Load path/name: data/StockHistData/dataManifest.json\n",
      "Data file existence indicated in manifest, skipping.\n",
      "Data file existence indicated in manifest, skipping.\n",
      "Data file existence indicated in manifest, skipping.\n",
      "Data file existence indicated in manifest, skipping.\n",
      "Data file existence indicated in manifest, skipping.\n",
      "Current Manifest:\n",
      "Month            2022-01  2022-02  2022-03  2022-04  2022-05  2022-06  \\\n",
      "Ticker Interval                                                         \n",
      "ARM    1               0        0        0        0        0        0   \n",
      "       5               0        0        0        0        0        0   \n",
      "       15              0        0        0        0        0        0   \n",
      "       30              0        0        0        0        0        0   \n",
      "       60              0        0        0        0        0        0   \n",
      "GOOG   1               1        1        1        1        1        1   \n",
      "       5               1        1        1        1        1        1   \n",
      "       15              1        1        1        1        1        1   \n",
      "       30              1        1        1        1        1        1   \n",
      "       60              1        1        1        1        1        1   \n",
      "KO     1               1        1        1        1        1        1   \n",
      "       5               1        1        1        1        1        1   \n",
      "       15              1        1        1        1        1        1   \n",
      "       30              1        1        1        1        1        1   \n",
      "       60              1        1        1        1        1        1   \n",
      "MSFT   1               1        1        1        1        1        1   \n",
      "       5               1        1        1        1        1        1   \n",
      "       15              1        1        1        1        1        1   \n",
      "       30              1        1        1        1        1        1   \n",
      "       60              1        1        1        1        1        1   \n",
      "NVDA   1               1        1        1        1        1        1   \n",
      "       5               1        1        1        1        1        1   \n",
      "       15              1        1        1        1        1        1   \n",
      "       30              1        1        1        1        1        1   \n",
      "       60              1        1        1        1        1        1   \n",
      "\n",
      "Month            2022-07  2022-08  2022-09  2022-10  ...  2024-04  2024-05  \\\n",
      "Ticker Interval                                      ...                     \n",
      "ARM    1               0        0        0        0  ...        1        1   \n",
      "       5               0        0        0        0  ...        1        1   \n",
      "       15              0        0        0        0  ...        1        1   \n",
      "       30              0        0        0        0  ...        1        1   \n",
      "       60              0        0        0        0  ...        1        1   \n",
      "GOOG   1               1        1        1        1  ...        0        0   \n",
      "       5               1        1        1        1  ...        0        0   \n",
      "       15              1        1        1        1  ...        0        0   \n",
      "       30              1        1        1        1  ...        0        0   \n",
      "       60              1        1        1        1  ...        0        0   \n",
      "KO     1               1        1        1        1  ...        1        1   \n",
      "       5               1        1        1        1  ...        1        1   \n",
      "       15              1        1        1        1  ...        1        1   \n",
      "       30              1        1        1        1  ...        1        1   \n",
      "       60              1        1        1        1  ...        1        1   \n",
      "MSFT   1               1        1        1        1  ...        1        1   \n",
      "       5               1        1        1        1  ...        1        1   \n",
      "       15              1        1        1        1  ...        1        1   \n",
      "       30              1        1        1        1  ...        1        1   \n",
      "       60              1        1        1        1  ...        1        1   \n",
      "NVDA   1               1        1        1        1  ...        1        1   \n",
      "       5               1        1        1        1  ...        1        1   \n",
      "       15              1        1        1        1  ...        1        1   \n",
      "       30              1        1        1        1  ...        1        1   \n",
      "       60              1        1        1        1  ...        1        1   \n",
      "\n",
      "Month            2024-06  2024-07  2024-08  2024-09  2024-10  2024-11  \\\n",
      "Ticker Interval                                                         \n",
      "ARM    1               1        1        1        1        1        1   \n",
      "       5               1        1        1        1        1        1   \n",
      "       15              1        1        1        1        1        1   \n",
      "       30              1        1        1        1        1        1   \n",
      "       60              1        1        1        1        1        1   \n",
      "GOOG   1               0        0        0        0        0        0   \n",
      "       5               0        0        0        0        0        0   \n",
      "       15              0        0        0        0        0        0   \n",
      "       30              0        0        0        0        0        0   \n",
      "       60              0        0        0        0        0        0   \n",
      "KO     1               1        1        1        1        1        1   \n",
      "       5               1        1        1        1        1        1   \n",
      "       15              1        1        1        1        1        1   \n",
      "       30              1        1        1        1        1        1   \n",
      "       60              1        1        1        1        1        1   \n",
      "MSFT   1               1        1        1        1        1        1   \n",
      "       5               1        1        1        1        1        1   \n",
      "       15              1        1        1        1        1        1   \n",
      "       30              1        1        1        1        1        1   \n",
      "       60              1        1        1        1        1        1   \n",
      "NVDA   1               1        1        1        1        1        1   \n",
      "       5               1        1        1        1        1        1   \n",
      "       15              1        1        1        1        1        1   \n",
      "       30              1        1        1        1        1        1   \n",
      "       60              1        1        1        1        1        1   \n",
      "\n",
      "Month            2024-12  2025-01  \n",
      "Ticker Interval                    \n",
      "ARM    1               1        1  \n",
      "       5               1        1  \n",
      "       15              1        1  \n",
      "       30              1        1  \n",
      "       60              1        1  \n",
      "GOOG   1               0        1  \n",
      "       5               0        1  \n",
      "       15              0        1  \n",
      "       30              0        1  \n",
      "       60              0        1  \n",
      "KO     1               1        1  \n",
      "       5               1        1  \n",
      "       15              1        1  \n",
      "       30              1        1  \n",
      "       60              1        1  \n",
      "MSFT   1               1        1  \n",
      "       5               1        1  \n",
      "       15              1        1  \n",
      "       30              1        1  \n",
      "       60              1        1  \n",
      "NVDA   1               1        1  \n",
      "       5               1        1  \n",
      "       15              1        1  \n",
      "       30              1        1  \n",
      "       60              1        1  \n",
      "\n",
      "[25 rows x 37 columns]\n",
      "Data Manifest Initialised\n",
      "Loading Manifest Data\n",
      "Load path/name: data/StockHistData/dataManifest.json\n",
      "Connecting to engine: Engine(postgresql+psycopg2://postgres:***@localhost:5432/marketdata)\n"
     ]
    }
   ],
   "source": [
    "## Here we create settings for the database building/development:\n",
    "# - Tickers to download\n",
    "# - Months of intraday data to request\n",
    "# - Time intervals (granularity/resolution)\n",
    "# - API request key (taken from file)\n",
    "\n",
    "savepath = r'data/StockHistData/'\n",
    "\n",
    "# Tickers to request, make sure it is the correct one (there is a search API call to check)\n",
    "dltickers = [\"GOOG\"]\n",
    "\n",
    "# Months to request (string, \"year-month\") i.e. 2020-02\n",
    "dlmonths = [\"2025-01\"] # Need to add functionality to take whole years\n",
    "\n",
    "# Options for time resolution: 1, 5, 15, 30, 60\n",
    "dlintervals = [1, 5, 15, 30, 60]\n",
    "\n",
    "##### Alphavantage API key for data acquisition\n",
    "# .env file method\n",
    "env_path = Path(\".\") / \"APIkey.env\" # Environment variables file must be same folder as this code (#REGEXSEARCHFOR API IF NO FILE)\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "alphaAPIkey = os.getenv(\"ALPHA_API_KEY\") #MAYBE TRY .TXT IF NO .ENV\n",
    "# .txt file method\n",
    "#with open('APIkey.txt',) as keyfile:\n",
    "#    alphaAPIkey = keyfile.read()\n",
    "\n",
    "\n",
    "##### Setting environment variables for the SQL login details\n",
    "SQLloginfilename = \"SQLlogin\" # Default filename \"SQLlogin\" \n",
    "#SQLdetails_path = Path(\".\") / f\"{SQLloginfilename}.env\"\n",
    "#set_key(SQLdetails_path, 'DRIVER', 'hello') #dotenv.set_key\n",
    "#set_key(SQLdetails_path, 'DIALECT', 'mate')\n",
    "#set_key(SQLdetails_path, 'ENV_USER', 'put')\n",
    "#set_key(SQLdetails_path, 'PASSWORD', 'your')\n",
    "#set_key(SQLdetails_path, 'HOST_MACHINE', 'own')\n",
    "#set_key(SQLdetails_path, 'PORT', 'stuff')\n",
    "#set_key(SQLdetails_path, 'DBNAME', 'here')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### Sourcing market data\n",
    "DownloadIntraday(savepath, dltickers, dlintervals, dlmonths, alphaAPIkey, verbose=True)\n",
    "\n",
    "checkManifest = DataManifest()\n",
    "checkManifest.loadManifest(savepath)\n",
    "checkManifest.connectSQL(SQLloginfilename)\n",
    "\n",
    "#checkManifest.validateManifest(fastValidate = False, show = False)\n",
    "#print(checkManifest.DF) # Maybe I can add method to show manifest in a more compact form\n",
    "#checkManifest.saveManifest(checkManifest.directory)\n",
    "\n",
    "# ADD CODE TO CONVERT SQL TABLE TO THE DF FORM (MULTIINDEX)c\n",
    "\n",
    "# PSEUDOCODE FOR MANIFEST:\n",
    "# DROP MANIFESTID\n",
    "# MAKE MULTIINDEX ONCE AGAIN FROM STOCKS, INTERVAL COLUMNS\n",
    "# RENAME COLUMN NAME TO MONTH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09fdb622-1758-4fbb-9354-9ed17cfbcdbd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pytest\n",
    "from python_on_whales import docker\n",
    "import time\n",
    "\n",
    "\n",
    "# Run a container\n",
    "#docker.run(\"hello-world\")\n",
    "\n",
    "# Pull an image\n",
    "#docker.pull(\"postgres:15\")\n",
    "\n",
    "\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def test_postgres():\n",
    "    container = docker.run(\n",
    "        \"postgres:15\",\n",
    "        detach=True,\n",
    "        name=\"test_pg\",\n",
    "        envs={\"POSTGRES_PASSWORD\": \"test\", \"POSTGRES_USER\": \"test\", \"POSTGRES_DB\": \"testdb\"},\n",
    "        publish=[\"5433:5432\"]\n",
    "    )\n",
    "    time.sleep(3)  # wait for the DB to boot\n",
    "\n",
    "    yield \"postgresql://test:test@localhost:5433/testdb\"\n",
    "\n",
    "    docker.container.remove(\"test_pg\", force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c77da0-37fc-4441-8a12-00d6acf59eef",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#%%writefile DockerInstallWrapper.py\n",
    "# Docker Installation Process\n",
    "import os\n",
    "\n",
    "def install_docker():\n",
    "    import subprocess\n",
    "    import shutil\n",
    "\n",
    "    docker_path = shutil.which(\"docker\")\n",
    "    if docker_path:\n",
    "        print(f\"Docker already installed, located at: {docker_path}\")\n",
    "        return\n",
    "    else:\n",
    "        print(\"Docker not found in PATH\")\n",
    "\n",
    "    # Windows installation procedure\n",
    "    if os.name == \"nt\":\n",
    "        dockerInstaller_path = shutil.which(\"dockerinstaller\")    \n",
    "        if dockerInstaller_path:\n",
    "            print(f\"Docker installer found at: {dockerInstaller_path}\")\n",
    "        else:\n",
    "            print(\"Downloading docker installer...\")\n",
    "            subprocess.run([\"powershell\", \"-Command\", \"Invoke-WebRequest -Uri 'https://desktop.docker.com/win/main/amd64/Docker Desktop Installer.exe' -OutFile 'DockerInstaller.exe'\"], shell=True)\n",
    "        \n",
    "        quietChoice = input(\"Do you want to install quietly (without GUI, automatically)? (y/n): \").strip().lower()\n",
    "        if quietChoice == 'y':\n",
    "            print(\"Installing docker quietly...\")\n",
    "            subprocess.run([\"powershell\", \"-Command\", 'Start-Process -FilePath \"DockerInstaller.exe\" -ArgumentList \"install --quiet\" -Verb RunAs -Wait'], shell=True)\n",
    "        else:\n",
    "            print(\"Installing docker...\")\n",
    "            subprocess.run([\"powershell\", \"-Command\", 'Start-Process -FilePath \"DockerInstaller.exe\" -ArgumentList \"install\" -Verb RunAs -Wait'], shell=True)\n",
    "\n",
    "    # Linux/macOS installation procedure\n",
    "    elif os.name == \"posix\":\n",
    "        print(\"Installing docker...\")\n",
    "        subprocess.run(\"curl -fsSL https://get.docker.com | sh\", shell=True)\n",
    "        subprocess.run(\"sudo usermod -aG docker $USER\", shell=True)\n",
    "\n",
    "    prompt_restart()\n",
    "\n",
    "def prompt_restart():\n",
    "    choice = input(\"Docker installation complete. Do you want to restart now? (y/n): \").strip().lower()\n",
    "    if choice == 'y':\n",
    "        force_restart()\n",
    "    else:\n",
    "        print(\"Restart skipped. You may need to restart manually for changes to take full effect.\")\n",
    "\n",
    "def force_restart():\n",
    "    print(\"Restarting device.\")\n",
    "    if os.name == \"nt\":  # Windows\n",
    "        subprocess.run([\"powershell\", \"Restart-Computer -Force\"], shell=True)\n",
    "    elif os.name == \"posix\":  # Linux/macOS\n",
    "        subprocess.run(\"sudo shutdown -r now\", shell=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    installChoice = input(\"Do you want to install Docker to gain access to SQL functionality? (y/n): \").strip().lower()\n",
    "    \n",
    "    if installChoice == 'y':\n",
    "        install_docker()\n",
    "    else:\n",
    "        print(\"Install skipped. You need to install docker to be able to use SQL functionality.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a37bb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "y2\n"
     ]
    }
   ],
   "source": [
    "# TEST SQL SYNC\n",
    "# TEST SQL SAVE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbbcae28-02a3-4ddd-bec3-e1c9b65d20f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractData(targetData: str, manifest: DataManifest, start, end, fromSQL = False, condition = None, **filters) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts data from a target dataset (market data or data manifest), with a provided start and end period, using\n",
    "    a provided extraction method and filtering condition(s) if desired. The function returns a DataFrame.\n",
    "    Input:\n",
    "    - targetData - String indicating the targeted dataset is either the market/stock data (marketData), data manifest (dataManifest),\n",
    "    or compressed data manifest (compDataManifest) containing year-only manifest data.\n",
    "    - start - (Period, str, datetime, date or pandas.Timestamp) indicating the beginning of the time period to extract, see notes for details.\n",
    "    - end - (Period, str, datetime, date or pandas.Timestamp) indicating the end of the time period to extract, see notes for details.\n",
    "    - manifest - DataManifest object relating to the data to be extracted.\n",
    "    Optional inputs:\n",
    "    - fromSQL - Boolean indicating where to extract data from, the default (False) takes data directly from .csv for market data and\n",
    "    .json for manifests, while True takes data from SQL (manifest must have a valid SQL database, and SQLengine attribute).\n",
    "    - condition - A user-custom callable condition for filtering or selecting subset of data, e.g. lambda functions. The\n",
    "    condition must take\n",
    "    - filters - Custom inputtable keyword-argument (kwarg) variables to act as simple equality filters (i.e. inputting Ticker =\n",
    "    \"TEST\" as a kwarg filters the 'Ticker' column for \"TEST\" datapoints only) \n",
    "\n",
    "    Notes:\n",
    "    - The targetData variable is searches for 'market' or 'stock' to indicate marketData or 'manifest' to search for dataManifest\n",
    "    while 'comp' and 'manifest' is both searched for compDatamanifest. \n",
    "    - The manifest input DataManifest must have a viable directory, and where applicable, SQLengine, attributes.\n",
    "    - If manifest data is being extracted directly (fromSQL == False), it is sourced from the .json file (and not the currently\n",
    "    loaded manifest's DF attribute)\n",
    "    - Datatypes acceptable to start/end are whatever pandas.Period() accepts, described above (but could change if pandas changes)\n",
    "    - If any of the start or end inputs contain 'all', the function returns the entire dataset available.\n",
    "    - If start is None, the function returns all data before end period. If end is None, the function returns all data after start period.\n",
    "    - The start datetime is the inputted argument, assuming the beginning of any period truncated (i.e. if only date given,\n",
    "    assumes start of day, or start of year if only year given, etc.)\n",
    "    - The end datetime is similarly the inputted argument, except it is the end of any period truncated (i.e. EoD if date\n",
    "    provided or end-of-month if year-month provided)\n",
    "    - Regarding the start and end periods, for data manifests and compressed data manifests, the period need not be more\n",
    "    specific than year-month or year, respectively.\n",
    "    - For market data extraction, for start periods, truncated data assumes the beginning of the period (e.g. if only year\n",
    "    given, assume year start), while for end periods truncation assumes the end of the period.\n",
    "    - For data manifests, truncated data is always inclusive of the whole period (e.g. if end period is 2015 for a data\n",
    "    manifest, then the whole of 2015 is considered), and over-specific data is extended (i.e. if start period is 2015-03-29,\n",
    "    then the 2015-03 data point is included).\n",
    "    - If fromSQL variable is not True/Truthy, it is automatically treated as False (come on, you should be able to not ruin\n",
    "    an optional boolean... :D)\n",
    "    - The 'condition' provided must accept only the DataFrame that is to be returned as an argument (it is applied to the\n",
    "    DataFrame before it is returned)\n",
    "    - If both 'condition' and 'filters' are provided, both are applied.\n",
    "    - Extracting stock data from SQL is faster for larger datasets.\n",
    "    \"\"\"\n",
    "    # Check the start and end inputs for 'all' or None flags\n",
    "    period_inputs = [start,end]\n",
    "    if any( (isinstance(period,str) and 'all' in period.lower() ) for period in period_inputs ): # If any 'all' set to max limit\n",
    "        start = '1900'\n",
    "        end = '2200-01-01'\n",
    "    if start is None: \n",
    "        start = '1900'\n",
    "    if end is None: # Doing these two after 'all' check is more efficient (likelier to skip)\n",
    "        end = '2200-01-01'\n",
    "\n",
    "    # Get start and end times for start and end args inputted\n",
    "    startDT = pd.Period(start).start_time\n",
    "    endDT = pd.Period(end).end_time\n",
    "    # Get the range of all months that are considered\n",
    "    startMstr = startDT.strftime(\"%Y-%m\")\n",
    "    endMstr = endDT.strftime(\"%Y-%m\")\n",
    "\n",
    "    # List of all months\n",
    "    monthList = list(pd.date_range(start = startMstr, end = endMstr, freq = 'MS', inclusive = 'both').strftime(\"%Y-%m\").values)\n",
    "\n",
    "    resultDF = pd.DataFrame()\n",
    "    \n",
    "    # The method for acquiring market data and manifest data are different, method for filtering for time period is also different for each (SQL vs DataManifest) method\n",
    "    if fromSQL: # If extracting from SQL\n",
    "        if 'market' in targetData.lower() or 'stock' in targetData.lower(): # Getting market data\n",
    "            resultDF = pd.read_sql(f'SELECT * FROM \"stockData\" WHERE \"DateTime\" BETWEEN \\'{str(startDT)}\\' AND \\'{str(endDT)}\\';', manifest.SQLengine)#, index_col = ['Ticker','Interval'])\n",
    "            resultDF.drop(columns=['Nominal'], inplace = True) # Drop nominal (for now)\n",
    "\n",
    "        elif 'comp' in targetData.lower() and 'manifest' in targetData.lower(): # Getting compressed manifest data\n",
    "            print('To add this functionality in the future...') #TODO: ADD THIS FUNCTIONALITY\n",
    "\n",
    "        elif 'manifest' in targetData.lower(): # Getting (regular) manifest data\n",
    "            # Edit view to include new manifest columns\n",
    "            updateViewQuery = \"\"\"\n",
    "                DO $$\n",
    "                DECLARE\n",
    "                    col_list text;\n",
    "                BEGIN\n",
    "                    SELECT string_agg('m.\"' || column_name || '\"', ', ' ORDER BY ordinal_position)\n",
    "                    INTO col_list\n",
    "                    FROM information_schema.columns\n",
    "                    WHERE table_name = 'manifestTable'\n",
    "                      AND column_name NOT IN ('TickerID')\n",
    "                      AND table_schema = 'public';\n",
    "                \n",
    "                    EXECUTE format('\n",
    "                        CREATE OR REPLACE VIEW \"manifestData\" AS\n",
    "                        SELECT t.\"Ticker\", %s\n",
    "                        FROM \"manifestTable\" m\n",
    "                        JOIN \"tickerTable\" t ON m.\"TickerID\" = t.\"TickerID\";\n",
    "                    ', col_list);\n",
    "                END $$;\n",
    "                \"\"\"\n",
    "            ExecuteSQL(updateViewQuery, manifest.SQLengine)\n",
    "\n",
    "            # Extract list of columns that are within the given timeframe (from SQL-side, for efficiency)\n",
    "            # Dynamically obtain list of columns within given date limits\n",
    "            colListQuery = f\"\"\"\n",
    "                SELECT string_agg('\"' || column_name || '\"', ', ')\n",
    "                FROM information_schema.columns\n",
    "                WHERE table_name = 'manifestData'\n",
    "                AND table_schema = 'public'\n",
    "                AND column_name ~ {\"'^\\\\d{4}-\\\\d{2}$'\"}\n",
    "                AND to_date(column_name, 'YYYY-MM') BETWEEN '{startDT.strftime(\"%Y-%m-%d\")}' AND '{endDT.strftime(\"%Y-%m-%d\")}';\n",
    "                \"\"\"\n",
    "            \n",
    "            dynamicCols = ExecuteSQL(colListQuery, manifest.SQLengine, fetch=True)\n",
    "            dynamicColsString = dynamicCols[0][0]\n",
    "            fixedColsString = '\"Ticker\", \"Interval\"'\n",
    "            searchString = \"\"\n",
    "            if dynamicColsString is not None:\n",
    "                searchString = f\"{fixedColsString}, {dynamicColsString}\"\n",
    "            else:\n",
    "                searchString = fixedColsString\n",
    "            \n",
    "            # Extract whole or partial view \n",
    "            resultDF = pd.read_sql(f'SELECT {searchString} FROM \"manifestData\";', manifest.SQLengine, index_col = ['Ticker','Interval'])\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"The targetData input must match 'market'/'stock', 'manifest' or 'comp' & 'manifest'.\")\n",
    "\n",
    "    else: # If extracting data from DataManifest\n",
    "        # First check months to make sure we remove any months in monthList not within DF columns (otherwise raises error in all non-SQL cases)\n",
    "        dummyList = copy.deepcopy(monthList)\n",
    "        for month in monthList:\n",
    "            if month not in list(manifest.DF.columns.values): dummyList.remove(month)\n",
    "        monthList = dummyList\n",
    "\n",
    "        # Getting market data\n",
    "        if 'market' in targetData.lower() or 'stock' in targetData.lower():\n",
    "            # Load manifest\n",
    "            manifest.loadManifest(manifest.directory)\n",
    "            \n",
    "            ### Obtain data directly and stitch together\n",
    "            # Get combination of ticker/interval to combine with month for .csv file name\n",
    "            remainderDF = manifest.DF[monthList] # Ignore non-whole months\n",
    "            remainderDF = remainderDF.loc[(remainderDF != 0).any(axis = 1)] # Gets rid of rows (ticker, interval) that are zeroes (for all months in manifest)\n",
    "            \n",
    "            for tick, interv in list(remainderDF.index.values):\n",
    "                # If data for each month exists in manifest, add full file to aggregate DF\n",
    "                for month in monthList:\n",
    "                    if int(manifest.DF.loc[tick, interv][month]): # Check data exists on manifest #NOTE: MAY RAISE ERROR\n",
    "                        addDF = manifest.loadData_fromcsv(tick, interv, month, convert_DateTime = True, echo = False)\n",
    "                        # Add ticker name on DF (as normally its not specified, and we are mixing the datasets)\n",
    "                        addDF['Ticker'] = tick\n",
    "                        addDF['Interval'] = interv\n",
    "                        addDF.insert(0, 'Ticker', addDF.pop('Ticker'))\n",
    "                        addDF.insert(1, 'Interval', addDF.pop('Interval'))\n",
    "                        resultDF = pd.concat([resultDF, addDF], axis = 0, ignore_index = True)\n",
    "                    \n",
    "            resultDF = resultDF[(pd.to_datetime(resultDF.DateTime) >= startDT) & (pd.to_datetime(resultDF.DateTime) <= endDT)]\n",
    "                \n",
    "        elif 'comp' in targetData.lower() and 'manifest' in targetData.lower(): # Getting compressed manifest data\n",
    "            print('To add this functionality in the future...')\n",
    "\n",
    "        elif 'manifest' in targetData.lower():  # Getting (regular) manifest data\n",
    "            # Load manifest directly from .json file (now is manifest.DF)\n",
    "            manifest.loadManifest(manifest.directory)\n",
    "        \n",
    "            # Filter for each month in list (including first and last months)\n",
    "            resultDF = manifest.DF[monthList]\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"The targetData input must match 'market'/'stock', 'manifest' or 'comp' & 'manifest'.\")\n",
    "    \n",
    "    # If custom filter function given and callable, apply it\n",
    "    if callable(condition):\n",
    "        resultDF = resultDF[condition(resultDF)]\n",
    "\n",
    "    # Apply simply equality filters from 'filters' kwargs\n",
    "    for col, val in filters.items():\n",
    "        resultDF = resultDF[resultDF[col] == val]\n",
    "    \n",
    "    return resultDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bacd1cf1-236e-44db-84c7-385f1d7b27e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Manifest Initialised\n",
      "Loading Manifest Data\n",
      "Load path/name: data/StockHistData/dataManifest.json\n",
      "Validating data manifest DataFrame.\n",
      "Conducting fast validation.\n",
      "Searching for file ARM_1_2023-09\n",
      "Loading file data: ARM_1_2023-09.csv\n",
      "File ARM_1_2023-09 found.\n",
      "Searching for file ARM_1_2023-10\n",
      "Loading file data: ARM_1_2023-10.csv\n",
      "File ARM_1_2023-10 found.\n",
      "Searching for file ARM_1_2023-11\n",
      "Loading file data: ARM_1_2023-11.csv\n",
      "File ARM_1_2023-11 found.\n",
      "Searching for file ARM_1_2023-12\n",
      "Loading file data: ARM_1_2023-12.csv\n",
      "File ARM_1_2023-12 found.\n",
      "Searching for file ARM_1_2024-01\n",
      "Loading file data: ARM_1_2024-01.csv\n",
      "File ARM_1_2024-01 found.\n",
      "Searching for file ARM_1_2024-02\n",
      "Loading file data: ARM_1_2024-02.csv\n",
      "File ARM_1_2024-02 found.\n",
      "Searching for file ARM_1_2024-03\n",
      "Loading file data: ARM_1_2024-03.csv\n",
      "File ARM_1_2024-03 found.\n",
      "Searching for file ARM_1_2024-04\n",
      "Loading file data: ARM_1_2024-04.csv\n",
      "File ARM_1_2024-04 found.\n",
      "Searching for file ARM_1_2024-05\n",
      "Loading file data: ARM_1_2024-05.csv\n",
      "File ARM_1_2024-05 found.\n",
      "Searching for file ARM_1_2024-06\n",
      "Loading file data: ARM_1_2024-06.csv\n",
      "File ARM_1_2024-06 found.\n",
      "Searching for file ARM_1_2024-07\n",
      "Loading file data: ARM_1_2024-07.csv\n",
      "File ARM_1_2024-07 found.\n",
      "Searching for file ARM_1_2024-08\n",
      "Loading file data: ARM_1_2024-08.csv\n",
      "File ARM_1_2024-08 found.\n",
      "Searching for file ARM_1_2024-09\n",
      "Loading file data: ARM_1_2024-09.csv\n",
      "File ARM_1_2024-09 found.\n",
      "Searching for file ARM_1_2024-10\n",
      "Loading file data: ARM_1_2024-10.csv\n",
      "File ARM_1_2024-10 found.\n",
      "Searching for file ARM_1_2024-11\n",
      "Loading file data: ARM_1_2024-11.csv\n",
      "File ARM_1_2024-11 found.\n",
      "Searching for file ARM_1_2024-12\n",
      "Loading file data: ARM_1_2024-12.csv\n",
      "File ARM_1_2024-12 found.\n",
      "Searching for file ARM_1_2025-01\n",
      "Loading file data: ARM_1_2025-01.csv\n",
      "File ARM_1_2025-01 found.\n",
      "Searching for file ARM_5_2023-09\n",
      "Loading file data: ARM_5_2023-09.csv\n",
      "File ARM_5_2023-09 found.\n",
      "Searching for file ARM_5_2023-10\n",
      "Loading file data: ARM_5_2023-10.csv\n",
      "File ARM_5_2023-10 found.\n",
      "Searching for file ARM_5_2023-11\n",
      "Loading file data: ARM_5_2023-11.csv\n",
      "File ARM_5_2023-11 found.\n",
      "Searching for file ARM_5_2023-12\n",
      "Loading file data: ARM_5_2023-12.csv\n",
      "File ARM_5_2023-12 found.\n",
      "Searching for file ARM_5_2024-01\n",
      "Loading file data: ARM_5_2024-01.csv\n",
      "File ARM_5_2024-01 found.\n",
      "Searching for file ARM_5_2024-02\n",
      "Loading file data: ARM_5_2024-02.csv\n",
      "File ARM_5_2024-02 found.\n",
      "Searching for file ARM_5_2024-03\n",
      "Loading file data: ARM_5_2024-03.csv\n",
      "File ARM_5_2024-03 found.\n",
      "Searching for file ARM_5_2024-04\n",
      "Loading file data: ARM_5_2024-04.csv\n",
      "File ARM_5_2024-04 found.\n",
      "Searching for file ARM_5_2024-05\n",
      "Loading file data: ARM_5_2024-05.csv\n",
      "File ARM_5_2024-05 found.\n",
      "Searching for file ARM_5_2024-06\n",
      "Loading file data: ARM_5_2024-06.csv\n",
      "File ARM_5_2024-06 found.\n",
      "Searching for file ARM_5_2024-07\n",
      "Loading file data: ARM_5_2024-07.csv\n",
      "File ARM_5_2024-07 found.\n",
      "Searching for file ARM_5_2024-08\n",
      "Loading file data: ARM_5_2024-08.csv\n",
      "File ARM_5_2024-08 found.\n",
      "Searching for file ARM_5_2024-09\n",
      "Loading file data: ARM_5_2024-09.csv\n",
      "File ARM_5_2024-09 found.\n",
      "Searching for file ARM_5_2024-10\n",
      "Loading file data: ARM_5_2024-10.csv\n",
      "File ARM_5_2024-10 found.\n",
      "Searching for file ARM_5_2024-11\n",
      "Loading file data: ARM_5_2024-11.csv\n",
      "File ARM_5_2024-11 found.\n",
      "Searching for file ARM_5_2024-12\n",
      "Loading file data: ARM_5_2024-12.csv\n",
      "File ARM_5_2024-12 found.\n",
      "Searching for file ARM_5_2025-01\n",
      "Loading file data: ARM_5_2025-01.csv\n",
      "File ARM_5_2025-01 found.\n",
      "Searching for file ARM_15_2023-09\n",
      "Loading file data: ARM_15_2023-09.csv\n",
      "File ARM_15_2023-09 found.\n",
      "Searching for file ARM_15_2023-10\n",
      "Loading file data: ARM_15_2023-10.csv\n",
      "File ARM_15_2023-10 found.\n",
      "Searching for file ARM_15_2023-11\n",
      "Loading file data: ARM_15_2023-11.csv\n",
      "File ARM_15_2023-11 found.\n",
      "Searching for file ARM_15_2023-12\n",
      "Loading file data: ARM_15_2023-12.csv\n",
      "File ARM_15_2023-12 found.\n",
      "Searching for file ARM_15_2024-01\n",
      "Loading file data: ARM_15_2024-01.csv\n",
      "File ARM_15_2024-01 found.\n",
      "Searching for file ARM_15_2024-02\n",
      "Loading file data: ARM_15_2024-02.csv\n",
      "File ARM_15_2024-02 found.\n",
      "Searching for file ARM_15_2024-03\n",
      "Loading file data: ARM_15_2024-03.csv\n",
      "File ARM_15_2024-03 found.\n",
      "Searching for file ARM_15_2024-04\n",
      "Loading file data: ARM_15_2024-04.csv\n",
      "File ARM_15_2024-04 found.\n",
      "Searching for file ARM_15_2024-05\n",
      "Loading file data: ARM_15_2024-05.csv\n",
      "File ARM_15_2024-05 found.\n",
      "Searching for file ARM_15_2024-06\n",
      "Loading file data: ARM_15_2024-06.csv\n",
      "File ARM_15_2024-06 found.\n",
      "Searching for file ARM_15_2024-07\n",
      "Loading file data: ARM_15_2024-07.csv\n",
      "File ARM_15_2024-07 found.\n",
      "Searching for file ARM_15_2024-08\n",
      "Loading file data: ARM_15_2024-08.csv\n",
      "File ARM_15_2024-08 found.\n",
      "Searching for file ARM_15_2024-09\n",
      "Loading file data: ARM_15_2024-09.csv\n",
      "File ARM_15_2024-09 found.\n",
      "Searching for file ARM_15_2024-10\n",
      "Loading file data: ARM_15_2024-10.csv\n",
      "File ARM_15_2024-10 found.\n",
      "Searching for file ARM_15_2024-11\n",
      "Loading file data: ARM_15_2024-11.csv\n",
      "File ARM_15_2024-11 found.\n",
      "Searching for file ARM_15_2024-12\n",
      "Loading file data: ARM_15_2024-12.csv\n",
      "File ARM_15_2024-12 found.\n",
      "Searching for file ARM_15_2025-01\n",
      "Loading file data: ARM_15_2025-01.csv\n",
      "File ARM_15_2025-01 found.\n",
      "Searching for file ARM_30_2023-09\n",
      "Loading file data: ARM_30_2023-09.csv\n",
      "File ARM_30_2023-09 found.\n",
      "Searching for file ARM_30_2023-10\n",
      "Loading file data: ARM_30_2023-10.csv\n",
      "File ARM_30_2023-10 found.\n",
      "Searching for file ARM_30_2023-11\n",
      "Loading file data: ARM_30_2023-11.csv\n",
      "File ARM_30_2023-11 found.\n",
      "Searching for file ARM_30_2023-12\n",
      "Loading file data: ARM_30_2023-12.csv\n",
      "File ARM_30_2023-12 found.\n",
      "Searching for file ARM_30_2024-01\n",
      "Loading file data: ARM_30_2024-01.csv\n",
      "File ARM_30_2024-01 found.\n",
      "Searching for file ARM_30_2024-02\n",
      "Loading file data: ARM_30_2024-02.csv\n",
      "File ARM_30_2024-02 found.\n",
      "Searching for file ARM_30_2024-03\n",
      "Loading file data: ARM_30_2024-03.csv\n",
      "File ARM_30_2024-03 found.\n",
      "Searching for file ARM_30_2024-04\n",
      "Loading file data: ARM_30_2024-04.csv\n",
      "File ARM_30_2024-04 found.\n",
      "Searching for file ARM_30_2024-05\n",
      "Loading file data: ARM_30_2024-05.csv\n",
      "File ARM_30_2024-05 found.\n",
      "Searching for file ARM_30_2024-06\n",
      "Loading file data: ARM_30_2024-06.csv\n",
      "File ARM_30_2024-06 found.\n",
      "Searching for file ARM_30_2024-07\n",
      "Loading file data: ARM_30_2024-07.csv\n",
      "File ARM_30_2024-07 found.\n",
      "Searching for file ARM_30_2024-08\n",
      "Loading file data: ARM_30_2024-08.csv\n",
      "File ARM_30_2024-08 found.\n",
      "Searching for file ARM_30_2024-09\n",
      "Loading file data: ARM_30_2024-09.csv\n",
      "File ARM_30_2024-09 found.\n",
      "Searching for file ARM_30_2024-10\n",
      "Loading file data: ARM_30_2024-10.csv\n",
      "File ARM_30_2024-10 found.\n",
      "Searching for file ARM_30_2024-11\n",
      "Loading file data: ARM_30_2024-11.csv\n",
      "File ARM_30_2024-11 found.\n",
      "Searching for file ARM_30_2024-12\n",
      "Loading file data: ARM_30_2024-12.csv\n",
      "File ARM_30_2024-12 found.\n",
      "Searching for file ARM_30_2025-01\n",
      "Loading file data: ARM_30_2025-01.csv\n",
      "File ARM_30_2025-01 found.\n",
      "Searching for file ARM_60_2023-09\n",
      "Loading file data: ARM_60_2023-09.csv\n",
      "File ARM_60_2023-09 found.\n",
      "Searching for file ARM_60_2023-10\n",
      "Loading file data: ARM_60_2023-10.csv\n",
      "File ARM_60_2023-10 found.\n",
      "Searching for file ARM_60_2023-11\n",
      "Loading file data: ARM_60_2023-11.csv\n",
      "File ARM_60_2023-11 found.\n",
      "Searching for file ARM_60_2023-12\n",
      "Loading file data: ARM_60_2023-12.csv\n",
      "File ARM_60_2023-12 found.\n",
      "Searching for file ARM_60_2024-01\n",
      "Loading file data: ARM_60_2024-01.csv\n",
      "File ARM_60_2024-01 found.\n",
      "Searching for file ARM_60_2024-02\n",
      "Loading file data: ARM_60_2024-02.csv\n",
      "File ARM_60_2024-02 found.\n",
      "Searching for file ARM_60_2024-03\n",
      "Loading file data: ARM_60_2024-03.csv\n",
      "File ARM_60_2024-03 found.\n",
      "Searching for file ARM_60_2024-04\n",
      "Loading file data: ARM_60_2024-04.csv\n",
      "File ARM_60_2024-04 found.\n",
      "Searching for file ARM_60_2024-05\n",
      "Loading file data: ARM_60_2024-05.csv\n",
      "File ARM_60_2024-05 found.\n",
      "Searching for file ARM_60_2024-06\n",
      "Loading file data: ARM_60_2024-06.csv\n",
      "File ARM_60_2024-06 found.\n",
      "Searching for file ARM_60_2024-07\n",
      "Loading file data: ARM_60_2024-07.csv\n",
      "File ARM_60_2024-07 found.\n",
      "Searching for file ARM_60_2024-08\n",
      "Loading file data: ARM_60_2024-08.csv\n",
      "File ARM_60_2024-08 found.\n",
      "Searching for file ARM_60_2024-09\n",
      "Loading file data: ARM_60_2024-09.csv\n",
      "File ARM_60_2024-09 found.\n",
      "Searching for file ARM_60_2024-10\n",
      "Loading file data: ARM_60_2024-10.csv\n",
      "File ARM_60_2024-10 found.\n",
      "Searching for file ARM_60_2024-11\n",
      "Loading file data: ARM_60_2024-11.csv\n",
      "File ARM_60_2024-11 found.\n",
      "Searching for file ARM_60_2024-12\n",
      "Loading file data: ARM_60_2024-12.csv\n",
      "File ARM_60_2024-12 found.\n",
      "Searching for file ARM_60_2025-01\n",
      "Loading file data: ARM_60_2025-01.csv\n",
      "File ARM_60_2025-01 found.\n",
      "Searching for file GOOG_1_2022-01\n",
      "Loading file data: GOOG_1_2022-01.csv\n",
      "File GOOG_1_2022-01 found.\n",
      "Searching for file GOOG_1_2022-02\n",
      "Loading file data: GOOG_1_2022-02.csv\n",
      "File GOOG_1_2022-02 found.\n",
      "Searching for file GOOG_1_2022-03\n",
      "Loading file data: GOOG_1_2022-03.csv\n",
      "File GOOG_1_2022-03 found.\n",
      "Searching for file GOOG_1_2022-04\n",
      "Loading file data: GOOG_1_2022-04.csv\n",
      "File GOOG_1_2022-04 found.\n",
      "Searching for file GOOG_1_2022-05\n",
      "Loading file data: GOOG_1_2022-05.csv\n",
      "File GOOG_1_2022-05 found.\n",
      "Searching for file GOOG_1_2022-06\n",
      "Loading file data: GOOG_1_2022-06.csv\n",
      "File GOOG_1_2022-06 found.\n",
      "Searching for file GOOG_1_2022-07\n",
      "Loading file data: GOOG_1_2022-07.csv\n",
      "File GOOG_1_2022-07 found.\n",
      "Searching for file GOOG_1_2022-08\n",
      "Loading file data: GOOG_1_2022-08.csv\n",
      "File GOOG_1_2022-08 found.\n",
      "Searching for file GOOG_1_2022-09\n",
      "Loading file data: GOOG_1_2022-09.csv\n",
      "File GOOG_1_2022-09 found.\n",
      "Searching for file GOOG_1_2022-10\n",
      "Loading file data: GOOG_1_2022-10.csv\n",
      "File GOOG_1_2022-10 found.\n",
      "Searching for file GOOG_1_2022-11\n",
      "Loading file data: GOOG_1_2022-11.csv\n",
      "File GOOG_1_2022-11 found.\n",
      "Searching for file GOOG_1_2022-12\n",
      "Loading file data: GOOG_1_2022-12.csv\n",
      "File GOOG_1_2022-12 found.\n",
      "Searching for file GOOG_1_2023-07\n",
      "Loading file data: GOOG_1_2023-07.csv\n",
      "File GOOG_1_2023-07 found.\n",
      "Searching for file GOOG_1_2023-08\n",
      "Loading file data: GOOG_1_2023-08.csv\n",
      "File GOOG_1_2023-08 found.\n",
      "Searching for file GOOG_1_2023-09\n",
      "Loading file data: GOOG_1_2023-09.csv\n",
      "File GOOG_1_2023-09 found.\n",
      "Searching for file GOOG_1_2023-10\n",
      "Loading file data: GOOG_1_2023-10.csv\n",
      "File GOOG_1_2023-10 found.\n",
      "Searching for file GOOG_1_2023-11\n",
      "Loading file data: GOOG_1_2023-11.csv\n",
      "File GOOG_1_2023-11 found.\n",
      "Searching for file GOOG_1_2023-12\n",
      "Loading file data: GOOG_1_2023-12.csv\n",
      "File GOOG_1_2023-12 found.\n",
      "Searching for file GOOG_1_2025-01\n",
      "Loading file data: GOOG_1_2025-01.csv\n",
      "File GOOG_1_2025-01 found.\n",
      "Searching for file GOOG_5_2022-01\n",
      "Loading file data: GOOG_5_2022-01.csv\n",
      "File GOOG_5_2022-01 found.\n",
      "Searching for file GOOG_5_2022-02\n",
      "Loading file data: GOOG_5_2022-02.csv\n",
      "File GOOG_5_2022-02 found.\n",
      "Searching for file GOOG_5_2022-03\n",
      "Loading file data: GOOG_5_2022-03.csv\n",
      "File GOOG_5_2022-03 found.\n",
      "Searching for file GOOG_5_2022-04\n",
      "Loading file data: GOOG_5_2022-04.csv\n",
      "File GOOG_5_2022-04 found.\n",
      "Searching for file GOOG_5_2022-05\n",
      "Loading file data: GOOG_5_2022-05.csv\n",
      "File GOOG_5_2022-05 found.\n",
      "Searching for file GOOG_5_2022-06\n",
      "Loading file data: GOOG_5_2022-06.csv\n",
      "File GOOG_5_2022-06 found.\n",
      "Searching for file GOOG_5_2022-07\n",
      "Loading file data: GOOG_5_2022-07.csv\n",
      "File GOOG_5_2022-07 found.\n",
      "Searching for file GOOG_5_2022-08\n",
      "Loading file data: GOOG_5_2022-08.csv\n",
      "File GOOG_5_2022-08 found.\n",
      "Searching for file GOOG_5_2022-09\n",
      "Loading file data: GOOG_5_2022-09.csv\n",
      "File GOOG_5_2022-09 found.\n",
      "Searching for file GOOG_5_2022-10\n",
      "Loading file data: GOOG_5_2022-10.csv\n",
      "File GOOG_5_2022-10 found.\n",
      "Searching for file GOOG_5_2022-11\n",
      "Loading file data: GOOG_5_2022-11.csv\n",
      "File GOOG_5_2022-11 found.\n",
      "Searching for file GOOG_5_2022-12\n",
      "Loading file data: GOOG_5_2022-12.csv\n",
      "File GOOG_5_2022-12 found.\n",
      "Searching for file GOOG_5_2023-07\n",
      "Loading file data: GOOG_5_2023-07.csv\n",
      "File GOOG_5_2023-07 found.\n",
      "Searching for file GOOG_5_2023-08\n",
      "Loading file data: GOOG_5_2023-08.csv\n",
      "File GOOG_5_2023-08 found.\n",
      "Searching for file GOOG_5_2023-09\n",
      "Loading file data: GOOG_5_2023-09.csv\n",
      "File GOOG_5_2023-09 found.\n",
      "Searching for file GOOG_5_2023-10\n",
      "Loading file data: GOOG_5_2023-10.csv\n",
      "File GOOG_5_2023-10 found.\n",
      "Searching for file GOOG_5_2023-11\n",
      "Loading file data: GOOG_5_2023-11.csv\n",
      "File GOOG_5_2023-11 found.\n",
      "Searching for file GOOG_5_2023-12\n",
      "Loading file data: GOOG_5_2023-12.csv\n",
      "File GOOG_5_2023-12 found.\n",
      "Searching for file GOOG_5_2025-01\n",
      "Loading file data: GOOG_5_2025-01.csv\n",
      "File GOOG_5_2025-01 found.\n",
      "Searching for file GOOG_15_2022-01\n",
      "Loading file data: GOOG_15_2022-01.csv\n",
      "File GOOG_15_2022-01 found.\n",
      "Searching for file GOOG_15_2022-02\n",
      "Loading file data: GOOG_15_2022-02.csv\n",
      "File GOOG_15_2022-02 found.\n",
      "Searching for file GOOG_15_2022-03\n",
      "Loading file data: GOOG_15_2022-03.csv\n",
      "File GOOG_15_2022-03 found.\n",
      "Searching for file GOOG_15_2022-04\n",
      "Loading file data: GOOG_15_2022-04.csv\n",
      "File GOOG_15_2022-04 found.\n",
      "Searching for file GOOG_15_2022-05\n",
      "Loading file data: GOOG_15_2022-05.csv\n",
      "File GOOG_15_2022-05 found.\n",
      "Searching for file GOOG_15_2022-06\n",
      "Loading file data: GOOG_15_2022-06.csv\n",
      "File GOOG_15_2022-06 found.\n",
      "Searching for file GOOG_15_2022-07\n",
      "Loading file data: GOOG_15_2022-07.csv\n",
      "File GOOG_15_2022-07 found.\n",
      "Searching for file GOOG_15_2022-08\n",
      "Loading file data: GOOG_15_2022-08.csv\n",
      "File GOOG_15_2022-08 found.\n",
      "Searching for file GOOG_15_2022-09\n",
      "Loading file data: GOOG_15_2022-09.csv\n",
      "File GOOG_15_2022-09 found.\n",
      "Searching for file GOOG_15_2022-10\n",
      "Loading file data: GOOG_15_2022-10.csv\n",
      "File GOOG_15_2022-10 found.\n",
      "Searching for file GOOG_15_2022-11\n",
      "Loading file data: GOOG_15_2022-11.csv\n",
      "File GOOG_15_2022-11 found.\n",
      "Searching for file GOOG_15_2022-12\n",
      "Loading file data: GOOG_15_2022-12.csv\n",
      "File GOOG_15_2022-12 found.\n",
      "Searching for file GOOG_15_2023-07\n",
      "Loading file data: GOOG_15_2023-07.csv\n",
      "File GOOG_15_2023-07 found.\n",
      "Searching for file GOOG_15_2023-08\n",
      "Loading file data: GOOG_15_2023-08.csv\n",
      "File GOOG_15_2023-08 found.\n",
      "Searching for file GOOG_15_2023-09\n",
      "Loading file data: GOOG_15_2023-09.csv\n",
      "File GOOG_15_2023-09 found.\n",
      "Searching for file GOOG_15_2023-10\n",
      "Loading file data: GOOG_15_2023-10.csv\n",
      "File GOOG_15_2023-10 found.\n",
      "Searching for file GOOG_15_2023-11\n",
      "Loading file data: GOOG_15_2023-11.csv\n",
      "File GOOG_15_2023-11 found.\n",
      "Searching for file GOOG_15_2023-12\n",
      "Loading file data: GOOG_15_2023-12.csv\n",
      "File GOOG_15_2023-12 found.\n",
      "Searching for file GOOG_15_2025-01\n",
      "Loading file data: GOOG_15_2025-01.csv\n",
      "File GOOG_15_2025-01 found.\n",
      "Searching for file GOOG_30_2022-01\n",
      "Loading file data: GOOG_30_2022-01.csv\n",
      "File GOOG_30_2022-01 found.\n",
      "Searching for file GOOG_30_2022-02\n",
      "Loading file data: GOOG_30_2022-02.csv\n",
      "File GOOG_30_2022-02 found.\n",
      "Searching for file GOOG_30_2022-03\n",
      "Loading file data: GOOG_30_2022-03.csv\n",
      "File GOOG_30_2022-03 found.\n",
      "Searching for file GOOG_30_2022-04\n",
      "Loading file data: GOOG_30_2022-04.csv\n",
      "File GOOG_30_2022-04 found.\n",
      "Searching for file GOOG_30_2022-05\n",
      "Loading file data: GOOG_30_2022-05.csv\n",
      "File GOOG_30_2022-05 found.\n",
      "Searching for file GOOG_30_2022-06\n",
      "Loading file data: GOOG_30_2022-06.csv\n",
      "File GOOG_30_2022-06 found.\n",
      "Searching for file GOOG_30_2022-07\n",
      "Loading file data: GOOG_30_2022-07.csv\n",
      "File GOOG_30_2022-07 found.\n",
      "Searching for file GOOG_30_2022-08\n",
      "Loading file data: GOOG_30_2022-08.csv\n",
      "File GOOG_30_2022-08 found.\n",
      "Searching for file GOOG_30_2022-09\n",
      "Loading file data: GOOG_30_2022-09.csv\n",
      "File GOOG_30_2022-09 found.\n",
      "Searching for file GOOG_30_2022-10\n",
      "Loading file data: GOOG_30_2022-10.csv\n",
      "File GOOG_30_2022-10 found.\n",
      "Searching for file GOOG_30_2022-11\n",
      "Loading file data: GOOG_30_2022-11.csv\n",
      "File GOOG_30_2022-11 found.\n",
      "Searching for file GOOG_30_2022-12\n",
      "Loading file data: GOOG_30_2022-12.csv\n",
      "File GOOG_30_2022-12 found.\n",
      "Searching for file GOOG_30_2023-07\n",
      "Loading file data: GOOG_30_2023-07.csv\n",
      "File GOOG_30_2023-07 found.\n",
      "Searching for file GOOG_30_2023-08\n",
      "Loading file data: GOOG_30_2023-08.csv\n",
      "File GOOG_30_2023-08 found.\n",
      "Searching for file GOOG_30_2023-09\n",
      "Loading file data: GOOG_30_2023-09.csv\n",
      "File GOOG_30_2023-09 found.\n",
      "Searching for file GOOG_30_2023-10\n",
      "Loading file data: GOOG_30_2023-10.csv\n",
      "File GOOG_30_2023-10 found.\n",
      "Searching for file GOOG_30_2023-11\n",
      "Loading file data: GOOG_30_2023-11.csv\n",
      "File GOOG_30_2023-11 found.\n",
      "Searching for file GOOG_30_2023-12\n",
      "Loading file data: GOOG_30_2023-12.csv\n",
      "File GOOG_30_2023-12 found.\n",
      "Searching for file GOOG_30_2025-01\n",
      "Loading file data: GOOG_30_2025-01.csv\n",
      "File GOOG_30_2025-01 found.\n",
      "Searching for file GOOG_60_2022-01\n",
      "Loading file data: GOOG_60_2022-01.csv\n",
      "File GOOG_60_2022-01 found.\n",
      "Searching for file GOOG_60_2022-02\n",
      "Loading file data: GOOG_60_2022-02.csv\n",
      "File GOOG_60_2022-02 found.\n",
      "Searching for file GOOG_60_2022-03\n",
      "Loading file data: GOOG_60_2022-03.csv\n",
      "File GOOG_60_2022-03 found.\n",
      "Searching for file GOOG_60_2022-04\n",
      "Loading file data: GOOG_60_2022-04.csv\n",
      "File GOOG_60_2022-04 found.\n",
      "Searching for file GOOG_60_2022-05\n",
      "Loading file data: GOOG_60_2022-05.csv\n",
      "File GOOG_60_2022-05 found.\n",
      "Searching for file GOOG_60_2022-06\n",
      "Loading file data: GOOG_60_2022-06.csv\n",
      "File GOOG_60_2022-06 found.\n",
      "Searching for file GOOG_60_2022-07\n",
      "Loading file data: GOOG_60_2022-07.csv\n",
      "File GOOG_60_2022-07 found.\n",
      "Searching for file GOOG_60_2022-08\n",
      "Loading file data: GOOG_60_2022-08.csv\n",
      "File GOOG_60_2022-08 found.\n",
      "Searching for file GOOG_60_2022-09\n",
      "Loading file data: GOOG_60_2022-09.csv\n",
      "File GOOG_60_2022-09 found.\n",
      "Searching for file GOOG_60_2022-10\n",
      "Loading file data: GOOG_60_2022-10.csv\n",
      "File GOOG_60_2022-10 found.\n",
      "Searching for file GOOG_60_2022-11\n",
      "Loading file data: GOOG_60_2022-11.csv\n",
      "File GOOG_60_2022-11 found.\n",
      "Searching for file GOOG_60_2022-12\n",
      "Loading file data: GOOG_60_2022-12.csv\n",
      "File GOOG_60_2022-12 found.\n",
      "Searching for file GOOG_60_2023-07\n",
      "Loading file data: GOOG_60_2023-07.csv\n",
      "File GOOG_60_2023-07 found.\n",
      "Searching for file GOOG_60_2023-08\n",
      "Loading file data: GOOG_60_2023-08.csv\n",
      "File GOOG_60_2023-08 found.\n",
      "Searching for file GOOG_60_2023-09\n",
      "Loading file data: GOOG_60_2023-09.csv\n",
      "File GOOG_60_2023-09 found.\n",
      "Searching for file GOOG_60_2023-10\n",
      "Loading file data: GOOG_60_2023-10.csv\n",
      "File GOOG_60_2023-10 found.\n",
      "Searching for file GOOG_60_2023-11\n",
      "Loading file data: GOOG_60_2023-11.csv\n",
      "File GOOG_60_2023-11 found.\n",
      "Searching for file GOOG_60_2023-12\n",
      "Loading file data: GOOG_60_2023-12.csv\n",
      "File GOOG_60_2023-12 found.\n",
      "Searching for file GOOG_60_2025-01\n",
      "Loading file data: GOOG_60_2025-01.csv\n",
      "File GOOG_60_2025-01 found.\n",
      "Searching for file KO_1_2022-01\n",
      "Loading file data: KO_1_2022-01.csv\n",
      "File KO_1_2022-01 found.\n",
      "Searching for file KO_1_2022-02\n",
      "Loading file data: KO_1_2022-02.csv\n",
      "File KO_1_2022-02 found.\n",
      "Searching for file KO_1_2022-03\n",
      "Loading file data: KO_1_2022-03.csv\n",
      "File KO_1_2022-03 found.\n",
      "Searching for file KO_1_2022-04\n",
      "Loading file data: KO_1_2022-04.csv\n",
      "File KO_1_2022-04 found.\n",
      "Searching for file KO_1_2022-05\n",
      "Loading file data: KO_1_2022-05.csv\n",
      "File KO_1_2022-05 found.\n",
      "Searching for file KO_1_2022-06\n",
      "Loading file data: KO_1_2022-06.csv\n",
      "File KO_1_2022-06 found.\n",
      "Searching for file KO_1_2022-07\n",
      "Loading file data: KO_1_2022-07.csv\n",
      "File KO_1_2022-07 found.\n",
      "Searching for file KO_1_2022-08\n",
      "Loading file data: KO_1_2022-08.csv\n",
      "File KO_1_2022-08 found.\n",
      "Searching for file KO_1_2022-09\n",
      "Loading file data: KO_1_2022-09.csv\n",
      "File KO_1_2022-09 found.\n",
      "Searching for file KO_1_2022-10\n",
      "Loading file data: KO_1_2022-10.csv\n",
      "File KO_1_2022-10 found.\n",
      "Searching for file KO_1_2022-11\n",
      "Loading file data: KO_1_2022-11.csv\n",
      "File KO_1_2022-11 found.\n",
      "Searching for file KO_1_2022-12\n",
      "Loading file data: KO_1_2022-12.csv\n",
      "File KO_1_2022-12 found.\n",
      "Searching for file KO_1_2023-01\n",
      "Loading file data: KO_1_2023-01.csv\n",
      "File KO_1_2023-01 found.\n",
      "Searching for file KO_1_2023-02\n",
      "Loading file data: KO_1_2023-02.csv\n",
      "File KO_1_2023-02 found.\n",
      "Searching for file KO_1_2023-03\n",
      "Loading file data: KO_1_2023-03.csv\n",
      "File KO_1_2023-03 found.\n",
      "Searching for file KO_1_2023-04\n",
      "Loading file data: KO_1_2023-04.csv\n",
      "File KO_1_2023-04 found.\n",
      "Searching for file KO_1_2023-05\n",
      "Loading file data: KO_1_2023-05.csv\n",
      "File KO_1_2023-05 found.\n",
      "Searching for file KO_1_2023-06\n",
      "Loading file data: KO_1_2023-06.csv\n",
      "File KO_1_2023-06 found.\n",
      "Searching for file KO_1_2023-07\n",
      "Loading file data: KO_1_2023-07.csv\n",
      "File KO_1_2023-07 found.\n",
      "Searching for file KO_1_2023-08\n",
      "Loading file data: KO_1_2023-08.csv\n",
      "File KO_1_2023-08 found.\n",
      "Searching for file KO_1_2023-09\n",
      "Loading file data: KO_1_2023-09.csv\n",
      "File KO_1_2023-09 found.\n",
      "Searching for file KO_1_2023-10\n",
      "Loading file data: KO_1_2023-10.csv\n",
      "File KO_1_2023-10 found.\n",
      "Searching for file KO_1_2023-11\n",
      "Loading file data: KO_1_2023-11.csv\n",
      "File KO_1_2023-11 found.\n",
      "Searching for file KO_1_2023-12\n",
      "Loading file data: KO_1_2023-12.csv\n",
      "File KO_1_2023-12 found.\n",
      "Searching for file KO_1_2024-01\n",
      "Loading file data: KO_1_2024-01.csv\n",
      "File KO_1_2024-01 found.\n",
      "Searching for file KO_1_2024-02\n",
      "Loading file data: KO_1_2024-02.csv\n",
      "File KO_1_2024-02 found.\n",
      "Searching for file KO_1_2024-03\n",
      "Loading file data: KO_1_2024-03.csv\n",
      "File KO_1_2024-03 found.\n",
      "Searching for file KO_1_2024-04\n",
      "Loading file data: KO_1_2024-04.csv\n",
      "File KO_1_2024-04 found.\n",
      "Searching for file KO_1_2024-05\n",
      "Loading file data: KO_1_2024-05.csv\n",
      "File KO_1_2024-05 found.\n",
      "Searching for file KO_1_2024-06\n",
      "Loading file data: KO_1_2024-06.csv\n",
      "File KO_1_2024-06 found.\n",
      "Searching for file KO_1_2024-07\n",
      "Loading file data: KO_1_2024-07.csv\n",
      "File KO_1_2024-07 found.\n",
      "Searching for file KO_1_2024-08\n",
      "Loading file data: KO_1_2024-08.csv\n",
      "File KO_1_2024-08 found.\n",
      "Searching for file KO_1_2024-09\n",
      "Loading file data: KO_1_2024-09.csv\n",
      "File KO_1_2024-09 found.\n",
      "Searching for file KO_1_2024-10\n",
      "Loading file data: KO_1_2024-10.csv\n",
      "File KO_1_2024-10 found.\n",
      "Searching for file KO_1_2024-11\n",
      "Loading file data: KO_1_2024-11.csv\n",
      "File KO_1_2024-11 found.\n",
      "Searching for file KO_1_2024-12\n",
      "Loading file data: KO_1_2024-12.csv\n",
      "File KO_1_2024-12 found.\n",
      "Searching for file KO_1_2025-01\n",
      "Loading file data: KO_1_2025-01.csv\n",
      "File KO_1_2025-01 found.\n",
      "Searching for file KO_5_2022-01\n",
      "Loading file data: KO_5_2022-01.csv\n",
      "File KO_5_2022-01 found.\n",
      "Searching for file KO_5_2022-02\n",
      "Loading file data: KO_5_2022-02.csv\n",
      "File KO_5_2022-02 found.\n",
      "Searching for file KO_5_2022-03\n",
      "Loading file data: KO_5_2022-03.csv\n",
      "File KO_5_2022-03 found.\n",
      "Searching for file KO_5_2022-04\n",
      "Loading file data: KO_5_2022-04.csv\n",
      "File KO_5_2022-04 found.\n",
      "Searching for file KO_5_2022-05\n",
      "Loading file data: KO_5_2022-05.csv\n",
      "File KO_5_2022-05 found.\n",
      "Searching for file KO_5_2022-06\n",
      "Loading file data: KO_5_2022-06.csv\n",
      "File KO_5_2022-06 found.\n",
      "Searching for file KO_5_2022-07\n",
      "Loading file data: KO_5_2022-07.csv\n",
      "File KO_5_2022-07 found.\n",
      "Searching for file KO_5_2022-08\n",
      "Loading file data: KO_5_2022-08.csv\n",
      "File KO_5_2022-08 found.\n",
      "Searching for file KO_5_2022-09\n",
      "Loading file data: KO_5_2022-09.csv\n",
      "File KO_5_2022-09 found.\n",
      "Searching for file KO_5_2022-10\n",
      "Loading file data: KO_5_2022-10.csv\n",
      "File KO_5_2022-10 found.\n",
      "Searching for file KO_5_2022-11\n",
      "Loading file data: KO_5_2022-11.csv\n",
      "File KO_5_2022-11 found.\n",
      "Searching for file KO_5_2022-12\n",
      "Loading file data: KO_5_2022-12.csv\n",
      "File KO_5_2022-12 found.\n",
      "Searching for file KO_5_2023-01\n",
      "Loading file data: KO_5_2023-01.csv\n",
      "File KO_5_2023-01 found.\n",
      "Searching for file KO_5_2023-02\n",
      "Loading file data: KO_5_2023-02.csv\n",
      "File KO_5_2023-02 found.\n",
      "Searching for file KO_5_2023-03\n",
      "Loading file data: KO_5_2023-03.csv\n",
      "File KO_5_2023-03 found.\n",
      "Searching for file KO_5_2023-04\n",
      "Loading file data: KO_5_2023-04.csv\n",
      "File KO_5_2023-04 found.\n",
      "Searching for file KO_5_2023-05\n",
      "Loading file data: KO_5_2023-05.csv\n",
      "File KO_5_2023-05 found.\n",
      "Searching for file KO_5_2023-06\n",
      "Loading file data: KO_5_2023-06.csv\n",
      "File KO_5_2023-06 found.\n",
      "Searching for file KO_5_2023-07\n",
      "Loading file data: KO_5_2023-07.csv\n",
      "File KO_5_2023-07 found.\n",
      "Searching for file KO_5_2023-08\n",
      "Loading file data: KO_5_2023-08.csv\n",
      "File KO_5_2023-08 found.\n",
      "Searching for file KO_5_2023-09\n",
      "Loading file data: KO_5_2023-09.csv\n",
      "File KO_5_2023-09 found.\n",
      "Searching for file KO_5_2023-10\n",
      "Loading file data: KO_5_2023-10.csv\n",
      "File KO_5_2023-10 found.\n",
      "Searching for file KO_5_2023-11\n",
      "Loading file data: KO_5_2023-11.csv\n",
      "File KO_5_2023-11 found.\n",
      "Searching for file KO_5_2023-12\n",
      "Loading file data: KO_5_2023-12.csv\n",
      "File KO_5_2023-12 found.\n",
      "Searching for file KO_5_2024-01\n",
      "Loading file data: KO_5_2024-01.csv\n",
      "File KO_5_2024-01 found.\n",
      "Searching for file KO_5_2024-02\n",
      "Loading file data: KO_5_2024-02.csv\n",
      "File KO_5_2024-02 found.\n",
      "Searching for file KO_5_2024-03\n",
      "Loading file data: KO_5_2024-03.csv\n",
      "File KO_5_2024-03 found.\n",
      "Searching for file KO_5_2024-04\n",
      "Loading file data: KO_5_2024-04.csv\n",
      "File KO_5_2024-04 found.\n",
      "Searching for file KO_5_2024-05\n",
      "Loading file data: KO_5_2024-05.csv\n",
      "File KO_5_2024-05 found.\n",
      "Searching for file KO_5_2024-06\n",
      "Loading file data: KO_5_2024-06.csv\n",
      "File KO_5_2024-06 found.\n",
      "Searching for file KO_5_2024-07\n",
      "Loading file data: KO_5_2024-07.csv\n",
      "File KO_5_2024-07 found.\n",
      "Searching for file KO_5_2024-08\n",
      "Loading file data: KO_5_2024-08.csv\n",
      "File KO_5_2024-08 found.\n",
      "Searching for file KO_5_2024-09\n",
      "Loading file data: KO_5_2024-09.csv\n",
      "File KO_5_2024-09 found.\n",
      "Searching for file KO_5_2024-10\n",
      "Loading file data: KO_5_2024-10.csv\n",
      "File KO_5_2024-10 found.\n",
      "Searching for file KO_5_2024-11\n",
      "Loading file data: KO_5_2024-11.csv\n",
      "File KO_5_2024-11 found.\n",
      "Searching for file KO_5_2024-12\n",
      "Loading file data: KO_5_2024-12.csv\n",
      "File KO_5_2024-12 found.\n",
      "Searching for file KO_5_2025-01\n",
      "Loading file data: KO_5_2025-01.csv\n",
      "File KO_5_2025-01 found.\n",
      "Searching for file KO_15_2022-01\n",
      "Loading file data: KO_15_2022-01.csv\n",
      "File KO_15_2022-01 found.\n",
      "Searching for file KO_15_2022-02\n",
      "Loading file data: KO_15_2022-02.csv\n",
      "File KO_15_2022-02 found.\n",
      "Searching for file KO_15_2022-03\n",
      "Loading file data: KO_15_2022-03.csv\n",
      "File KO_15_2022-03 found.\n",
      "Searching for file KO_15_2022-04\n",
      "Loading file data: KO_15_2022-04.csv\n",
      "File KO_15_2022-04 found.\n",
      "Searching for file KO_15_2022-05\n",
      "Loading file data: KO_15_2022-05.csv\n",
      "File KO_15_2022-05 found.\n",
      "Searching for file KO_15_2022-06\n",
      "Loading file data: KO_15_2022-06.csv\n",
      "File KO_15_2022-06 found.\n",
      "Searching for file KO_15_2022-07\n",
      "Loading file data: KO_15_2022-07.csv\n",
      "File KO_15_2022-07 found.\n",
      "Searching for file KO_15_2022-08\n",
      "Loading file data: KO_15_2022-08.csv\n",
      "File KO_15_2022-08 found.\n",
      "Searching for file KO_15_2022-09\n",
      "Loading file data: KO_15_2022-09.csv\n",
      "File KO_15_2022-09 found.\n",
      "Searching for file KO_15_2022-10\n",
      "Loading file data: KO_15_2022-10.csv\n",
      "File KO_15_2022-10 found.\n",
      "Searching for file KO_15_2022-11\n",
      "Loading file data: KO_15_2022-11.csv\n",
      "File KO_15_2022-11 found.\n",
      "Searching for file KO_15_2022-12\n",
      "Loading file data: KO_15_2022-12.csv\n",
      "File KO_15_2022-12 found.\n",
      "Searching for file KO_15_2023-01\n",
      "Loading file data: KO_15_2023-01.csv\n",
      "File KO_15_2023-01 found.\n",
      "Searching for file KO_15_2023-02\n",
      "Loading file data: KO_15_2023-02.csv\n",
      "File KO_15_2023-02 found.\n",
      "Searching for file KO_15_2023-03\n",
      "Loading file data: KO_15_2023-03.csv\n",
      "File KO_15_2023-03 found.\n",
      "Searching for file KO_15_2023-04\n",
      "Loading file data: KO_15_2023-04.csv\n",
      "File KO_15_2023-04 found.\n",
      "Searching for file KO_15_2023-05\n",
      "Loading file data: KO_15_2023-05.csv\n",
      "File KO_15_2023-05 found.\n",
      "Searching for file KO_15_2023-06\n",
      "Loading file data: KO_15_2023-06.csv\n",
      "File KO_15_2023-06 found.\n",
      "Searching for file KO_15_2023-07\n",
      "Loading file data: KO_15_2023-07.csv\n",
      "File KO_15_2023-07 found.\n",
      "Searching for file KO_15_2023-08\n",
      "Loading file data: KO_15_2023-08.csv\n",
      "File KO_15_2023-08 found.\n",
      "Searching for file KO_15_2023-09\n",
      "Loading file data: KO_15_2023-09.csv\n",
      "File KO_15_2023-09 found.\n",
      "Searching for file KO_15_2023-10\n",
      "Loading file data: KO_15_2023-10.csv\n",
      "File KO_15_2023-10 found.\n",
      "Searching for file KO_15_2023-11\n",
      "Loading file data: KO_15_2023-11.csv\n",
      "File KO_15_2023-11 found.\n",
      "Searching for file KO_15_2023-12\n",
      "Loading file data: KO_15_2023-12.csv\n",
      "File KO_15_2023-12 found.\n",
      "Searching for file KO_15_2024-01\n",
      "Loading file data: KO_15_2024-01.csv\n",
      "File KO_15_2024-01 found.\n",
      "Searching for file KO_15_2024-02\n",
      "Loading file data: KO_15_2024-02.csv\n",
      "File KO_15_2024-02 found.\n",
      "Searching for file KO_15_2024-03\n",
      "Loading file data: KO_15_2024-03.csv\n",
      "File KO_15_2024-03 found.\n",
      "Searching for file KO_15_2024-04\n",
      "Loading file data: KO_15_2024-04.csv\n",
      "File KO_15_2024-04 found.\n",
      "Searching for file KO_15_2024-05\n",
      "Loading file data: KO_15_2024-05.csv\n",
      "File KO_15_2024-05 found.\n",
      "Searching for file KO_15_2024-06\n",
      "Loading file data: KO_15_2024-06.csv\n",
      "File KO_15_2024-06 found.\n",
      "Searching for file KO_15_2024-07\n",
      "Loading file data: KO_15_2024-07.csv\n",
      "File KO_15_2024-07 found.\n",
      "Searching for file KO_15_2024-08\n",
      "Loading file data: KO_15_2024-08.csv\n",
      "File KO_15_2024-08 found.\n",
      "Searching for file KO_15_2024-09\n",
      "Loading file data: KO_15_2024-09.csv\n",
      "File KO_15_2024-09 found.\n",
      "Searching for file KO_15_2024-10\n",
      "Loading file data: KO_15_2024-10.csv\n",
      "File KO_15_2024-10 found.\n",
      "Searching for file KO_15_2024-11\n",
      "Loading file data: KO_15_2024-11.csv\n",
      "File KO_15_2024-11 found.\n",
      "Searching for file KO_15_2024-12\n",
      "Loading file data: KO_15_2024-12.csv\n",
      "File KO_15_2024-12 found.\n",
      "Searching for file KO_15_2025-01\n",
      "Loading file data: KO_15_2025-01.csv\n",
      "File KO_15_2025-01 found.\n",
      "Searching for file KO_30_2022-01\n",
      "Loading file data: KO_30_2022-01.csv\n",
      "File KO_30_2022-01 found.\n",
      "Searching for file KO_30_2022-02\n",
      "Loading file data: KO_30_2022-02.csv\n",
      "File KO_30_2022-02 found.\n",
      "Searching for file KO_30_2022-03\n",
      "Loading file data: KO_30_2022-03.csv\n",
      "File KO_30_2022-03 found.\n",
      "Searching for file KO_30_2022-04\n",
      "Loading file data: KO_30_2022-04.csv\n",
      "File KO_30_2022-04 found.\n",
      "Searching for file KO_30_2022-05\n",
      "Loading file data: KO_30_2022-05.csv\n",
      "File KO_30_2022-05 found.\n",
      "Searching for file KO_30_2022-06\n",
      "Loading file data: KO_30_2022-06.csv\n",
      "File KO_30_2022-06 found.\n",
      "Searching for file KO_30_2022-07\n",
      "Loading file data: KO_30_2022-07.csv\n",
      "File KO_30_2022-07 found.\n",
      "Searching for file KO_30_2022-08\n",
      "Loading file data: KO_30_2022-08.csv\n",
      "File KO_30_2022-08 found.\n",
      "Searching for file KO_30_2022-09\n",
      "Loading file data: KO_30_2022-09.csv\n",
      "File KO_30_2022-09 found.\n",
      "Searching for file KO_30_2022-10\n",
      "Loading file data: KO_30_2022-10.csv\n",
      "File KO_30_2022-10 found.\n",
      "Searching for file KO_30_2022-11\n",
      "Loading file data: KO_30_2022-11.csv\n",
      "File KO_30_2022-11 found.\n",
      "Searching for file KO_30_2022-12\n",
      "Loading file data: KO_30_2022-12.csv\n",
      "File KO_30_2022-12 found.\n",
      "Searching for file KO_30_2023-01\n",
      "Loading file data: KO_30_2023-01.csv\n",
      "File KO_30_2023-01 found.\n",
      "Searching for file KO_30_2023-02\n",
      "Loading file data: KO_30_2023-02.csv\n",
      "File KO_30_2023-02 found.\n",
      "Searching for file KO_30_2023-03\n",
      "Loading file data: KO_30_2023-03.csv\n",
      "File KO_30_2023-03 found.\n",
      "Searching for file KO_30_2023-04\n",
      "Loading file data: KO_30_2023-04.csv\n",
      "File KO_30_2023-04 found.\n",
      "Searching for file KO_30_2023-05\n",
      "Loading file data: KO_30_2023-05.csv\n",
      "File KO_30_2023-05 found.\n",
      "Searching for file KO_30_2023-06\n",
      "Loading file data: KO_30_2023-06.csv\n",
      "File KO_30_2023-06 found.\n",
      "Searching for file KO_30_2023-07\n",
      "Loading file data: KO_30_2023-07.csv\n",
      "File KO_30_2023-07 found.\n",
      "Searching for file KO_30_2023-08\n",
      "Loading file data: KO_30_2023-08.csv\n",
      "File KO_30_2023-08 found.\n",
      "Searching for file KO_30_2023-09\n",
      "Loading file data: KO_30_2023-09.csv\n",
      "File KO_30_2023-09 found.\n",
      "Searching for file KO_30_2023-10\n",
      "Loading file data: KO_30_2023-10.csv\n",
      "File KO_30_2023-10 found.\n",
      "Searching for file KO_30_2023-11\n",
      "Loading file data: KO_30_2023-11.csv\n",
      "File KO_30_2023-11 found.\n",
      "Searching for file KO_30_2023-12\n",
      "Loading file data: KO_30_2023-12.csv\n",
      "File KO_30_2023-12 found.\n",
      "Searching for file KO_30_2024-01\n",
      "Loading file data: KO_30_2024-01.csv\n",
      "File KO_30_2024-01 found.\n",
      "Searching for file KO_30_2024-02\n",
      "Loading file data: KO_30_2024-02.csv\n",
      "File KO_30_2024-02 found.\n",
      "Searching for file KO_30_2024-03\n",
      "Loading file data: KO_30_2024-03.csv\n",
      "File KO_30_2024-03 found.\n",
      "Searching for file KO_30_2024-04\n",
      "Loading file data: KO_30_2024-04.csv\n",
      "File KO_30_2024-04 found.\n",
      "Searching for file KO_30_2024-05\n",
      "Loading file data: KO_30_2024-05.csv\n",
      "File KO_30_2024-05 found.\n",
      "Searching for file KO_30_2024-06\n",
      "Loading file data: KO_30_2024-06.csv\n",
      "File KO_30_2024-06 found.\n",
      "Searching for file KO_30_2024-07\n",
      "Loading file data: KO_30_2024-07.csv\n",
      "File KO_30_2024-07 found.\n",
      "Searching for file KO_30_2024-08\n",
      "Loading file data: KO_30_2024-08.csv\n",
      "File KO_30_2024-08 found.\n",
      "Searching for file KO_30_2024-09\n",
      "Loading file data: KO_30_2024-09.csv\n",
      "File KO_30_2024-09 found.\n",
      "Searching for file KO_30_2024-10\n",
      "Loading file data: KO_30_2024-10.csv\n",
      "File KO_30_2024-10 found.\n",
      "Searching for file KO_30_2024-11\n",
      "Loading file data: KO_30_2024-11.csv\n",
      "File KO_30_2024-11 found.\n",
      "Searching for file KO_30_2024-12\n",
      "Loading file data: KO_30_2024-12.csv\n",
      "File KO_30_2024-12 found.\n",
      "Searching for file KO_30_2025-01\n",
      "Loading file data: KO_30_2025-01.csv\n",
      "File KO_30_2025-01 found.\n",
      "Searching for file KO_60_2022-01\n",
      "Loading file data: KO_60_2022-01.csv\n",
      "File KO_60_2022-01 found.\n",
      "Searching for file KO_60_2022-02\n",
      "Loading file data: KO_60_2022-02.csv\n",
      "File KO_60_2022-02 found.\n",
      "Searching for file KO_60_2022-03\n",
      "Loading file data: KO_60_2022-03.csv\n",
      "File KO_60_2022-03 found.\n",
      "Searching for file KO_60_2022-04\n",
      "Loading file data: KO_60_2022-04.csv\n",
      "File KO_60_2022-04 found.\n",
      "Searching for file KO_60_2022-05\n",
      "Loading file data: KO_60_2022-05.csv\n",
      "File KO_60_2022-05 found.\n",
      "Searching for file KO_60_2022-06\n",
      "Loading file data: KO_60_2022-06.csv\n",
      "File KO_60_2022-06 found.\n",
      "Searching for file KO_60_2022-07\n",
      "Loading file data: KO_60_2022-07.csv\n",
      "File KO_60_2022-07 found.\n",
      "Searching for file KO_60_2022-08\n",
      "Loading file data: KO_60_2022-08.csv\n",
      "File KO_60_2022-08 found.\n",
      "Searching for file KO_60_2022-09\n",
      "Loading file data: KO_60_2022-09.csv\n",
      "File KO_60_2022-09 found.\n",
      "Searching for file KO_60_2022-10\n",
      "Loading file data: KO_60_2022-10.csv\n",
      "File KO_60_2022-10 found.\n",
      "Searching for file KO_60_2022-11\n",
      "Loading file data: KO_60_2022-11.csv\n",
      "File KO_60_2022-11 found.\n",
      "Searching for file KO_60_2022-12\n",
      "Loading file data: KO_60_2022-12.csv\n",
      "File KO_60_2022-12 found.\n",
      "Searching for file KO_60_2023-01\n",
      "Loading file data: KO_60_2023-01.csv\n",
      "File KO_60_2023-01 found.\n",
      "Searching for file KO_60_2023-02\n",
      "Loading file data: KO_60_2023-02.csv\n",
      "File KO_60_2023-02 found.\n",
      "Searching for file KO_60_2023-03\n",
      "Loading file data: KO_60_2023-03.csv\n",
      "File KO_60_2023-03 found.\n",
      "Searching for file KO_60_2023-04\n",
      "Loading file data: KO_60_2023-04.csv\n",
      "File KO_60_2023-04 found.\n",
      "Searching for file KO_60_2023-05\n",
      "Loading file data: KO_60_2023-05.csv\n",
      "File KO_60_2023-05 found.\n",
      "Searching for file KO_60_2023-06\n",
      "Loading file data: KO_60_2023-06.csv\n",
      "File KO_60_2023-06 found.\n",
      "Searching for file KO_60_2023-07\n",
      "Loading file data: KO_60_2023-07.csv\n",
      "File KO_60_2023-07 found.\n",
      "Searching for file KO_60_2023-08\n",
      "Loading file data: KO_60_2023-08.csv\n",
      "File KO_60_2023-08 found.\n",
      "Searching for file KO_60_2023-09\n",
      "Loading file data: KO_60_2023-09.csv\n",
      "File KO_60_2023-09 found.\n",
      "Searching for file KO_60_2023-10\n",
      "Loading file data: KO_60_2023-10.csv\n",
      "File KO_60_2023-10 found.\n",
      "Searching for file KO_60_2023-11\n",
      "Loading file data: KO_60_2023-11.csv\n",
      "File KO_60_2023-11 found.\n",
      "Searching for file KO_60_2023-12\n",
      "Loading file data: KO_60_2023-12.csv\n",
      "File KO_60_2023-12 found.\n",
      "Searching for file KO_60_2024-01\n",
      "Loading file data: KO_60_2024-01.csv\n",
      "File KO_60_2024-01 found.\n",
      "Searching for file KO_60_2024-02\n",
      "Loading file data: KO_60_2024-02.csv\n",
      "File KO_60_2024-02 found.\n",
      "Searching for file KO_60_2024-03\n",
      "Loading file data: KO_60_2024-03.csv\n",
      "File KO_60_2024-03 found.\n",
      "Searching for file KO_60_2024-04\n",
      "Loading file data: KO_60_2024-04.csv\n",
      "File KO_60_2024-04 found.\n",
      "Searching for file KO_60_2024-05\n",
      "Loading file data: KO_60_2024-05.csv\n",
      "File KO_60_2024-05 found.\n",
      "Searching for file KO_60_2024-06\n",
      "Loading file data: KO_60_2024-06.csv\n",
      "File KO_60_2024-06 found.\n",
      "Searching for file KO_60_2024-07\n",
      "Loading file data: KO_60_2024-07.csv\n",
      "File KO_60_2024-07 found.\n",
      "Searching for file KO_60_2024-08\n",
      "Loading file data: KO_60_2024-08.csv\n",
      "File KO_60_2024-08 found.\n",
      "Searching for file KO_60_2024-09\n",
      "Loading file data: KO_60_2024-09.csv\n",
      "File KO_60_2024-09 found.\n",
      "Searching for file KO_60_2024-10\n",
      "Loading file data: KO_60_2024-10.csv\n",
      "File KO_60_2024-10 found.\n",
      "Searching for file KO_60_2024-11\n",
      "Loading file data: KO_60_2024-11.csv\n",
      "File KO_60_2024-11 found.\n",
      "Searching for file KO_60_2024-12\n",
      "Loading file data: KO_60_2024-12.csv\n",
      "File KO_60_2024-12 found.\n",
      "Searching for file KO_60_2025-01\n",
      "Loading file data: KO_60_2025-01.csv\n",
      "File KO_60_2025-01 found.\n",
      "Searching for file MSFT_1_2022-01\n",
      "Loading file data: MSFT_1_2022-01.csv\n",
      "File MSFT_1_2022-01 found.\n",
      "Searching for file MSFT_1_2022-02\n",
      "Loading file data: MSFT_1_2022-02.csv\n",
      "File MSFT_1_2022-02 found.\n",
      "Searching for file MSFT_1_2022-03\n",
      "Loading file data: MSFT_1_2022-03.csv\n",
      "File MSFT_1_2022-03 found.\n",
      "Searching for file MSFT_1_2022-04\n",
      "Loading file data: MSFT_1_2022-04.csv\n",
      "File MSFT_1_2022-04 found.\n",
      "Searching for file MSFT_1_2022-05\n",
      "Loading file data: MSFT_1_2022-05.csv\n",
      "File MSFT_1_2022-05 found.\n",
      "Searching for file MSFT_1_2022-06\n",
      "Loading file data: MSFT_1_2022-06.csv\n",
      "File MSFT_1_2022-06 found.\n",
      "Searching for file MSFT_1_2022-07\n",
      "Loading file data: MSFT_1_2022-07.csv\n",
      "File MSFT_1_2022-07 found.\n",
      "Searching for file MSFT_1_2022-08\n",
      "Loading file data: MSFT_1_2022-08.csv\n",
      "File MSFT_1_2022-08 found.\n",
      "Searching for file MSFT_1_2022-09\n",
      "Loading file data: MSFT_1_2022-09.csv\n",
      "File MSFT_1_2022-09 found.\n",
      "Searching for file MSFT_1_2022-10\n",
      "Loading file data: MSFT_1_2022-10.csv\n",
      "File MSFT_1_2022-10 found.\n",
      "Searching for file MSFT_1_2022-11\n",
      "Loading file data: MSFT_1_2022-11.csv\n",
      "File MSFT_1_2022-11 found.\n",
      "Searching for file MSFT_1_2022-12\n",
      "Loading file data: MSFT_1_2022-12.csv\n",
      "File MSFT_1_2022-12 found.\n",
      "Searching for file MSFT_1_2023-01\n",
      "Loading file data: MSFT_1_2023-01.csv\n",
      "File MSFT_1_2023-01 found.\n",
      "Searching for file MSFT_1_2023-02\n",
      "Loading file data: MSFT_1_2023-02.csv\n",
      "File MSFT_1_2023-02 found.\n",
      "Searching for file MSFT_1_2023-03\n",
      "Loading file data: MSFT_1_2023-03.csv\n",
      "File MSFT_1_2023-03 found.\n",
      "Searching for file MSFT_1_2023-04\n",
      "Loading file data: MSFT_1_2023-04.csv\n",
      "File MSFT_1_2023-04 found.\n",
      "Searching for file MSFT_1_2023-05\n",
      "Loading file data: MSFT_1_2023-05.csv\n",
      "File MSFT_1_2023-05 found.\n",
      "Searching for file MSFT_1_2023-06\n",
      "Loading file data: MSFT_1_2023-06.csv\n",
      "File MSFT_1_2023-06 found.\n",
      "Searching for file MSFT_1_2023-07\n",
      "Loading file data: MSFT_1_2023-07.csv\n",
      "File MSFT_1_2023-07 found.\n",
      "Searching for file MSFT_1_2023-08\n",
      "Loading file data: MSFT_1_2023-08.csv\n",
      "File MSFT_1_2023-08 found.\n",
      "Searching for file MSFT_1_2023-09\n",
      "Loading file data: MSFT_1_2023-09.csv\n",
      "File MSFT_1_2023-09 found.\n",
      "Searching for file MSFT_1_2023-10\n",
      "Loading file data: MSFT_1_2023-10.csv\n",
      "File MSFT_1_2023-10 found.\n",
      "Searching for file MSFT_1_2023-11\n",
      "Loading file data: MSFT_1_2023-11.csv\n",
      "File MSFT_1_2023-11 found.\n",
      "Searching for file MSFT_1_2023-12\n",
      "Loading file data: MSFT_1_2023-12.csv\n",
      "File MSFT_1_2023-12 found.\n",
      "Searching for file MSFT_1_2024-01\n",
      "Loading file data: MSFT_1_2024-01.csv\n",
      "File MSFT_1_2024-01 found.\n",
      "Searching for file MSFT_1_2024-02\n",
      "Loading file data: MSFT_1_2024-02.csv\n",
      "File MSFT_1_2024-02 found.\n",
      "Searching for file MSFT_1_2024-03\n",
      "Loading file data: MSFT_1_2024-03.csv\n",
      "File MSFT_1_2024-03 found.\n",
      "Searching for file MSFT_1_2024-04\n",
      "Loading file data: MSFT_1_2024-04.csv\n",
      "File MSFT_1_2024-04 found.\n",
      "Searching for file MSFT_1_2024-05\n",
      "Loading file data: MSFT_1_2024-05.csv\n",
      "File MSFT_1_2024-05 found.\n",
      "Searching for file MSFT_1_2024-06\n",
      "Loading file data: MSFT_1_2024-06.csv\n",
      "File MSFT_1_2024-06 found.\n",
      "Searching for file MSFT_1_2024-07\n",
      "Loading file data: MSFT_1_2024-07.csv\n",
      "File MSFT_1_2024-07 found.\n",
      "Searching for file MSFT_1_2024-08\n",
      "Loading file data: MSFT_1_2024-08.csv\n",
      "File MSFT_1_2024-08 found.\n",
      "Searching for file MSFT_1_2024-09\n",
      "Loading file data: MSFT_1_2024-09.csv\n",
      "File MSFT_1_2024-09 found.\n",
      "Searching for file MSFT_1_2024-10\n",
      "Loading file data: MSFT_1_2024-10.csv\n",
      "File MSFT_1_2024-10 found.\n",
      "Searching for file MSFT_1_2024-11\n",
      "Loading file data: MSFT_1_2024-11.csv\n",
      "File MSFT_1_2024-11 found.\n",
      "Searching for file MSFT_1_2024-12\n",
      "Loading file data: MSFT_1_2024-12.csv\n",
      "File MSFT_1_2024-12 found.\n",
      "Searching for file MSFT_1_2025-01\n",
      "Loading file data: MSFT_1_2025-01.csv\n",
      "File MSFT_1_2025-01 found.\n",
      "Searching for file MSFT_5_2022-01\n",
      "Loading file data: MSFT_5_2022-01.csv\n",
      "File MSFT_5_2022-01 found.\n",
      "Searching for file MSFT_5_2022-02\n",
      "Loading file data: MSFT_5_2022-02.csv\n",
      "File MSFT_5_2022-02 found.\n",
      "Searching for file MSFT_5_2022-03\n",
      "Loading file data: MSFT_5_2022-03.csv\n",
      "File MSFT_5_2022-03 found.\n",
      "Searching for file MSFT_5_2022-04\n",
      "Loading file data: MSFT_5_2022-04.csv\n",
      "File MSFT_5_2022-04 found.\n",
      "Searching for file MSFT_5_2022-05\n",
      "Loading file data: MSFT_5_2022-05.csv\n",
      "File MSFT_5_2022-05 found.\n",
      "Searching for file MSFT_5_2022-06\n",
      "Loading file data: MSFT_5_2022-06.csv\n",
      "File MSFT_5_2022-06 found.\n",
      "Searching for file MSFT_5_2022-07\n",
      "Loading file data: MSFT_5_2022-07.csv\n",
      "File MSFT_5_2022-07 found.\n",
      "Searching for file MSFT_5_2022-08\n",
      "Loading file data: MSFT_5_2022-08.csv\n",
      "File MSFT_5_2022-08 found.\n",
      "Searching for file MSFT_5_2022-09\n",
      "Loading file data: MSFT_5_2022-09.csv\n",
      "File MSFT_5_2022-09 found.\n",
      "Searching for file MSFT_5_2022-10\n",
      "Loading file data: MSFT_5_2022-10.csv\n",
      "File MSFT_5_2022-10 found.\n",
      "Searching for file MSFT_5_2022-11\n",
      "Loading file data: MSFT_5_2022-11.csv\n",
      "File MSFT_5_2022-11 found.\n",
      "Searching for file MSFT_5_2022-12\n",
      "Loading file data: MSFT_5_2022-12.csv\n",
      "File MSFT_5_2022-12 found.\n",
      "Searching for file MSFT_5_2023-01\n",
      "Loading file data: MSFT_5_2023-01.csv\n",
      "File MSFT_5_2023-01 found.\n",
      "Searching for file MSFT_5_2023-02\n",
      "Loading file data: MSFT_5_2023-02.csv\n",
      "File MSFT_5_2023-02 found.\n",
      "Searching for file MSFT_5_2023-03\n",
      "Loading file data: MSFT_5_2023-03.csv\n",
      "File MSFT_5_2023-03 found.\n",
      "Searching for file MSFT_5_2023-04\n",
      "Loading file data: MSFT_5_2023-04.csv\n",
      "File MSFT_5_2023-04 found.\n",
      "Searching for file MSFT_5_2023-05\n",
      "Loading file data: MSFT_5_2023-05.csv\n",
      "File MSFT_5_2023-05 found.\n",
      "Searching for file MSFT_5_2023-06\n",
      "Loading file data: MSFT_5_2023-06.csv\n",
      "File MSFT_5_2023-06 found.\n",
      "Searching for file MSFT_5_2023-07\n",
      "Loading file data: MSFT_5_2023-07.csv\n",
      "File MSFT_5_2023-07 found.\n",
      "Searching for file MSFT_5_2023-08\n",
      "Loading file data: MSFT_5_2023-08.csv\n",
      "File MSFT_5_2023-08 found.\n",
      "Searching for file MSFT_5_2023-09\n",
      "Loading file data: MSFT_5_2023-09.csv\n",
      "File MSFT_5_2023-09 found.\n",
      "Searching for file MSFT_5_2023-10\n",
      "Loading file data: MSFT_5_2023-10.csv\n",
      "File MSFT_5_2023-10 found.\n",
      "Searching for file MSFT_5_2023-11\n",
      "Loading file data: MSFT_5_2023-11.csv\n",
      "File MSFT_5_2023-11 found.\n",
      "Searching for file MSFT_5_2023-12\n",
      "Loading file data: MSFT_5_2023-12.csv\n",
      "File MSFT_5_2023-12 found.\n",
      "Searching for file MSFT_5_2024-01\n",
      "Loading file data: MSFT_5_2024-01.csv\n",
      "File MSFT_5_2024-01 found.\n",
      "Searching for file MSFT_5_2024-02\n",
      "Loading file data: MSFT_5_2024-02.csv\n",
      "File MSFT_5_2024-02 found.\n",
      "Searching for file MSFT_5_2024-03\n",
      "Loading file data: MSFT_5_2024-03.csv\n",
      "File MSFT_5_2024-03 found.\n",
      "Searching for file MSFT_5_2024-04\n",
      "Loading file data: MSFT_5_2024-04.csv\n",
      "File MSFT_5_2024-04 found.\n",
      "Searching for file MSFT_5_2024-05\n",
      "Loading file data: MSFT_5_2024-05.csv\n",
      "File MSFT_5_2024-05 found.\n",
      "Searching for file MSFT_5_2024-06\n",
      "Loading file data: MSFT_5_2024-06.csv\n",
      "File MSFT_5_2024-06 found.\n",
      "Searching for file MSFT_5_2024-07\n",
      "Loading file data: MSFT_5_2024-07.csv\n",
      "File MSFT_5_2024-07 found.\n",
      "Searching for file MSFT_5_2024-08\n",
      "Loading file data: MSFT_5_2024-08.csv\n",
      "File MSFT_5_2024-08 found.\n",
      "Searching for file MSFT_5_2024-09\n",
      "Loading file data: MSFT_5_2024-09.csv\n",
      "File MSFT_5_2024-09 found.\n",
      "Searching for file MSFT_5_2024-10\n",
      "Loading file data: MSFT_5_2024-10.csv\n",
      "File MSFT_5_2024-10 found.\n",
      "Searching for file MSFT_5_2024-11\n",
      "Loading file data: MSFT_5_2024-11.csv\n",
      "File MSFT_5_2024-11 found.\n",
      "Searching for file MSFT_5_2024-12\n",
      "Loading file data: MSFT_5_2024-12.csv\n",
      "File MSFT_5_2024-12 found.\n",
      "Searching for file MSFT_5_2025-01\n",
      "Loading file data: MSFT_5_2025-01.csv\n",
      "File MSFT_5_2025-01 found.\n",
      "Searching for file MSFT_15_2022-01\n",
      "Loading file data: MSFT_15_2022-01.csv\n",
      "File MSFT_15_2022-01 found.\n",
      "Searching for file MSFT_15_2022-02\n",
      "Loading file data: MSFT_15_2022-02.csv\n",
      "File MSFT_15_2022-02 found.\n",
      "Searching for file MSFT_15_2022-03\n",
      "Loading file data: MSFT_15_2022-03.csv\n",
      "File MSFT_15_2022-03 found.\n",
      "Searching for file MSFT_15_2022-04\n",
      "Loading file data: MSFT_15_2022-04.csv\n",
      "File MSFT_15_2022-04 found.\n",
      "Searching for file MSFT_15_2022-05\n",
      "Loading file data: MSFT_15_2022-05.csv\n",
      "File MSFT_15_2022-05 found.\n",
      "Searching for file MSFT_15_2022-06\n",
      "Loading file data: MSFT_15_2022-06.csv\n",
      "File MSFT_15_2022-06 found.\n",
      "Searching for file MSFT_15_2022-07\n",
      "Loading file data: MSFT_15_2022-07.csv\n",
      "File MSFT_15_2022-07 found.\n",
      "Searching for file MSFT_15_2022-08\n",
      "Loading file data: MSFT_15_2022-08.csv\n",
      "File MSFT_15_2022-08 found.\n",
      "Searching for file MSFT_15_2022-09\n",
      "Loading file data: MSFT_15_2022-09.csv\n",
      "File MSFT_15_2022-09 found.\n",
      "Searching for file MSFT_15_2022-10\n",
      "Loading file data: MSFT_15_2022-10.csv\n",
      "File MSFT_15_2022-10 found.\n",
      "Searching for file MSFT_15_2022-11\n",
      "Loading file data: MSFT_15_2022-11.csv\n",
      "File MSFT_15_2022-11 found.\n",
      "Searching for file MSFT_15_2022-12\n",
      "Loading file data: MSFT_15_2022-12.csv\n",
      "File MSFT_15_2022-12 found.\n",
      "Searching for file MSFT_15_2023-01\n",
      "Loading file data: MSFT_15_2023-01.csv\n",
      "File MSFT_15_2023-01 found.\n",
      "Searching for file MSFT_15_2023-02\n",
      "Loading file data: MSFT_15_2023-02.csv\n",
      "File MSFT_15_2023-02 found.\n",
      "Searching for file MSFT_15_2023-03\n",
      "Loading file data: MSFT_15_2023-03.csv\n",
      "File MSFT_15_2023-03 found.\n",
      "Searching for file MSFT_15_2023-04\n",
      "Loading file data: MSFT_15_2023-04.csv\n",
      "File MSFT_15_2023-04 found.\n",
      "Searching for file MSFT_15_2023-05\n",
      "Loading file data: MSFT_15_2023-05.csv\n",
      "File MSFT_15_2023-05 found.\n",
      "Searching for file MSFT_15_2023-06\n",
      "Loading file data: MSFT_15_2023-06.csv\n",
      "File MSFT_15_2023-06 found.\n",
      "Searching for file MSFT_15_2023-07\n",
      "Loading file data: MSFT_15_2023-07.csv\n",
      "File MSFT_15_2023-07 found.\n",
      "Searching for file MSFT_15_2023-08\n",
      "Loading file data: MSFT_15_2023-08.csv\n",
      "File MSFT_15_2023-08 found.\n",
      "Searching for file MSFT_15_2023-09\n",
      "Loading file data: MSFT_15_2023-09.csv\n",
      "File MSFT_15_2023-09 found.\n",
      "Searching for file MSFT_15_2023-10\n",
      "Loading file data: MSFT_15_2023-10.csv\n",
      "File MSFT_15_2023-10 found.\n",
      "Searching for file MSFT_15_2023-11\n",
      "Loading file data: MSFT_15_2023-11.csv\n",
      "File MSFT_15_2023-11 found.\n",
      "Searching for file MSFT_15_2023-12\n",
      "Loading file data: MSFT_15_2023-12.csv\n",
      "File MSFT_15_2023-12 found.\n",
      "Searching for file MSFT_15_2024-01\n",
      "Loading file data: MSFT_15_2024-01.csv\n",
      "File MSFT_15_2024-01 found.\n",
      "Searching for file MSFT_15_2024-02\n",
      "Loading file data: MSFT_15_2024-02.csv\n",
      "File MSFT_15_2024-02 found.\n",
      "Searching for file MSFT_15_2024-03\n",
      "Loading file data: MSFT_15_2024-03.csv\n",
      "File MSFT_15_2024-03 found.\n",
      "Searching for file MSFT_15_2024-04\n",
      "Loading file data: MSFT_15_2024-04.csv\n",
      "File MSFT_15_2024-04 found.\n",
      "Searching for file MSFT_15_2024-05\n",
      "Loading file data: MSFT_15_2024-05.csv\n",
      "File MSFT_15_2024-05 found.\n",
      "Searching for file MSFT_15_2024-06\n",
      "Loading file data: MSFT_15_2024-06.csv\n",
      "File MSFT_15_2024-06 found.\n",
      "Searching for file MSFT_15_2024-07\n",
      "Loading file data: MSFT_15_2024-07.csv\n",
      "File MSFT_15_2024-07 found.\n",
      "Searching for file MSFT_15_2024-08\n",
      "Loading file data: MSFT_15_2024-08.csv\n",
      "File MSFT_15_2024-08 found.\n",
      "Searching for file MSFT_15_2024-09\n",
      "Loading file data: MSFT_15_2024-09.csv\n",
      "File MSFT_15_2024-09 found.\n",
      "Searching for file MSFT_15_2024-10\n",
      "Loading file data: MSFT_15_2024-10.csv\n",
      "File MSFT_15_2024-10 found.\n",
      "Searching for file MSFT_15_2024-11\n",
      "Loading file data: MSFT_15_2024-11.csv\n",
      "File MSFT_15_2024-11 found.\n",
      "Searching for file MSFT_15_2024-12\n",
      "Loading file data: MSFT_15_2024-12.csv\n",
      "File MSFT_15_2024-12 found.\n",
      "Searching for file MSFT_15_2025-01\n",
      "Loading file data: MSFT_15_2025-01.csv\n",
      "File MSFT_15_2025-01 found.\n",
      "Searching for file MSFT_30_2022-01\n",
      "Loading file data: MSFT_30_2022-01.csv\n",
      "File MSFT_30_2022-01 found.\n",
      "Searching for file MSFT_30_2022-02\n",
      "Loading file data: MSFT_30_2022-02.csv\n",
      "File MSFT_30_2022-02 found.\n",
      "Searching for file MSFT_30_2022-03\n",
      "Loading file data: MSFT_30_2022-03.csv\n",
      "File MSFT_30_2022-03 found.\n",
      "Searching for file MSFT_30_2022-04\n",
      "Loading file data: MSFT_30_2022-04.csv\n",
      "File MSFT_30_2022-04 found.\n",
      "Searching for file MSFT_30_2022-05\n",
      "Loading file data: MSFT_30_2022-05.csv\n",
      "File MSFT_30_2022-05 found.\n",
      "Searching for file MSFT_30_2022-06\n",
      "Loading file data: MSFT_30_2022-06.csv\n",
      "File MSFT_30_2022-06 found.\n",
      "Searching for file MSFT_30_2022-07\n",
      "Loading file data: MSFT_30_2022-07.csv\n",
      "File MSFT_30_2022-07 found.\n",
      "Searching for file MSFT_30_2022-08\n",
      "Loading file data: MSFT_30_2022-08.csv\n",
      "File MSFT_30_2022-08 found.\n",
      "Searching for file MSFT_30_2022-09\n",
      "Loading file data: MSFT_30_2022-09.csv\n",
      "File MSFT_30_2022-09 found.\n",
      "Searching for file MSFT_30_2022-10\n",
      "Loading file data: MSFT_30_2022-10.csv\n",
      "File MSFT_30_2022-10 found.\n",
      "Searching for file MSFT_30_2022-11\n",
      "Loading file data: MSFT_30_2022-11.csv\n",
      "File MSFT_30_2022-11 found.\n",
      "Searching for file MSFT_30_2022-12\n",
      "Loading file data: MSFT_30_2022-12.csv\n",
      "File MSFT_30_2022-12 found.\n",
      "Searching for file MSFT_30_2023-01\n",
      "Loading file data: MSFT_30_2023-01.csv\n",
      "File MSFT_30_2023-01 found.\n",
      "Searching for file MSFT_30_2023-02\n",
      "Loading file data: MSFT_30_2023-02.csv\n",
      "File MSFT_30_2023-02 found.\n",
      "Searching for file MSFT_30_2023-03\n",
      "Loading file data: MSFT_30_2023-03.csv\n",
      "File MSFT_30_2023-03 found.\n",
      "Searching for file MSFT_30_2023-04\n",
      "Loading file data: MSFT_30_2023-04.csv\n",
      "File MSFT_30_2023-04 found.\n",
      "Searching for file MSFT_30_2023-05\n",
      "Loading file data: MSFT_30_2023-05.csv\n",
      "File MSFT_30_2023-05 found.\n",
      "Searching for file MSFT_30_2023-06\n",
      "Loading file data: MSFT_30_2023-06.csv\n",
      "File MSFT_30_2023-06 found.\n",
      "Searching for file MSFT_30_2023-07\n",
      "Loading file data: MSFT_30_2023-07.csv\n",
      "File MSFT_30_2023-07 found.\n",
      "Searching for file MSFT_30_2023-08\n",
      "Loading file data: MSFT_30_2023-08.csv\n",
      "File MSFT_30_2023-08 found.\n",
      "Searching for file MSFT_30_2023-09\n",
      "Loading file data: MSFT_30_2023-09.csv\n",
      "File MSFT_30_2023-09 found.\n",
      "Searching for file MSFT_30_2023-10\n",
      "Loading file data: MSFT_30_2023-10.csv\n",
      "File MSFT_30_2023-10 found.\n",
      "Searching for file MSFT_30_2023-11\n",
      "Loading file data: MSFT_30_2023-11.csv\n",
      "File MSFT_30_2023-11 found.\n",
      "Searching for file MSFT_30_2023-12\n",
      "Loading file data: MSFT_30_2023-12.csv\n",
      "File MSFT_30_2023-12 found.\n",
      "Searching for file MSFT_30_2024-01\n",
      "Loading file data: MSFT_30_2024-01.csv\n",
      "File MSFT_30_2024-01 found.\n",
      "Searching for file MSFT_30_2024-02\n",
      "Loading file data: MSFT_30_2024-02.csv\n",
      "File MSFT_30_2024-02 found.\n",
      "Searching for file MSFT_30_2024-03\n",
      "Loading file data: MSFT_30_2024-03.csv\n",
      "File MSFT_30_2024-03 found.\n",
      "Searching for file MSFT_30_2024-04\n",
      "Loading file data: MSFT_30_2024-04.csv\n",
      "File MSFT_30_2024-04 found.\n",
      "Searching for file MSFT_30_2024-05\n",
      "Loading file data: MSFT_30_2024-05.csv\n",
      "File MSFT_30_2024-05 found.\n",
      "Searching for file MSFT_30_2024-06\n",
      "Loading file data: MSFT_30_2024-06.csv\n",
      "File MSFT_30_2024-06 found.\n",
      "Searching for file MSFT_30_2024-07\n",
      "Loading file data: MSFT_30_2024-07.csv\n",
      "File MSFT_30_2024-07 found.\n",
      "Searching for file MSFT_30_2024-08\n",
      "Loading file data: MSFT_30_2024-08.csv\n",
      "File MSFT_30_2024-08 found.\n",
      "Searching for file MSFT_30_2024-09\n",
      "Loading file data: MSFT_30_2024-09.csv\n",
      "File MSFT_30_2024-09 found.\n",
      "Searching for file MSFT_30_2024-10\n",
      "Loading file data: MSFT_30_2024-10.csv\n",
      "File MSFT_30_2024-10 found.\n",
      "Searching for file MSFT_30_2024-11\n",
      "Loading file data: MSFT_30_2024-11.csv\n",
      "File MSFT_30_2024-11 found.\n",
      "Searching for file MSFT_30_2024-12\n",
      "Loading file data: MSFT_30_2024-12.csv\n",
      "File MSFT_30_2024-12 found.\n",
      "Searching for file MSFT_30_2025-01\n",
      "Loading file data: MSFT_30_2025-01.csv\n",
      "File MSFT_30_2025-01 found.\n",
      "Searching for file MSFT_60_2022-01\n",
      "Loading file data: MSFT_60_2022-01.csv\n",
      "File MSFT_60_2022-01 found.\n",
      "Searching for file MSFT_60_2022-02\n",
      "Loading file data: MSFT_60_2022-02.csv\n",
      "File MSFT_60_2022-02 found.\n",
      "Searching for file MSFT_60_2022-03\n",
      "Loading file data: MSFT_60_2022-03.csv\n",
      "File MSFT_60_2022-03 found.\n",
      "Searching for file MSFT_60_2022-04\n",
      "Loading file data: MSFT_60_2022-04.csv\n",
      "File MSFT_60_2022-04 found.\n",
      "Searching for file MSFT_60_2022-05\n",
      "Loading file data: MSFT_60_2022-05.csv\n",
      "File MSFT_60_2022-05 found.\n",
      "Searching for file MSFT_60_2022-06\n",
      "Loading file data: MSFT_60_2022-06.csv\n",
      "File MSFT_60_2022-06 found.\n",
      "Searching for file MSFT_60_2022-07\n",
      "Loading file data: MSFT_60_2022-07.csv\n",
      "File MSFT_60_2022-07 found.\n",
      "Searching for file MSFT_60_2022-08\n",
      "Loading file data: MSFT_60_2022-08.csv\n",
      "File MSFT_60_2022-08 found.\n",
      "Searching for file MSFT_60_2022-09\n",
      "Loading file data: MSFT_60_2022-09.csv\n",
      "File MSFT_60_2022-09 found.\n",
      "Searching for file MSFT_60_2022-10\n",
      "Loading file data: MSFT_60_2022-10.csv\n",
      "File MSFT_60_2022-10 found.\n",
      "Searching for file MSFT_60_2022-11\n",
      "Loading file data: MSFT_60_2022-11.csv\n",
      "File MSFT_60_2022-11 found.\n",
      "Searching for file MSFT_60_2022-12\n",
      "Loading file data: MSFT_60_2022-12.csv\n",
      "File MSFT_60_2022-12 found.\n",
      "Searching for file MSFT_60_2023-01\n",
      "Loading file data: MSFT_60_2023-01.csv\n",
      "File MSFT_60_2023-01 found.\n",
      "Searching for file MSFT_60_2023-02\n",
      "Loading file data: MSFT_60_2023-02.csv\n",
      "File MSFT_60_2023-02 found.\n",
      "Searching for file MSFT_60_2023-03\n",
      "Loading file data: MSFT_60_2023-03.csv\n",
      "File MSFT_60_2023-03 found.\n",
      "Searching for file MSFT_60_2023-04\n",
      "Loading file data: MSFT_60_2023-04.csv\n",
      "File MSFT_60_2023-04 found.\n",
      "Searching for file MSFT_60_2023-05\n",
      "Loading file data: MSFT_60_2023-05.csv\n",
      "File MSFT_60_2023-05 found.\n",
      "Searching for file MSFT_60_2023-06\n",
      "Loading file data: MSFT_60_2023-06.csv\n",
      "File MSFT_60_2023-06 found.\n",
      "Searching for file MSFT_60_2023-07\n",
      "Loading file data: MSFT_60_2023-07.csv\n",
      "File MSFT_60_2023-07 found.\n",
      "Searching for file MSFT_60_2023-08\n",
      "Loading file data: MSFT_60_2023-08.csv\n",
      "File MSFT_60_2023-08 found.\n",
      "Searching for file MSFT_60_2023-09\n",
      "Loading file data: MSFT_60_2023-09.csv\n",
      "File MSFT_60_2023-09 found.\n",
      "Searching for file MSFT_60_2023-10\n",
      "Loading file data: MSFT_60_2023-10.csv\n",
      "File MSFT_60_2023-10 found.\n",
      "Searching for file MSFT_60_2023-11\n",
      "Loading file data: MSFT_60_2023-11.csv\n",
      "File MSFT_60_2023-11 found.\n",
      "Searching for file MSFT_60_2023-12\n",
      "Loading file data: MSFT_60_2023-12.csv\n",
      "File MSFT_60_2023-12 found.\n",
      "Searching for file MSFT_60_2024-01\n",
      "Loading file data: MSFT_60_2024-01.csv\n",
      "File MSFT_60_2024-01 found.\n",
      "Searching for file MSFT_60_2024-02\n",
      "Loading file data: MSFT_60_2024-02.csv\n",
      "File MSFT_60_2024-02 found.\n",
      "Searching for file MSFT_60_2024-03\n",
      "Loading file data: MSFT_60_2024-03.csv\n",
      "File MSFT_60_2024-03 found.\n",
      "Searching for file MSFT_60_2024-04\n",
      "Loading file data: MSFT_60_2024-04.csv\n",
      "File MSFT_60_2024-04 found.\n",
      "Searching for file MSFT_60_2024-05\n",
      "Loading file data: MSFT_60_2024-05.csv\n",
      "File MSFT_60_2024-05 found.\n",
      "Searching for file MSFT_60_2024-06\n",
      "Loading file data: MSFT_60_2024-06.csv\n",
      "File MSFT_60_2024-06 found.\n",
      "Searching for file MSFT_60_2024-07\n",
      "Loading file data: MSFT_60_2024-07.csv\n",
      "File MSFT_60_2024-07 found.\n",
      "Searching for file MSFT_60_2024-08\n",
      "Loading file data: MSFT_60_2024-08.csv\n",
      "File MSFT_60_2024-08 found.\n",
      "Searching for file MSFT_60_2024-09\n",
      "Loading file data: MSFT_60_2024-09.csv\n",
      "File MSFT_60_2024-09 found.\n",
      "Searching for file MSFT_60_2024-10\n",
      "Loading file data: MSFT_60_2024-10.csv\n",
      "File MSFT_60_2024-10 found.\n",
      "Searching for file MSFT_60_2024-11\n",
      "Loading file data: MSFT_60_2024-11.csv\n",
      "File MSFT_60_2024-11 found.\n",
      "Searching for file MSFT_60_2024-12\n",
      "Loading file data: MSFT_60_2024-12.csv\n",
      "File MSFT_60_2024-12 found.\n",
      "Searching for file MSFT_60_2025-01\n",
      "Loading file data: MSFT_60_2025-01.csv\n",
      "File MSFT_60_2025-01 found.\n",
      "Searching for file NVDA_1_2022-01\n",
      "Loading file data: NVDA_1_2022-01.csv\n",
      "File NVDA_1_2022-01 found.\n",
      "Searching for file NVDA_1_2022-02\n",
      "Loading file data: NVDA_1_2022-02.csv\n",
      "File NVDA_1_2022-02 found.\n",
      "Searching for file NVDA_1_2022-03\n",
      "Loading file data: NVDA_1_2022-03.csv\n",
      "File NVDA_1_2022-03 found.\n",
      "Searching for file NVDA_1_2022-04\n",
      "Loading file data: NVDA_1_2022-04.csv\n",
      "File NVDA_1_2022-04 found.\n",
      "Searching for file NVDA_1_2022-05\n",
      "Loading file data: NVDA_1_2022-05.csv\n",
      "File NVDA_1_2022-05 found.\n",
      "Searching for file NVDA_1_2022-06\n",
      "Loading file data: NVDA_1_2022-06.csv\n",
      "File NVDA_1_2022-06 found.\n",
      "Searching for file NVDA_1_2022-07\n",
      "Loading file data: NVDA_1_2022-07.csv\n",
      "File NVDA_1_2022-07 found.\n",
      "Searching for file NVDA_1_2022-08\n",
      "Loading file data: NVDA_1_2022-08.csv\n",
      "File NVDA_1_2022-08 found.\n",
      "Searching for file NVDA_1_2022-09\n",
      "Loading file data: NVDA_1_2022-09.csv\n",
      "File NVDA_1_2022-09 found.\n",
      "Searching for file NVDA_1_2022-10\n",
      "Loading file data: NVDA_1_2022-10.csv\n",
      "File NVDA_1_2022-10 found.\n",
      "Searching for file NVDA_1_2022-11\n",
      "Loading file data: NVDA_1_2022-11.csv\n",
      "File NVDA_1_2022-11 found.\n",
      "Searching for file NVDA_1_2022-12\n",
      "Loading file data: NVDA_1_2022-12.csv\n",
      "File NVDA_1_2022-12 found.\n",
      "Searching for file NVDA_1_2023-01\n",
      "Loading file data: NVDA_1_2023-01.csv\n",
      "File NVDA_1_2023-01 found.\n",
      "Searching for file NVDA_1_2023-02\n",
      "Loading file data: NVDA_1_2023-02.csv\n",
      "File NVDA_1_2023-02 found.\n",
      "Searching for file NVDA_1_2023-03\n",
      "Loading file data: NVDA_1_2023-03.csv\n",
      "File NVDA_1_2023-03 found.\n",
      "Searching for file NVDA_1_2023-04\n",
      "Loading file data: NVDA_1_2023-04.csv\n",
      "File NVDA_1_2023-04 found.\n",
      "Searching for file NVDA_1_2023-05\n",
      "Loading file data: NVDA_1_2023-05.csv\n",
      "File NVDA_1_2023-05 found.\n",
      "Searching for file NVDA_1_2023-06\n",
      "Loading file data: NVDA_1_2023-06.csv\n",
      "File NVDA_1_2023-06 found.\n",
      "Searching for file NVDA_1_2023-07\n",
      "Loading file data: NVDA_1_2023-07.csv\n",
      "File NVDA_1_2023-07 found.\n",
      "Searching for file NVDA_1_2023-08\n",
      "Loading file data: NVDA_1_2023-08.csv\n",
      "File NVDA_1_2023-08 found.\n",
      "Searching for file NVDA_1_2023-09\n",
      "Loading file data: NVDA_1_2023-09.csv\n",
      "File NVDA_1_2023-09 found.\n",
      "Searching for file NVDA_1_2023-10\n",
      "Loading file data: NVDA_1_2023-10.csv\n",
      "File NVDA_1_2023-10 found.\n",
      "Searching for file NVDA_1_2023-11\n",
      "Loading file data: NVDA_1_2023-11.csv\n",
      "File NVDA_1_2023-11 found.\n",
      "Searching for file NVDA_1_2023-12\n",
      "Loading file data: NVDA_1_2023-12.csv\n",
      "File NVDA_1_2023-12 found.\n",
      "Searching for file NVDA_1_2024-01\n",
      "Loading file data: NVDA_1_2024-01.csv\n",
      "File NVDA_1_2024-01 found.\n",
      "Searching for file NVDA_1_2024-02\n",
      "Loading file data: NVDA_1_2024-02.csv\n",
      "File NVDA_1_2024-02 found.\n",
      "Searching for file NVDA_1_2024-03\n",
      "Loading file data: NVDA_1_2024-03.csv\n",
      "File NVDA_1_2024-03 found.\n",
      "Searching for file NVDA_1_2024-04\n",
      "Loading file data: NVDA_1_2024-04.csv\n",
      "File NVDA_1_2024-04 found.\n",
      "Searching for file NVDA_1_2024-05\n",
      "Loading file data: NVDA_1_2024-05.csv\n",
      "File NVDA_1_2024-05 found.\n",
      "Searching for file NVDA_1_2024-06\n",
      "Loading file data: NVDA_1_2024-06.csv\n",
      "File NVDA_1_2024-06 found.\n",
      "Searching for file NVDA_1_2024-07\n",
      "Loading file data: NVDA_1_2024-07.csv\n",
      "File NVDA_1_2024-07 found.\n",
      "Searching for file NVDA_1_2024-08\n",
      "Loading file data: NVDA_1_2024-08.csv\n",
      "File NVDA_1_2024-08 found.\n",
      "Searching for file NVDA_1_2024-09\n",
      "Loading file data: NVDA_1_2024-09.csv\n",
      "File NVDA_1_2024-09 found.\n",
      "Searching for file NVDA_1_2024-10\n",
      "Loading file data: NVDA_1_2024-10.csv\n",
      "File NVDA_1_2024-10 found.\n",
      "Searching for file NVDA_1_2024-11\n",
      "Loading file data: NVDA_1_2024-11.csv\n",
      "File NVDA_1_2024-11 found.\n",
      "Searching for file NVDA_1_2024-12\n",
      "Loading file data: NVDA_1_2024-12.csv\n",
      "File NVDA_1_2024-12 found.\n",
      "Searching for file NVDA_1_2025-01\n",
      "Loading file data: NVDA_1_2025-01.csv\n",
      "File NVDA_1_2025-01 found.\n",
      "Searching for file NVDA_5_2022-01\n",
      "Loading file data: NVDA_5_2022-01.csv\n",
      "File NVDA_5_2022-01 found.\n",
      "Searching for file NVDA_5_2022-02\n",
      "Loading file data: NVDA_5_2022-02.csv\n",
      "File NVDA_5_2022-02 found.\n",
      "Searching for file NVDA_5_2022-03\n",
      "Loading file data: NVDA_5_2022-03.csv\n",
      "File NVDA_5_2022-03 found.\n",
      "Searching for file NVDA_5_2022-04\n",
      "Loading file data: NVDA_5_2022-04.csv\n",
      "File NVDA_5_2022-04 found.\n",
      "Searching for file NVDA_5_2022-05\n",
      "Loading file data: NVDA_5_2022-05.csv\n",
      "File NVDA_5_2022-05 found.\n",
      "Searching for file NVDA_5_2022-06\n",
      "Loading file data: NVDA_5_2022-06.csv\n",
      "File NVDA_5_2022-06 found.\n",
      "Searching for file NVDA_5_2022-07\n",
      "Loading file data: NVDA_5_2022-07.csv\n",
      "File NVDA_5_2022-07 found.\n",
      "Searching for file NVDA_5_2022-08\n",
      "Loading file data: NVDA_5_2022-08.csv\n",
      "File NVDA_5_2022-08 found.\n",
      "Searching for file NVDA_5_2022-09\n",
      "Loading file data: NVDA_5_2022-09.csv\n",
      "File NVDA_5_2022-09 found.\n",
      "Searching for file NVDA_5_2022-10\n",
      "Loading file data: NVDA_5_2022-10.csv\n",
      "File NVDA_5_2022-10 found.\n",
      "Searching for file NVDA_5_2022-11\n",
      "Loading file data: NVDA_5_2022-11.csv\n",
      "File NVDA_5_2022-11 found.\n",
      "Searching for file NVDA_5_2022-12\n",
      "Loading file data: NVDA_5_2022-12.csv\n",
      "File NVDA_5_2022-12 found.\n",
      "Searching for file NVDA_5_2023-01\n",
      "Loading file data: NVDA_5_2023-01.csv\n",
      "File NVDA_5_2023-01 found.\n",
      "Searching for file NVDA_5_2023-02\n",
      "Loading file data: NVDA_5_2023-02.csv\n",
      "File NVDA_5_2023-02 found.\n",
      "Searching for file NVDA_5_2023-03\n",
      "Loading file data: NVDA_5_2023-03.csv\n",
      "File NVDA_5_2023-03 found.\n",
      "Searching for file NVDA_5_2023-04\n",
      "Loading file data: NVDA_5_2023-04.csv\n",
      "File NVDA_5_2023-04 found.\n",
      "Searching for file NVDA_5_2023-05\n",
      "Loading file data: NVDA_5_2023-05.csv\n",
      "File NVDA_5_2023-05 found.\n",
      "Searching for file NVDA_5_2023-06\n",
      "Loading file data: NVDA_5_2023-06.csv\n",
      "File NVDA_5_2023-06 found.\n",
      "Searching for file NVDA_5_2023-07\n",
      "Loading file data: NVDA_5_2023-07.csv\n",
      "File NVDA_5_2023-07 found.\n",
      "Searching for file NVDA_5_2023-08\n",
      "Loading file data: NVDA_5_2023-08.csv\n",
      "File NVDA_5_2023-08 found.\n",
      "Searching for file NVDA_5_2023-09\n",
      "Loading file data: NVDA_5_2023-09.csv\n",
      "File NVDA_5_2023-09 found.\n",
      "Searching for file NVDA_5_2023-10\n",
      "Loading file data: NVDA_5_2023-10.csv\n",
      "File NVDA_5_2023-10 found.\n",
      "Searching for file NVDA_5_2023-11\n",
      "Loading file data: NVDA_5_2023-11.csv\n",
      "File NVDA_5_2023-11 found.\n",
      "Searching for file NVDA_5_2023-12\n",
      "Loading file data: NVDA_5_2023-12.csv\n",
      "File NVDA_5_2023-12 found.\n",
      "Searching for file NVDA_5_2024-01\n",
      "Loading file data: NVDA_5_2024-01.csv\n",
      "File NVDA_5_2024-01 found.\n",
      "Searching for file NVDA_5_2024-02\n",
      "Loading file data: NVDA_5_2024-02.csv\n",
      "File NVDA_5_2024-02 found.\n",
      "Searching for file NVDA_5_2024-03\n",
      "Loading file data: NVDA_5_2024-03.csv\n",
      "File NVDA_5_2024-03 found.\n",
      "Searching for file NVDA_5_2024-04\n",
      "Loading file data: NVDA_5_2024-04.csv\n",
      "File NVDA_5_2024-04 found.\n",
      "Searching for file NVDA_5_2024-05\n",
      "Loading file data: NVDA_5_2024-05.csv\n",
      "File NVDA_5_2024-05 found.\n",
      "Searching for file NVDA_5_2024-06\n",
      "Loading file data: NVDA_5_2024-06.csv\n",
      "File NVDA_5_2024-06 found.\n",
      "Searching for file NVDA_5_2024-07\n",
      "Loading file data: NVDA_5_2024-07.csv\n",
      "File NVDA_5_2024-07 found.\n",
      "Searching for file NVDA_5_2024-08\n",
      "Loading file data: NVDA_5_2024-08.csv\n",
      "File NVDA_5_2024-08 found.\n",
      "Searching for file NVDA_5_2024-09\n",
      "Loading file data: NVDA_5_2024-09.csv\n",
      "File NVDA_5_2024-09 found.\n",
      "Searching for file NVDA_5_2024-10\n",
      "Loading file data: NVDA_5_2024-10.csv\n",
      "File NVDA_5_2024-10 found.\n",
      "Searching for file NVDA_5_2024-11\n",
      "Loading file data: NVDA_5_2024-11.csv\n",
      "File NVDA_5_2024-11 found.\n",
      "Searching for file NVDA_5_2024-12\n",
      "Loading file data: NVDA_5_2024-12.csv\n",
      "File NVDA_5_2024-12 found.\n",
      "Searching for file NVDA_5_2025-01\n",
      "Loading file data: NVDA_5_2025-01.csv\n",
      "File NVDA_5_2025-01 found.\n",
      "Searching for file NVDA_15_2022-01\n",
      "Loading file data: NVDA_15_2022-01.csv\n",
      "File NVDA_15_2022-01 found.\n",
      "Searching for file NVDA_15_2022-02\n",
      "Loading file data: NVDA_15_2022-02.csv\n",
      "File NVDA_15_2022-02 found.\n",
      "Searching for file NVDA_15_2022-03\n",
      "Loading file data: NVDA_15_2022-03.csv\n",
      "File NVDA_15_2022-03 found.\n",
      "Searching for file NVDA_15_2022-04\n",
      "Loading file data: NVDA_15_2022-04.csv\n",
      "File NVDA_15_2022-04 found.\n",
      "Searching for file NVDA_15_2022-05\n",
      "Loading file data: NVDA_15_2022-05.csv\n",
      "File NVDA_15_2022-05 found.\n",
      "Searching for file NVDA_15_2022-06\n",
      "Loading file data: NVDA_15_2022-06.csv\n",
      "File NVDA_15_2022-06 found.\n",
      "Searching for file NVDA_15_2022-07\n",
      "Loading file data: NVDA_15_2022-07.csv\n",
      "File NVDA_15_2022-07 found.\n",
      "Searching for file NVDA_15_2022-08\n",
      "Loading file data: NVDA_15_2022-08.csv\n",
      "File NVDA_15_2022-08 found.\n",
      "Searching for file NVDA_15_2022-09\n",
      "Loading file data: NVDA_15_2022-09.csv\n",
      "File NVDA_15_2022-09 found.\n",
      "Searching for file NVDA_15_2022-10\n",
      "Loading file data: NVDA_15_2022-10.csv\n",
      "File NVDA_15_2022-10 found.\n",
      "Searching for file NVDA_15_2022-11\n",
      "Loading file data: NVDA_15_2022-11.csv\n",
      "File NVDA_15_2022-11 found.\n",
      "Searching for file NVDA_15_2022-12\n",
      "Loading file data: NVDA_15_2022-12.csv\n",
      "File NVDA_15_2022-12 found.\n",
      "Searching for file NVDA_15_2023-01\n",
      "Loading file data: NVDA_15_2023-01.csv\n",
      "File NVDA_15_2023-01 found.\n",
      "Searching for file NVDA_15_2023-02\n",
      "Loading file data: NVDA_15_2023-02.csv\n",
      "File NVDA_15_2023-02 found.\n",
      "Searching for file NVDA_15_2023-03\n",
      "Loading file data: NVDA_15_2023-03.csv\n",
      "File NVDA_15_2023-03 found.\n",
      "Searching for file NVDA_15_2023-04\n",
      "Loading file data: NVDA_15_2023-04.csv\n",
      "File NVDA_15_2023-04 found.\n",
      "Searching for file NVDA_15_2023-05\n",
      "Loading file data: NVDA_15_2023-05.csv\n",
      "File NVDA_15_2023-05 found.\n",
      "Searching for file NVDA_15_2023-06\n",
      "Loading file data: NVDA_15_2023-06.csv\n",
      "File NVDA_15_2023-06 found.\n",
      "Searching for file NVDA_15_2023-07\n",
      "Loading file data: NVDA_15_2023-07.csv\n",
      "File NVDA_15_2023-07 found.\n",
      "Searching for file NVDA_15_2023-08\n",
      "Loading file data: NVDA_15_2023-08.csv\n",
      "File NVDA_15_2023-08 found.\n",
      "Searching for file NVDA_15_2023-09\n",
      "Loading file data: NVDA_15_2023-09.csv\n",
      "File NVDA_15_2023-09 found.\n",
      "Searching for file NVDA_15_2023-10\n",
      "Loading file data: NVDA_15_2023-10.csv\n",
      "File NVDA_15_2023-10 found.\n",
      "Searching for file NVDA_15_2023-11\n",
      "Loading file data: NVDA_15_2023-11.csv\n",
      "File NVDA_15_2023-11 found.\n",
      "Searching for file NVDA_15_2023-12\n",
      "Loading file data: NVDA_15_2023-12.csv\n",
      "File NVDA_15_2023-12 found.\n",
      "Searching for file NVDA_15_2024-01\n",
      "Loading file data: NVDA_15_2024-01.csv\n",
      "File NVDA_15_2024-01 found.\n",
      "Searching for file NVDA_15_2024-02\n",
      "Loading file data: NVDA_15_2024-02.csv\n",
      "File NVDA_15_2024-02 found.\n",
      "Searching for file NVDA_15_2024-03\n",
      "Loading file data: NVDA_15_2024-03.csv\n",
      "File NVDA_15_2024-03 found.\n",
      "Searching for file NVDA_15_2024-04\n",
      "Loading file data: NVDA_15_2024-04.csv\n",
      "File NVDA_15_2024-04 found.\n",
      "Searching for file NVDA_15_2024-05\n",
      "Loading file data: NVDA_15_2024-05.csv\n",
      "File NVDA_15_2024-05 found.\n",
      "Searching for file NVDA_15_2024-06\n",
      "Loading file data: NVDA_15_2024-06.csv\n",
      "File NVDA_15_2024-06 found.\n",
      "Searching for file NVDA_15_2024-07\n",
      "Loading file data: NVDA_15_2024-07.csv\n",
      "File NVDA_15_2024-07 found.\n",
      "Searching for file NVDA_15_2024-08\n",
      "Loading file data: NVDA_15_2024-08.csv\n",
      "File NVDA_15_2024-08 found.\n",
      "Searching for file NVDA_15_2024-09\n",
      "Loading file data: NVDA_15_2024-09.csv\n",
      "File NVDA_15_2024-09 found.\n",
      "Searching for file NVDA_15_2024-10\n",
      "Loading file data: NVDA_15_2024-10.csv\n",
      "File NVDA_15_2024-10 found.\n",
      "Searching for file NVDA_15_2024-11\n",
      "Loading file data: NVDA_15_2024-11.csv\n",
      "File NVDA_15_2024-11 found.\n",
      "Searching for file NVDA_15_2024-12\n",
      "Loading file data: NVDA_15_2024-12.csv\n",
      "File NVDA_15_2024-12 found.\n",
      "Searching for file NVDA_15_2025-01\n",
      "Loading file data: NVDA_15_2025-01.csv\n",
      "File NVDA_15_2025-01 found.\n",
      "Searching for file NVDA_30_2022-01\n",
      "Loading file data: NVDA_30_2022-01.csv\n",
      "File NVDA_30_2022-01 found.\n",
      "Searching for file NVDA_30_2022-02\n",
      "Loading file data: NVDA_30_2022-02.csv\n",
      "File NVDA_30_2022-02 found.\n",
      "Searching for file NVDA_30_2022-03\n",
      "Loading file data: NVDA_30_2022-03.csv\n",
      "File NVDA_30_2022-03 found.\n",
      "Searching for file NVDA_30_2022-04\n",
      "Loading file data: NVDA_30_2022-04.csv\n",
      "File NVDA_30_2022-04 found.\n",
      "Searching for file NVDA_30_2022-05\n",
      "Loading file data: NVDA_30_2022-05.csv\n",
      "File NVDA_30_2022-05 found.\n",
      "Searching for file NVDA_30_2022-06\n",
      "Loading file data: NVDA_30_2022-06.csv\n",
      "File NVDA_30_2022-06 found.\n",
      "Searching for file NVDA_30_2022-07\n",
      "Loading file data: NVDA_30_2022-07.csv\n",
      "File NVDA_30_2022-07 found.\n",
      "Searching for file NVDA_30_2022-08\n",
      "Loading file data: NVDA_30_2022-08.csv\n",
      "File NVDA_30_2022-08 found.\n",
      "Searching for file NVDA_30_2022-09\n",
      "Loading file data: NVDA_30_2022-09.csv\n",
      "File NVDA_30_2022-09 found.\n",
      "Searching for file NVDA_30_2022-10\n",
      "Loading file data: NVDA_30_2022-10.csv\n",
      "File NVDA_30_2022-10 found.\n",
      "Searching for file NVDA_30_2022-11\n",
      "Loading file data: NVDA_30_2022-11.csv\n",
      "File NVDA_30_2022-11 found.\n",
      "Searching for file NVDA_30_2022-12\n",
      "Loading file data: NVDA_30_2022-12.csv\n",
      "File NVDA_30_2022-12 found.\n",
      "Searching for file NVDA_30_2023-01\n",
      "Loading file data: NVDA_30_2023-01.csv\n",
      "File NVDA_30_2023-01 found.\n",
      "Searching for file NVDA_30_2023-02\n",
      "Loading file data: NVDA_30_2023-02.csv\n",
      "File NVDA_30_2023-02 found.\n",
      "Searching for file NVDA_30_2023-03\n",
      "Loading file data: NVDA_30_2023-03.csv\n",
      "File NVDA_30_2023-03 found.\n",
      "Searching for file NVDA_30_2023-04\n",
      "Loading file data: NVDA_30_2023-04.csv\n",
      "File NVDA_30_2023-04 found.\n",
      "Searching for file NVDA_30_2023-05\n",
      "Loading file data: NVDA_30_2023-05.csv\n",
      "File NVDA_30_2023-05 found.\n",
      "Searching for file NVDA_30_2023-06\n",
      "Loading file data: NVDA_30_2023-06.csv\n",
      "File NVDA_30_2023-06 found.\n",
      "Searching for file NVDA_30_2023-07\n",
      "Loading file data: NVDA_30_2023-07.csv\n",
      "File NVDA_30_2023-07 found.\n",
      "Searching for file NVDA_30_2023-08\n",
      "Loading file data: NVDA_30_2023-08.csv\n",
      "File NVDA_30_2023-08 found.\n",
      "Searching for file NVDA_30_2023-09\n",
      "Loading file data: NVDA_30_2023-09.csv\n",
      "File NVDA_30_2023-09 found.\n",
      "Searching for file NVDA_30_2023-10\n",
      "Loading file data: NVDA_30_2023-10.csv\n",
      "File NVDA_30_2023-10 found.\n",
      "Searching for file NVDA_30_2023-11\n",
      "Loading file data: NVDA_30_2023-11.csv\n",
      "File NVDA_30_2023-11 found.\n",
      "Searching for file NVDA_30_2023-12\n",
      "Loading file data: NVDA_30_2023-12.csv\n",
      "File NVDA_30_2023-12 found.\n",
      "Searching for file NVDA_30_2024-01\n",
      "Loading file data: NVDA_30_2024-01.csv\n",
      "File NVDA_30_2024-01 found.\n",
      "Searching for file NVDA_30_2024-02\n",
      "Loading file data: NVDA_30_2024-02.csv\n",
      "File NVDA_30_2024-02 found.\n",
      "Searching for file NVDA_30_2024-03\n",
      "Loading file data: NVDA_30_2024-03.csv\n",
      "File NVDA_30_2024-03 found.\n",
      "Searching for file NVDA_30_2024-04\n",
      "Loading file data: NVDA_30_2024-04.csv\n",
      "File NVDA_30_2024-04 found.\n",
      "Searching for file NVDA_30_2024-05\n",
      "Loading file data: NVDA_30_2024-05.csv\n",
      "File NVDA_30_2024-05 found.\n",
      "Searching for file NVDA_30_2024-06\n",
      "Loading file data: NVDA_30_2024-06.csv\n",
      "File NVDA_30_2024-06 found.\n",
      "Searching for file NVDA_30_2024-07\n",
      "Loading file data: NVDA_30_2024-07.csv\n",
      "File NVDA_30_2024-07 found.\n",
      "Searching for file NVDA_30_2024-08\n",
      "Loading file data: NVDA_30_2024-08.csv\n",
      "File NVDA_30_2024-08 found.\n",
      "Searching for file NVDA_30_2024-09\n",
      "Loading file data: NVDA_30_2024-09.csv\n",
      "File NVDA_30_2024-09 found.\n",
      "Searching for file NVDA_30_2024-10\n",
      "Loading file data: NVDA_30_2024-10.csv\n",
      "File NVDA_30_2024-10 found.\n",
      "Searching for file NVDA_30_2024-11\n",
      "Loading file data: NVDA_30_2024-11.csv\n",
      "File NVDA_30_2024-11 found.\n",
      "Searching for file NVDA_30_2024-12\n",
      "Loading file data: NVDA_30_2024-12.csv\n",
      "File NVDA_30_2024-12 found.\n",
      "Searching for file NVDA_30_2025-01\n",
      "Loading file data: NVDA_30_2025-01.csv\n",
      "File NVDA_30_2025-01 found.\n",
      "Searching for file NVDA_60_2022-01\n",
      "Loading file data: NVDA_60_2022-01.csv\n",
      "File NVDA_60_2022-01 found.\n",
      "Searching for file NVDA_60_2022-02\n",
      "Loading file data: NVDA_60_2022-02.csv\n",
      "File NVDA_60_2022-02 found.\n",
      "Searching for file NVDA_60_2022-03\n",
      "Loading file data: NVDA_60_2022-03.csv\n",
      "File NVDA_60_2022-03 found.\n",
      "Searching for file NVDA_60_2022-04\n",
      "Loading file data: NVDA_60_2022-04.csv\n",
      "File NVDA_60_2022-04 found.\n",
      "Searching for file NVDA_60_2022-05\n",
      "Loading file data: NVDA_60_2022-05.csv\n",
      "File NVDA_60_2022-05 found.\n",
      "Searching for file NVDA_60_2022-06\n",
      "Loading file data: NVDA_60_2022-06.csv\n",
      "File NVDA_60_2022-06 found.\n",
      "Searching for file NVDA_60_2022-07\n",
      "Loading file data: NVDA_60_2022-07.csv\n",
      "File NVDA_60_2022-07 found.\n",
      "Searching for file NVDA_60_2022-08\n",
      "Loading file data: NVDA_60_2022-08.csv\n",
      "File NVDA_60_2022-08 found.\n",
      "Searching for file NVDA_60_2022-09\n",
      "Loading file data: NVDA_60_2022-09.csv\n",
      "File NVDA_60_2022-09 found.\n",
      "Searching for file NVDA_60_2022-10\n",
      "Loading file data: NVDA_60_2022-10.csv\n",
      "File NVDA_60_2022-10 found.\n",
      "Searching for file NVDA_60_2022-11\n",
      "Loading file data: NVDA_60_2022-11.csv\n",
      "File NVDA_60_2022-11 found.\n",
      "Searching for file NVDA_60_2022-12\n",
      "Loading file data: NVDA_60_2022-12.csv\n",
      "File NVDA_60_2022-12 found.\n",
      "Searching for file NVDA_60_2023-01\n",
      "Loading file data: NVDA_60_2023-01.csv\n",
      "File NVDA_60_2023-01 found.\n",
      "Searching for file NVDA_60_2023-02\n",
      "Loading file data: NVDA_60_2023-02.csv\n",
      "File NVDA_60_2023-02 found.\n",
      "Searching for file NVDA_60_2023-03\n",
      "Loading file data: NVDA_60_2023-03.csv\n",
      "File NVDA_60_2023-03 found.\n",
      "Searching for file NVDA_60_2023-04\n",
      "Loading file data: NVDA_60_2023-04.csv\n",
      "File NVDA_60_2023-04 found.\n",
      "Searching for file NVDA_60_2023-05\n",
      "Loading file data: NVDA_60_2023-05.csv\n",
      "File NVDA_60_2023-05 found.\n",
      "Searching for file NVDA_60_2023-06\n",
      "Loading file data: NVDA_60_2023-06.csv\n",
      "File NVDA_60_2023-06 found.\n",
      "Searching for file NVDA_60_2023-07\n",
      "Loading file data: NVDA_60_2023-07.csv\n",
      "File NVDA_60_2023-07 found.\n",
      "Searching for file NVDA_60_2023-08\n",
      "Loading file data: NVDA_60_2023-08.csv\n",
      "File NVDA_60_2023-08 found.\n",
      "Searching for file NVDA_60_2023-09\n",
      "Loading file data: NVDA_60_2023-09.csv\n",
      "File NVDA_60_2023-09 found.\n",
      "Searching for file NVDA_60_2023-10\n",
      "Loading file data: NVDA_60_2023-10.csv\n",
      "File NVDA_60_2023-10 found.\n",
      "Searching for file NVDA_60_2023-11\n",
      "Loading file data: NVDA_60_2023-11.csv\n",
      "File NVDA_60_2023-11 found.\n",
      "Searching for file NVDA_60_2023-12\n",
      "Loading file data: NVDA_60_2023-12.csv\n",
      "File NVDA_60_2023-12 found.\n",
      "Searching for file NVDA_60_2024-01\n",
      "Loading file data: NVDA_60_2024-01.csv\n",
      "File NVDA_60_2024-01 found.\n",
      "Searching for file NVDA_60_2024-02\n",
      "Loading file data: NVDA_60_2024-02.csv\n",
      "File NVDA_60_2024-02 found.\n",
      "Searching for file NVDA_60_2024-03\n",
      "Loading file data: NVDA_60_2024-03.csv\n",
      "File NVDA_60_2024-03 found.\n",
      "Searching for file NVDA_60_2024-04\n",
      "Loading file data: NVDA_60_2024-04.csv\n",
      "File NVDA_60_2024-04 found.\n",
      "Searching for file NVDA_60_2024-05\n",
      "Loading file data: NVDA_60_2024-05.csv\n",
      "File NVDA_60_2024-05 found.\n",
      "Searching for file NVDA_60_2024-06\n",
      "Loading file data: NVDA_60_2024-06.csv\n",
      "File NVDA_60_2024-06 found.\n",
      "Searching for file NVDA_60_2024-07\n",
      "Loading file data: NVDA_60_2024-07.csv\n",
      "File NVDA_60_2024-07 found.\n",
      "Searching for file NVDA_60_2024-08\n",
      "Loading file data: NVDA_60_2024-08.csv\n",
      "File NVDA_60_2024-08 found.\n",
      "Searching for file NVDA_60_2024-09\n",
      "Loading file data: NVDA_60_2024-09.csv\n",
      "File NVDA_60_2024-09 found.\n",
      "Searching for file NVDA_60_2024-10\n",
      "Loading file data: NVDA_60_2024-10.csv\n",
      "File NVDA_60_2024-10 found.\n",
      "Searching for file NVDA_60_2024-11\n",
      "Loading file data: NVDA_60_2024-11.csv\n",
      "File NVDA_60_2024-11 found.\n",
      "Searching for file NVDA_60_2024-12\n",
      "Loading file data: NVDA_60_2024-12.csv\n",
      "File NVDA_60_2024-12 found.\n",
      "Searching for file NVDA_60_2025-01\n",
      "Loading file data: NVDA_60_2025-01.csv\n",
      "File NVDA_60_2025-01 found.\n",
      "Number of invalid/tested datapoints in manifest: 0/735\n",
      "Saving Manifest Data\n",
      "Save path/name: data/StockHistData/dataManifest.json\n",
      "JSON saved successfully to data/StockHistData/dataManifest.json\n",
      "No SQL connection, attempting to create connection engine...\n",
      "Connecting to engine: Engine(postgresql+psycopg2://postgres:***@localhost:5432/marketdata)\n",
      "Connection engine created\n",
      "Loading file data: KO_5_2024-04.csv\n",
      "Upserting table into SQL...\n",
      "manifest/DM\n",
      "Loading Manifest Data\n",
      "Load path/name: data/StockHistData/dataManifest.json\n",
      "Month            2022-01  2022-02  2022-03  2022-04  2022-05  2022-06  \\\n",
      "Ticker Interval                                                         \n",
      "ARM    1               0        0        0        0        0        0   \n",
      "       5               0        0        0        0        0        0   \n",
      "       15              0        0        0        0        0        0   \n",
      "       30              0        0        0        0        0        0   \n",
      "       60              0        0        0        0        0        0   \n",
      "GOOG   1               1        1        1        1        1        1   \n",
      "       5               1        1        1        1        1        1   \n",
      "       15              1        1        1        1        1        1   \n",
      "       30              1        1        1        1        1        1   \n",
      "       60              1        1        1        1        1        1   \n",
      "KO     1               1        1        1        1        1        1   \n",
      "       5               1        1        1        1        1        1   \n",
      "       15              1        1        1        1        1        1   \n",
      "       30              1        1        1        1        1        1   \n",
      "       60              1        1        1        1        1        1   \n",
      "MSFT   1               1        1        1        1        1        1   \n",
      "       5               1        1        1        1        1        1   \n",
      "       15              1        1        1        1        1        1   \n",
      "       30              1        1        1        1        1        1   \n",
      "       60              1        1        1        1        1        1   \n",
      "NVDA   1               1        1        1        1        1        1   \n",
      "       5               1        1        1        1        1        1   \n",
      "       15              1        1        1        1        1        1   \n",
      "       30              1        1        1        1        1        1   \n",
      "       60              1        1        1        1        1        1   \n",
      "\n",
      "Month            2022-07  2022-08  2022-09  2022-10  ...  2024-04  2024-05  \\\n",
      "Ticker Interval                                      ...                     \n",
      "ARM    1               0        0        0        0  ...        1        1   \n",
      "       5               0        0        0        0  ...        1        1   \n",
      "       15              0        0        0        0  ...        1        1   \n",
      "       30              0        0        0        0  ...        1        1   \n",
      "       60              0        0        0        0  ...        1        1   \n",
      "GOOG   1               1        1        1        1  ...        0        0   \n",
      "       5               1        1        1        1  ...        0        0   \n",
      "       15              1        1        1        1  ...        0        0   \n",
      "       30              1        1        1        1  ...        0        0   \n",
      "       60              1        1        1        1  ...        0        0   \n",
      "KO     1               1        1        1        1  ...        1        1   \n",
      "       5               1        1        1        1  ...        1        1   \n",
      "       15              1        1        1        1  ...        1        1   \n",
      "       30              1        1        1        1  ...        1        1   \n",
      "       60              1        1        1        1  ...        1        1   \n",
      "MSFT   1               1        1        1        1  ...        1        1   \n",
      "       5               1        1        1        1  ...        1        1   \n",
      "       15              1        1        1        1  ...        1        1   \n",
      "       30              1        1        1        1  ...        1        1   \n",
      "       60              1        1        1        1  ...        1        1   \n",
      "NVDA   1               1        1        1        1  ...        1        1   \n",
      "       5               1        1        1        1  ...        1        1   \n",
      "       15              1        1        1        1  ...        1        1   \n",
      "       30              1        1        1        1  ...        1        1   \n",
      "       60              1        1        1        1  ...        1        1   \n",
      "\n",
      "Month            2024-06  2024-07  2024-08  2024-09  2024-10  2024-11  \\\n",
      "Ticker Interval                                                         \n",
      "ARM    1               1        1        1        1        1        1   \n",
      "       5               1        1        1        1        1        1   \n",
      "       15              1        1        1        1        1        1   \n",
      "       30              1        1        1        1        1        1   \n",
      "       60              1        1        1        1        1        1   \n",
      "GOOG   1               0        0        0        0        0        0   \n",
      "       5               0        0        0        0        0        0   \n",
      "       15              0        0        0        0        0        0   \n",
      "       30              0        0        0        0        0        0   \n",
      "       60              0        0        0        0        0        0   \n",
      "KO     1               1        1        1        1        1        1   \n",
      "       5               1        1        1        1        1        1   \n",
      "       15              1        1        1        1        1        1   \n",
      "       30              1        1        1        1        1        1   \n",
      "       60              1        1        1        1        1        1   \n",
      "MSFT   1               1        1        1        1        1        1   \n",
      "       5               1        1        1        1        1        1   \n",
      "       15              1        1        1        1        1        1   \n",
      "       30              1        1        1        1        1        1   \n",
      "       60              1        1        1        1        1        1   \n",
      "NVDA   1               1        1        1        1        1        1   \n",
      "       5               1        1        1        1        1        1   \n",
      "       15              1        1        1        1        1        1   \n",
      "       30              1        1        1        1        1        1   \n",
      "       60              1        1        1        1        1        1   \n",
      "\n",
      "Month            2024-12  2025-01  \n",
      "Ticker Interval                    \n",
      "ARM    1               1        1  \n",
      "       5               1        1  \n",
      "       15              1        1  \n",
      "       30              1        1  \n",
      "       60              1        1  \n",
      "GOOG   1               0        1  \n",
      "       5               0        1  \n",
      "       15              0        1  \n",
      "       30              0        1  \n",
      "       60              0        1  \n",
      "KO     1               1        1  \n",
      "       5               1        1  \n",
      "       15              1        1  \n",
      "       30              1        1  \n",
      "       60              1        1  \n",
      "MSFT   1               1        1  \n",
      "       5               1        1  \n",
      "       15              1        1  \n",
      "       30              1        1  \n",
      "       60              1        1  \n",
      "NVDA   1               1        1  \n",
      "       5               1        1  \n",
      "       15              1        1  \n",
      "       30              1        1  \n",
      "       60              1        1  \n",
      "\n",
      "[25 rows x 37 columns]\n",
      "Total time elapsed : 0.09171581268310547 seconds\n",
      "manifest/SQL\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [(ARM, 1), (ARM, 5), (ARM, 15), (ARM, 30), (ARM, 60), (GOOG, 1), (GOOG, 5), (GOOG, 15), (GOOG, 30), (GOOG, 60), (KO, 1), (KO, 5), (KO, 15), (KO, 30), (KO, 60), (MSFT, 1), (MSFT, 5), (MSFT, 15), (MSFT, 30), (MSFT, 60), (NVDA, 1), (NVDA, 5), (NVDA, 15), (NVDA, 30), (NVDA, 60)]\n",
      "Total time elapsed : 0.027159452438354492 seconds\n",
      "market/DM\n",
      "Loading Manifest Data\n",
      "Load path/name: data/StockHistData/dataManifest.json\n",
      "        Ticker  Interval            DateTime    Open      High       Low  \\\n",
      "0          ARM         1 2023-09-29 19:59:00   53.58   53.5800   53.5400   \n",
      "1          ARM         1 2023-09-29 19:57:00   53.54   53.5800   53.5400   \n",
      "2          ARM         1 2023-09-29 19:56:00   53.53   53.5400   53.5300   \n",
      "3          ARM         1 2023-09-29 19:55:00   53.54   53.5400   53.5400   \n",
      "4          ARM         1 2023-09-29 19:54:00   53.54   53.5400   53.5300   \n",
      "...        ...       ...                 ...     ...       ...       ...   \n",
      "3203164   NVDA        60 2025-01-02 08:00:00  136.52  136.8264  133.0412   \n",
      "3203165   NVDA        60 2025-01-02 07:00:00  136.66  136.7200  136.2100   \n",
      "3203166   NVDA        60 2025-01-02 06:00:00  135.99  136.8500  135.6500   \n",
      "3203167   NVDA        60 2025-01-02 05:00:00  135.96  136.1100  135.6000   \n",
      "3203168   NVDA        60 2025-01-02 04:00:00  132.95  135.9700  132.9500   \n",
      "\n",
      "          Close   Volume  \n",
      "0         53.54      101  \n",
      "1         53.58      918  \n",
      "2         53.54      102  \n",
      "3         53.54        2  \n",
      "4         53.54      102  \n",
      "...         ...      ...  \n",
      "3203164  136.23  2821878  \n",
      "3203165  136.54   778926  \n",
      "3203166  136.66   430792  \n",
      "3203167  135.99   206505  \n",
      "3203168  135.95   797243  \n",
      "\n",
      "[3203169 rows x 8 columns]\n",
      "Total time elapsed : 34.01755452156067 seconds\n",
      "manifest/SQL\n",
      "       Ticker  Interval            DateTime     Open     High      Low  \\\n",
      "0        GOOG         1 2022-09-30 17:23:00  96.0144  96.0144  96.0144   \n",
      "1        GOOG         1 2022-09-30 17:21:00  95.9742  95.9742  95.9742   \n",
      "2        GOOG         1 2022-09-30 17:20:00  96.0041  96.0041  96.0041   \n",
      "3        GOOG         1 2022-09-30 17:18:00  95.9841  95.9841  95.9841   \n",
      "4        GOOG         1 2022-09-30 17:15:00  95.9642  95.9642  95.9642   \n",
      "...       ...       ...                 ...      ...      ...      ...   \n",
      "240931     KO         5 2024-04-01 04:20:00  59.8076  59.8174  59.8076   \n",
      "240932     KO         5 2024-04-01 04:15:00  59.7979  59.8076  59.7783   \n",
      "240933     KO         5 2024-04-01 04:10:00  59.8076  59.8076  59.7685   \n",
      "240934     KO         5 2024-04-01 04:05:00  59.8076  59.8076  59.7979   \n",
      "240935     KO         5 2024-04-01 04:00:00  59.7490  59.8859  59.7196   \n",
      "\n",
      "          Close  Volume  \n",
      "0       96.0144     100  \n",
      "1       95.9742     130  \n",
      "2       96.0041     300  \n",
      "3       95.9841     100  \n",
      "4       95.9642     176  \n",
      "...         ...     ...  \n",
      "240931  59.8174     669  \n",
      "240932  59.8076     498  \n",
      "240933  59.7685      11  \n",
      "240934  59.7979     205  \n",
      "240935  59.8076     265  \n",
      "\n",
      "[240936 rows x 8 columns]\n",
      "Total time elapsed : 2.095872163772583 seconds\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from arcanequant.quantlib.SQLManager import SQLSave\n",
    "# Testing manifest saving\n",
    "savepath = r'data/StockHistData/'\n",
    "SQLloginfilename = \"SQLlogin\" # Default filename \"SQLlogin\" \n",
    "\n",
    "checkManifest = DataManifest()\n",
    "checkManifest.loadManifest(savepath)\n",
    "checkManifest.validateManifest()\n",
    "checkManifest.saveManifest()\n",
    "#checkManifest.connectSQL(SQLloginfilename)\n",
    "# Saving market data from csv needs tagging of Ticker and Interval\n",
    "df1 = checkManifest.loadData_fromcsv('KO',5,'2024-04')\n",
    "df1['Ticker'] = 'KO'\n",
    "df1['Interval'] = 5\n",
    "\n",
    "SQLSave(df1, checkManifest.SQLengine, 'marketTable', echo = True)\n",
    "\n",
    "\n",
    "#SQLEstablish(checkManifest.SQLengine)\n",
    "#SQLRepair(dataManifest = checkManifest)\n",
    "\n",
    "t0 = time()\n",
    "# Testing manifest loading\n",
    "print('manifest/DM')\n",
    "zz = ExtractData('manifest', checkManifest,  start = 'all', end = '2022-11', fromSQL = False, condition = None)\n",
    "print(zz)\n",
    "t1 = time()\n",
    "print(f\"Total time elapsed : {t1-t0} seconds\")\n",
    "\n",
    "print('manifest/SQL')\n",
    "zz2 = ExtractData('manifest', checkManifest,  start = '2000-01', end = '2000-05', fromSQL = True, condition = None)\n",
    "print(zz2)\n",
    "t2 = time()\n",
    "print(f\"Total time elapsed : {t2-t1} seconds\")\n",
    "\n",
    "print('market/DM')\n",
    "# Testing stock data loading\n",
    "xx = ExtractData('market', checkManifest,  start = 'all', end = '2022-11', fromSQL = False, condition = None)\n",
    "print(xx)\n",
    "t3 = time()\n",
    "print(f\"Total time elapsed : {t3-t2} seconds\")\n",
    "\n",
    "print('manifest/SQL')\n",
    "yy = ExtractData('market', checkManifest,  start = 'all', end = '2022-11', fromSQL = True, condition = None)\n",
    "print(yy)\n",
    "t4 = time()\n",
    "print(f\"Total time elapsed : {t4-t3} seconds\")\n",
    "\n",
    "# Testing stock data saving\n",
    "#SQLSave(xx, checkManifest.SQLengine, 'marketTable', echo = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8af0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we read and post-process the .csv data and do a time-series analysis (rolling mean and std. deviation)\n",
    "# Intention is to figure out a good window size by assessing the variance and if it follows a chi-sq. distribution\n",
    "# The assumption here is the stock values are normally distributed\n",
    "# The issue is the stock mean/variance may change over time, how do I account for this?\n",
    "\n",
    "#### TO DO:\n",
    "# We are supposed to assess the % change of stock values\n",
    "# I will calculate this, and also its variance, I will do this for different resolutions and in a rolling window (I am thinking of sizes, 5, 10, 20, 50, 100)\n",
    "# Should I calculate a global (or yearly or monthly) variance and compare this to the window to see if theres a match for specific sizes (which correspond to same timeframe)?\n",
    "# Once I do this I can really go into the trade models I made previously\n",
    "\n",
    "##### THOUGHTS:\n",
    "# Is there a point in hypothesis testing a 'true' variance or mean for growth stocks? perhaps other than to test for a non-constant mean model?\n",
    "# There is point in hypothesis testing dividend stocks/ETFs (i.e. FTSE 100 Vanguard ETF)\n",
    "# Is it reasonable to model the mean (or exp. stock value) as fct of time in context of a fourier transform\n",
    "# (as many growth+dividend stocks have cyclical behaviour around quarterlies, typically goes up then down later) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_MSFT_test = checkManifest.loadData_fromcsv(\"MSFT\", 15, \"2024-08\")\n",
    "data_MSFT_test.set_index(pd.to_datetime(data_MSFT_test['DateTime']), inplace=True)\n",
    "\n",
    "\n",
    "############\n",
    "############\n",
    "# Modelling prices:\n",
    "# Get nominal price for each time, use to get percent change from last instance\n",
    "# Convert to histogram/empirical distribution and model the distribution\n",
    "\n",
    "# TO DO:\n",
    "# OBTAIN HISTOGRAM\n",
    "# I EXPECT OUTLIERS TO PROVIDE INFORMATION (PERHAPS W.R.T. INTER-DAY PRICE OR HIGHER VOLATILITY AT TRADING DAY START/END)\n",
    "\n",
    "# THE BIN RESOLUTION FOR HISTOGRAMS ARE IMPORTANT IN ASSESSING HOW CLOSELY THE DISTRIBUTION MATCHES THE DATA, MAYBE.\n",
    "# IF TOO COARSE, THE BIN CENTRE WILL BE TOO HIGH, OTHERWISE IT WILL BE TOO FLAT (AND UNEVEN)\n",
    "# IS THE BIN REPRESENTATION A BETTER ASSESSMENT OF THE RESIDUAL OF THE DATA <-> DISTRIBUTION FIT, OR CAN THE RAW DATA BE COMPARED DIRECTLY TO THE DISTRIBUTION?\n",
    "# THE LATTER SOUNDS LIKE IT DOESNT MAKE SENSE AS THE PDF IS TO DO WITH \"PROBABILITY DENSITY\" SO WE MUST USE BINS TO COLLECT REGIONS IN THE PRICE (X) DOMAIN\n",
    "# IF SO I MUST STUDY WHAT BIN SIZE IS OPTIMAL FOR RESIDUAL, IF THE OPTIMAL BIN SIZE CHANGES FOR DIFFERENT DISTRIBUTIONS (PROBABLY YES) OR FOR OUTLIER EFFECTS (ALSO YES)\n",
    "\n",
    "# TO CONSIDER:\n",
    "# MODELLING EACH TIME INSTANCE WITH ITS OWN LIMITED-DOMAIN DISTRIBUTION BASED ON MAX-MIN PRICE AND VOLUME\n",
    "# PRESUME EACH TRADE IS A RANDOM VARIABLE FOLLOWING (WHICH??) DISTRIBUTION\n",
    "# THEREFORE THE COLLECTIVE VOLUME FOLLOWS (WHICH? T-DISTRI.?) DISTRIBUTION\n",
    "\n",
    "# Obtain nominal price as mean of high and low prices (as we have no idea where the mean trade of each time instance may be so our most 'accurate' is probably the center of the limits\n",
    "# Note: can possibly assume a finite domain bell distribution for the nominal value\n",
    "data_MSFT_test['Nominal'] = (data_MSFT_test['High'] + data_MSFT_test['Low']) /2\n",
    "data_MSFT_test['PctChange_N'] = data_MSFT_test['Nominal'].diff()/data_MSFT_test['Nominal']*100\n",
    "#data_MSFT_test['PctChange_H'] = data_MSFT_test['High'].diff()/data_MSFT_test['High']*100\n",
    "#data_MSFT_test['PctChange_L'] = data_MSFT_test['Low'].diff()/data_MSFT_test['Low']*100\n",
    "\n",
    "\n",
    "# Bin count for histogram\n",
    "bincount_list = [5,10,20,40,80,160,320] # Note if too high, worse bell-curve representation\n",
    "# Interval to exclude outliers from\n",
    "exclusion_int = 0.01\n",
    "# Exclude outlier (in display)\n",
    "no_outliers = True\n",
    "\n",
    "# [min, max] ranges of the histogram\n",
    "rangemin = data_MSFT_test['PctChange_N'].min()\n",
    "rangemax = data_MSFT_test['PctChange_N'].max()\n",
    "# Exclusive interval range for histogram (excludes outliers)\n",
    "excl_min = data_MSFT_test['PctChange_N'].quantile(exclusion_int)\n",
    "excl_max = data_MSFT_test['PctChange_N'].quantile(1-exclusion_int)\n",
    "\n",
    "# Outlier data filtering\n",
    "data_filtered = data_MSFT_test[(data_MSFT_test['PctChange_N'] < excl_max) & (data_MSFT_test['PctChange_N'] > excl_min)]\n",
    "\n",
    "xmin = None\n",
    "xmax = None\n",
    "if no_outliers:\n",
    "    xmin = excl_min\n",
    "    xmax = excl_max\n",
    "else:\n",
    "    xmin = rangemin\n",
    "    xmax = rangemax\n",
    "\n",
    "# Bin sequence list for dataset histogram (for the range of bincounts)\n",
    "binseq = []\n",
    "for bincount in bincount_list:\n",
    "    binseq += [[xmin + i*(xmax-xmin)/bincount for i in range(bincount+1)]] # Can truncate for efficiency\n",
    "\n",
    "# CREATE MODEL TO FIT/SHOW ON PLOT AND ASSESS ACCURACY\n",
    "# CREATE CURVE AND EVALUATE CURVE VALUE ON HIST LOCATION, GET ITS RESIDUAL\n",
    "# JUST USE STATS TO FIND WHAT RESIDUAL WOULD BE FOR CURVES GIVEN DATAPOINTS\n",
    "# TEST CONFIDENCE INTERVALS?\n",
    "# WHAT ABOUT KERNEL DENSITY ESTIMATION?\n",
    "\n",
    "# Distribution testing\n",
    "mean_out = data_MSFT_test['PctChange_N'].mean()\n",
    "std_out = data_MSFT_test['PctChange_N'].std()\n",
    "mean_nout = data_filtered['PctChange_N'].mean()\n",
    "std_nout = data_filtered['PctChange_N'].std()\n",
    "\n",
    "pdf = stats.norm.pdf(data_MSFT_test['PctChange_N'].sort_values(), mean_out, std_out) # Values sorted as iterated through index\n",
    "pdf2 = stats.norm.pdf(data_MSFT_test['PctChange_N'].sort_values(), mean_nout, std_nout) # Values sorted as iterated through index\n",
    "plt.plot(data_MSFT_test['PctChange_N'].sort_values(), pdf, label = 'Inc. Outliers')\n",
    "plt.plot(data_MSFT_test['PctChange_N'].sort_values(), pdf2, label = 'Excl. Outliers')\n",
    "plt.xlim([xmin, xmax])\n",
    "\n",
    "\n",
    "\n",
    "# Histogram (density) plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "for bins in binseq:\n",
    "    data_MSFT_test['PctChange_N'].hist(density = True, bins=bins)\n",
    "plt.plot(data_MSFT_test['PctChange_N'].sort_values(), pdf, label = 'Inc. Outliers')\n",
    "plt.plot(data_MSFT_test['PctChange_N'].sort_values(), pdf2, label = 'Excl. Outliers')\n",
    "plt.xlim([xmin, xmax])\n",
    "\n",
    "# Histogram - Residual relation\n",
    "# CALCULATE CENTER OF BIN, ASSESS VALUE OF PDF AT THAT LOCATION, (X-Y)^2\n",
    "\n",
    "\n",
    "\n",
    "# Plot of percentage change vs. time\n",
    "plt.title('Microsoft Data Test Plot')\n",
    "plt.figure()\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "plt.title('PctChange_N')\n",
    "plt.plot(data_MSFT_test['PctChange_N'], label='PctChange_N', marker = 'o',markersize=3,linestyle='dashed')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cols_to_plot = ['Open', 'Close', 'SMA', 'Standard Deviation (O, 10)']\n",
    "\n",
    "data_MSFT_test['SMA'] = data_MSFT_test['Open'].rolling(10).mean()\n",
    "data_MSFT_test['SMAC'] = data_MSFT_test['Close'].rolling(10).mean()\n",
    "data_MSFT_test['Standard Deviation (O, 10)'] = data_MSFT_test['Open'].rolling(10).std()\n",
    "data_MSFT_test['Standard Deviation (O, 5)'] = data_MSFT_test['Open'].rolling(5).std()\n",
    "data_MSFT_test['Standard Deviation (O, 3)'] = data_MSFT_test['Open'].rolling(3).std()\n",
    "data_MSFT_test['Standard Deviation (O, 20)'] = data_MSFT_test['Open'].rolling(20).std()\n",
    "data_MSFT_test['Standard Deviation (O, 40)'] = data_MSFT_test['Open'].rolling(40).std()\n",
    "data_MSFT_test['Standard Deviation (C, 10)'] = data_MSFT_test['Close'].rolling(10).std()\n",
    "# TO DO: MAKE HISTOGRAM OF STD.D.\n",
    "\n",
    "## PLOTTING DATA\n",
    "data_MSFT_test[cols_to_plot].plot()\n",
    "plt.title('Microsoft Data Test Plot')\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Volume')\n",
    "plt.plot(data_MSFT_test['Volume'], label='Volume')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "plt.title('SMAO/C')\n",
    "plt.plot(data_MSFT_test['SMA'], label='SMA (Open)', marker = 'o',markersize=3,linestyle='dashed')\n",
    "plt.plot(data_MSFT_test['SMAC'], label='SMA (Close)', marker = 'x',markersize=3,linestyle='dashed')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "plt.title('Standard Deviation')\n",
    "plt.plot(data_MSFT_test['Standard Deviation (O, 10)'], label='Standard Deviation (Open)', marker = 'o',markersize=3,linestyle='dashed')\n",
    "plt.plot(data_MSFT_test['Standard Deviation (C, 10)'], label='Standard Deviation (Close)', marker = 'o',markersize=3,linestyle='dashed')\n",
    "\n",
    "std_dev_cols = ['Standard Deviation (O, 3)', 'Standard Deviation (O, 5)', 'Standard Deviation (O, 10)', 'Standard Deviation (O, 20)', 'Standard Deviation (O, 40)']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "plt.title('Standard Deviation')\n",
    "plt.plot(data_MSFT_test['Standard Deviation (O, 3)'], label='Standard Deviation (Open)', marker = 'o',markersize=3,linestyle='dashed')\n",
    "plt.plot(data_MSFT_test['Standard Deviation (O, 5)'], label='Standard Deviation (Open)', marker = 'o',markersize=3,linestyle='dashed')\n",
    "plt.plot(data_MSFT_test['Standard Deviation (O, 10)'], label='Standard Deviation (Open)', marker = 'o',markersize=3,linestyle='dashed')\n",
    "plt.plot(data_MSFT_test['Standard Deviation (O, 20)'], label='Standard Deviation (Open)', marker = 'o',markersize=3,linestyle='dashed')\n",
    "plt.plot(data_MSFT_test['Standard Deviation (O, 40)'], label='Standard Deviation (Open)', marker = 'o',markersize=3,linestyle='dashed')\n",
    "\n",
    "plt.title('Standard Deviation')\n",
    "data_MSFT_test[std_dev_cols].plot()\n",
    "\n",
    "## Here we also do a auto-correlation test\n",
    "#fig, ax = plt.subplots(figsize=(12, 6))\n",
    "#sm.graphics.tsa.plot_acf(data_MSFT_test['Close'], lags=195, ax=ax)\n",
    "#plt.title('Autocorrelation Function (ACF)')\n",
    "#plt.xlabel('Lags')\n",
    "#plt.ylabel('ACF')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8275aa8d-a137-4e2f-8df4-6d13fa6d987f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "## Here we read the .csv data and do a time-series analysis (rolling mean)\n",
    "\n",
    "data_ARM = pd.read_csv('StockHistData\\ARM.csv', index_col='Date', parse_dates=['Date'])\n",
    "\n",
    "#cols_to_plot = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'SMA', 'Standard Deviation']\n",
    "cols_to_plot = ['Close', 'SMA', 'Standard Deviation']\n",
    "print(data_ARM)\n",
    "data_ARM['SMA'] = data_ARM['Open'].rolling(10).mean()\n",
    "data_ARM['Standard Deviation'] = data_ARM['Open'].rolling(10).std()\n",
    "\n",
    "df_ARM = pd.DataFrame(data_ARM)\n",
    "\n",
    "data_ARM[cols_to_plot].plot()\n",
    "plt.title('ARM Technologies')\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Volume')\n",
    "plt.plot(data_ARM['Volume'], label='Volume')\n",
    "\n",
    "## Here we also do a auto-correlation test\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "sm.graphics.tsa.plot_acf(data_ARM['Close'], lags=195, ax=ax)\n",
    "plt.title('Autocorrelation Function (ACF)')\n",
    "plt.xlabel('Lags')\n",
    "plt.ylabel('ACF')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc7cd06-871d-43bd-8185-e34b0b16a972",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Same for MSFT\n",
    "data_MSFT = pd.read_csv('StockHistData\\MSFT.csv', index_col='Date', parse_dates=['Date'])\n",
    "\n",
    "#cols_to_plot = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'SMA', 'Standard Deviation']\n",
    "cols_to_plot = ['Close', 'SMA', 'Standard Deviation']\n",
    "\n",
    "data_MSFT['SMA'] = data_MSFT['Open'].rolling(10).mean()\n",
    "data_MSFT['Standard Deviation'] = data_MSFT['Open'].rolling(10).std()\n",
    "\n",
    "df_MSFT = pd.DataFrame(data_MSFT)\n",
    "\n",
    "data_MSFT[cols_to_plot].plot()\n",
    "plt.title('Microsoft')\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Volume')\n",
    "plt.plot(data_MSFT['Volume'], label='Volume')\n",
    "\n",
    "## Here we also do a auto-correlation test\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "sm.graphics.tsa.plot_acf(data_MSFT['Close'], lags=1257, ax=ax)\n",
    "plt.title('Autocorrelation Function (ACF)')\n",
    "plt.xlabel('Lags')\n",
    "plt.ylabel('ACF')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b3cd4b-a5f1-4d7b-ac2b-aabf79312517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebb5f8a-dc16-4314-8ddb-b50ff410405d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Then NVDA\n",
    "data_NVDA = pd.read_csv('StockHistData\\\\NVDA.csv', index_col='Date', parse_dates=['Date'])\n",
    "\n",
    "#cols_to_plot = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'SMA', 'Standard Deviation']\n",
    "cols_to_plot = ['Close', 'SMA', 'Standard Deviation']\n",
    "\n",
    "data_NVDA['SMA'] = data_NVDA['Open'].rolling(10).mean()\n",
    "data_NVDA['Standard Deviation'] = data_NVDA['Open'].rolling(10).std()\n",
    "\n",
    "df_NVDA = pd.DataFrame(data_NVDA)\n",
    "\n",
    "data_NVDA[cols_to_plot].plot()\n",
    "plt.title('Nvidia')\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Volume')\n",
    "plt.plot(data_NVDA['Volume'], label='Volume')\n",
    "\n",
    "## Here we also do a auto-correlation test\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "sm.graphics.tsa.plot_acf(data_NVDA['Close'], lags=1257, ax=ax)\n",
    "plt.title('Autocorrelation Function (ACF)')\n",
    "plt.xlabel('Lags')\n",
    "plt.ylabel('ACF')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5673a70-cb95-4f60-9607-2cfa82f35117",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets try for KO\n",
    "data_KO = pd.read_csv('StockHistData\\\\KO.csv', index_col='Date', parse_dates=['Date'])\n",
    "\n",
    "#cols_to_plot = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'SMA', 'Standard Deviation']\n",
    "cols_to_plot = ['Close', 'SMA', 'Standard Deviation']\n",
    "\n",
    "data_KO['SMA'] = data_KO['Open'].rolling(10).mean()\n",
    "data_KO['Standard Deviation'] = data_KO['Open'].rolling(10).std()\n",
    "\n",
    "df_KO = pd.DataFrame(data_KO)\n",
    "\n",
    "data_KO[cols_to_plot].plot()\n",
    "plt.title('Coca-cola')\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Volume')\n",
    "plt.plot(data_KO['Volume'], label='Volume')\n",
    "\n",
    "## Here we also do a auto-correlation test\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "sm.graphics.tsa.plot_acf(data_KO['Close'], lags=1257, ax=ax)\n",
    "plt.title('Autocorrelation Function (ACF)')\n",
    "plt.xlabel('Lags')\n",
    "plt.ylabel('ACF')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d167f87b-4ec3-44ad-90fe-4a6b8e9f7f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlations between the different stocks\n",
    "correlation1 = df_MSFT['Close'][-1257:].corr(df_NVDA['Close'][-1257:])\n",
    "correlation1k = df_MSFT['Close'][-1257:].corr(df_NVDA['Close'][-1257:], method='kendall')\n",
    "correlation1s = df_MSFT['Close'][-1257:].corr(df_NVDA['Close'][-1257:], method='spearman')\n",
    "\n",
    "print(f\"Correlation coefficient between MSFT and NVDA: {correlation1} ({correlation1k} with Kendall and {correlation1s} with Spearman)\")\n",
    "\n",
    "correlation2 = df_MSFT['Close'][-195:].corr(df_ARM['Close'][-195:])\n",
    "correlation2k = df_MSFT['Close'][-195:].corr(df_ARM['Close'][-195:], method='kendall')\n",
    "correlation2s = df_MSFT['Close'][-195:].corr(df_ARM['Close'][-195:], method='spearman')\n",
    "\n",
    "print(f\"Correlation coefficient between MSFT and ARM: {correlation2} ({correlation2k} with Kendall and {correlation2s} with Spearman)\")\n",
    "\n",
    "correlation3 = df_ARM['Close'][-195:].corr(df_NVDA['Close'][-195:])\n",
    "correlation3k = df_ARM['Close'][-195:].corr(df_NVDA['Close'][-195:], method='kendall')\n",
    "correlation3s = df_ARM['Close'][-195:].corr(df_NVDA['Close'][-195:], method='spearman')\n",
    "\n",
    "print(f\"Correlation coefficient between ARM and NVDA: {correlation3} ({correlation3k} with Kendall and {correlation3s} with Spearman)\")\n",
    "\n",
    "correlation4 = df_KO['Close'][-1257:].corr(df_NVDA['Close'][-1257:])\n",
    "correlation4k = df_KO['Close'][-1257:].corr(df_NVDA['Close'][-1257:], method='kendall')\n",
    "correlation4s = df_KO['Close'][-1257:].corr(df_NVDA['Close'][-1257:], method='spearman')\n",
    "\n",
    "print(f\"Correlation coefficient between KO and NVDA: {correlation4} ({correlation4k} with Kendall and {correlation4s} with Spearman)\")\n",
    "\n",
    "correlation5 = df_KO['Close'][-1257:].corr(df_MSFT['Close'][-1257:])\n",
    "correlation5k = df_KO['Close'][-1257:].corr(df_MSFT['Close'][-1257:], method='kendall')\n",
    "correlation5s = df_KO['Close'][-1257:].corr(df_MSFT['Close'][-1257:], method='spearman')\n",
    "\n",
    "print(f\"Correlation coefficient between ARM and NVDA: {correlation5} ({correlation5k} with Kendall and {correlation5s} with Spearman)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1165160f-72e0-4c4b-a765-a4c425686004",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we do simply Quantitative analysis (mean, std)\n",
    "import numpy as np\n",
    "\n",
    "# Sample Data\n",
    "returns = np.array([0.01, 0.02, -0.01, 0.03, 0.005])\n",
    "\n",
    "# Calculate Mean and Standard Deviation\n",
    "mean_return = np.mean(returns)\n",
    "std_deviation = np.std(returns)\n",
    "print(f\"Mean Return: {mean_return}, Standard Deviation: {std_deviation}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01726d8-1d92-4be2-b01e-2be0831d3c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we have code to do hypothesis testing\n",
    "from scipy import stats\n",
    "\n",
    "group_A = [0.01, 0.02, 0.015, 0.023, 0.016]\n",
    "group_B = [0.02, 0.025, 0.03, 0.019, 0.021]\n",
    "sig_value = 0.05\n",
    "\n",
    "## We are doing a t-test so see if the mean of group A and group B are the same\n",
    "## The test is bayesian, we assume they are until proven otherwise with a significance value of 0.05 (5%)\n",
    "t_statistic, p_value = stats.ttest_ind(group_A, group_B)\n",
    "print(f\"t-statistic: {t_statistic}, p-value: {p_value}\")\n",
    "\n",
    "trunc_p = '%.3f'%(100*p_value)\n",
    "\n",
    "if p_value >= sig_value:\n",
    "    print(f\"Hypothesis of Group A and Group B means being equal is NOT REJECTED (significance value of {100*sig_value}%, p-value of {trunc_p}%)\")\n",
    "else:\n",
    "    print(f\"Hypothesis of Group A and Group B means being equal is REJECTED (significance value of {100*sig_value}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e153e5-c7b7-4a20-ab09-cd41b6940bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we do some more time-series analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Seed for reproducibility (we will create synthetic data)\n",
    "np.random.seed(99)\n",
    "\n",
    "# Generate synthetic data\n",
    "n_samples = 100\n",
    "dates = pd.date_range(start='2024-01-01', periods=n_samples, freq='B')  # Business days\n",
    "price_changes = np.random.normal(loc=0, scale=1, size=n_samples)  # Random price changes\n",
    "prices = np.cumsum(price_changes) + 100  # Simulated stock prices (random walk)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Date': dates,\n",
    "    'Close': prices\n",
    "})\n",
    "\n",
    "# Plot the time series\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df['Date'], df['Close'], label='Close Price')\n",
    "plt.title('Synthetic Stock Closing Prices')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and plot ACF\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "sm.graphics.tsa.plot_acf(df['Close'], lags=30, ax=ax)\n",
    "plt.title('Autocorrelation Function (ACF)')\n",
    "plt.xlabel('Lags')\n",
    "plt.ylabel('ACF')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c125341-7ef7-408f-a23d-fc4ca588924d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we do some predictive modelling\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Sample Data\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([2, 3, 2.5, 4, 4.5])\n",
    "\n",
    "# Train Model\n",
    "model = LinearRegression().fit(X, y)\n",
    "print(f\"Coefficient: {model.coef_}, Intercept: {model.intercept_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba1aa73-a950-4905-9c48-f4a6e0958223",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we have some code for option pricing (black-scholes)\n",
    "\n",
    "import scipy.stats as si\n",
    "import numpy as np\n",
    "\n",
    "def black_scholes(S, K, T, r, sigma, option_type='call'):\n",
    "    d1 = (np.log(S / K) + (r + 0.5 * sigma ** 2) * T) / (sigma * np.sqrt(T))\n",
    "    d2 = d1 - sigma * np.sqrt(T)\n",
    "    if option_type == 'call':\n",
    "        option_price = (S * si.norm.cdf(d1, 0, 1) - K * np.exp(-r * T) * si.norm.cdf(d2, 0, 1))\n",
    "    elif option_type == 'put':\n",
    "        option_price = (K * np.exp(-r * T) * si.norm.cdf(-d2, 0, 1) - S * si.norm.cdf(-d1, 0, 1))\n",
    "    return option_price\n",
    "\n",
    "# Example Parameters\n",
    "S = 100  # Current stock price\n",
    "K = 105  # Strike price\n",
    "T = 1    # Time to maturity in years\n",
    "r = 0.05 # Risk-free rate\n",
    "sigma = 0.2 # Volatility\n",
    "\n",
    "call_price = black_scholes(S, K, T, r, sigma, option_type='call')\n",
    "print(f\"Call Option Price: {call_price}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30655935-ea66-43cb-b321-97777facd378",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here's some code of logistic regression\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'Open': [100.0, 101.0, 100.5, 101.2],\n",
    "    'High': [102.0, 103.0, 102.5, 101.5],\n",
    "    'Low': [98.0, 99.5, 99.0, 99.8],\n",
    "    'Close': [101.0, 100.5, 101.5, 100.8],\n",
    "    'Volume': [1500000, 1700000, 1800000, 1300000],\n",
    "    'SMA_10': [99.5, 100.0, 100.2, 100.8],\n",
    "    'RSI': [55.0, 52.5, 58.0, 59],\n",
    "    'Label': [1, 0, 1, 0]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Features and target\n",
    "X = df[['Open', 'High', 'Low', 'Close', 'Volume', 'SMA_10', 'RSI']]\n",
    "y = df['Label']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler(Y) # type: ignore\n",
    "X_train_scaled = scaler.it_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled,y_train)\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c00d66-9492-4728-9d7e-738a8015fd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Seed for reproducibility\n",
    "np.random.seed(10)\n",
    "\n",
    "# Generate synthetic data\n",
    "n_samples = 1000\n",
    "dates = pd.date_range(start='2024-01-01', periods=n_samples, freq='B')\n",
    "open_prices = np.random.uniform(low=100, high=200, size=n_samples)\n",
    "high_prices = open_prices + np.random.uniform(low=0, high=10, size=n_samples)\n",
    "low_prices = open_prices - np.random.uniform(low=0, high=10, size=n_samples)\n",
    "close_prices = open_prices + np.random.uniform(low=-5, high=5, size=n_samples)\n",
    "volume = np.random.randint(low=100000, high=5000000, size=n_samples)\n",
    "\n",
    "# Simple Moving Average (SMA) with window of 10\n",
    "sma_10 = pd.Series(close_prices).rolling(window=10).mean().ffill()\n",
    "\n",
    "# Relative Strength Index (RSI)\n",
    "def calculate_rsi(series, window=14):\n",
    "    delta = series.diff().ffill()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "rsi = calculate_rsi(pd.Series(close_prices))\n",
    "\n",
    "# Label: 1 if next day's close price is higher, else 0\n",
    "labels = np.where(np.roll(close_prices, -1) > close_prices, 1, 0)\n",
    "labels[-1] = 0  # Last label cannot be determined\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Date': dates,\n",
    "    'Open': open_prices,\n",
    "    'High': high_prices,\n",
    "    'Low': low_prices,\n",
    "    'Close': close_prices,\n",
    "    'Volume': volume,\n",
    "    'SMA_10': sma_10,\n",
    "    'RSI': rsi,\n",
    "    'Label': labels\n",
    "})\n",
    "\n",
    "# Remove the last row as it doesn't have a valid label\n",
    "df = df[:-1]\n",
    "\n",
    "# Features and target\n",
    "X = df[['Open', 'High', 'Low', 'Close', 'Volume', 'SMA_10', 'RSI']]\n",
    "y = df['Label']\n",
    "t = df['Date']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test, t_train, t_test = train_test_split(X, y, t, test_size=0.33, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(\"Data type: \",X_train_scaled.dtype )\n",
    "print(\"Data type: \",y_train.dtype )\n",
    "print(\"Dimensions: \",np.shape(X_train_scaled))\n",
    "print(\"Dimensions: \",np.shape(y_train))\n",
    "\n",
    "# Train the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Plot some of the data to visualize\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(dates, close_prices, label='Close Price')\n",
    "plt.plot(dates, sma_10, label='SMA 10')\n",
    "plt.title('Stock Prices and SMA 10')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369ca1d0-f34e-4b65-87ce-d68b79efbb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Information store of how we run python script elsewhere\n",
    "\n",
    "%run \"D:\\Finance Study\\Python and MATLAB Code\\Core.py\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9d3225-e353-4a5f-b3e5-d8550b4e7085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we try to make a signal indicator from the SMIIO (Stochastic Momentum Index ergodic Indicator plus SMI ergodic Oscillator) of a stock\n",
    "# The code makes an SMI of the stock by calculating the True Strength Index (TSI) of the stock using a slow and a fast period. Also makes an\n",
    "# (ergodic) Indicator by using an Exponential Moving Average (EMA) using the signal period. Finally, makes (ergodic) Oscillator signal by\n",
    "# subtracting the Indicator from the TSI.\n",
    "\n",
    "\n",
    "\n",
    "fast_period = 3 # In the freq of file (days)\n",
    "slow_period = 15\n",
    "signal_period = 3\n",
    "\n",
    "\n",
    "df_smi = ta.momentum.smi(df_NVDA['Close'],fast_period,slow_period,signal_period)\n",
    "\n",
    "\n",
    "smi_name = \"_{}_{}_{}\".format(fast_period,slow_period,signal_period)\n",
    "#SMI + smi_name is the SMI of the stock\n",
    "#SMIs + smi_name is the indicator made from signal line\n",
    "#SMIo + smi_name is the oscillator made by SMI - SMIs\n",
    "\n",
    "\n",
    "cols_plot = [\"SMI\"+smi_name,\"SMIs\"+smi_name,\"SMIo\"+smi_name]\n",
    "\n",
    "df_smi[cols_plot].plot()\n",
    "\n",
    "df_smi.fillna(0, inplace = True) # Removing NaN values\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(df_smi)\n",
    "\n",
    "subtrac=df_smi.iloc[:,0] - df_smi.iloc[:,1] # This is the same as SMIo\n",
    "#plt.figure(figsize=(12,6))\n",
    "#plt.plot(subtrac)\n",
    "\n",
    "\n",
    "df_smi = df_smi.reset_index() # Get date as a non index col to use for bar plot\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.bar(df_smi[\"Date\"],df_smi[\"SMIo\"+smi_name],2,12,color=np.where(df_smi[\"SMIo\"+smi_name] < 0, 'crimson', 'green'))\n",
    "\n",
    "\n",
    "## BEAUTIFULSOUP FOR WEBSCRAPING IN PYTHON\n",
    "## SELENIUM FOR dynamic html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c3f118-822a-4f85-88c7-ae27322de30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we try to make a simple backtesting simulation model where we use an indicator to test approximate success in trading over course of a day\n",
    "# or longer. We use the SMIIO model and simulate a trade whenever a condition is reached (e.g. change from negative to positive).\n",
    "# The trade is initially done as 1 stock trade per instance and we assess the percentage of success (profit made), and size of success.\n",
    "\n",
    "fast = 3\n",
    "slow = 15\n",
    "sig = 4\n",
    "\n",
    "df_smiio = ta.momentum.smi(df_NVDA['Close'],fast,slow,sig)\n",
    "\n",
    "smiio_name = \"_{}_{}_{}\".format(fast,slow,sig)\n",
    "\n",
    "df_smiio = df_smiio.reset_index()\n",
    "\n",
    "#df_smiio.fillna(0, inplace = True) # Removing NaN values\n",
    "\n",
    "\n",
    "for i in range(0,len(df_smiio['SMIo'+smiio_name])):\n",
    "\n",
    "    if np.isnan(df_smiio['SMIo'+smiio_name][i]):\n",
    "        continue\n",
    "\n",
    "    ind_diff = df_smiio['SMIo'+smiio_name][i] - df_smiio['SMIo'+smiio_name][i-1]\n",
    "\n",
    "    last_val = df_smiio['SMIo'+smiio_name][i-1]\n",
    "    \n",
    "    #print(df_smiio['SMIo'+smiio_name][i])\n",
    "    #print(ind_diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5fef8b-5995-4a66-b64c-6b26df302711",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Here we will write basic functions to 'purchase' and 'sell' stocks, and a function to sum up the transactions for a profit/loss measurement\n",
    "histCols = ['Date','Ticker','Volume','Value','TimingState']\n",
    "global dfTradeHist\n",
    "dfTradeHist = pd.DataFrame(columns = histCols)\n",
    "\n",
    "#######\n",
    "def PrepDataFrame(dataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Make all necessary preparation to use the dataframe in model (NOT FULLY COMPLETE). Currently this is:\n",
    "    - Moving any date/time outside of dataframe index\n",
    "    - Changing date/time to a full date-time style (i.e. dates-only will also include time (00:00:00))\n",
    "    \"\"\"\n",
    "    # Guard function to get indexData (usually date) as a non index col to use (if as index) \n",
    "    if 'Date' not in dataFrame.columns: \n",
    "        dataFrame = dataFrame.reset_index()\n",
    "    \n",
    "    # Converts possible date-only to date-time\n",
    "    dataFrame['Date'] = pd.to_datetime(dataFrame.Date, format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # Convert date-time to string (seems to create more bugs)\n",
    "    #dataFrame['Date'] = dataFrame['Date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    return dataFrame\n",
    "#######\n",
    "\n",
    "#######\n",
    "## A FUNCTION TO REMOVE DATA? PROB NOT NEEDED\n",
    "#######\n",
    "\n",
    "\n",
    "\n",
    "#######\n",
    "## A SET OF FUNCTIONS TO ANALYSE STOCK DATA, TRAJECTORY, VOLATILITY ETC.\n",
    "#######\n",
    "\n",
    "\n",
    "#######\n",
    "## FUNCTION TO MODIFY AN ACCURACY/ANALYTICS MATRIX DIRECTLY (INSTEAD OF DOING IT WITHIN THE MODEL ITSELF) FOR INTEROPERABILITY WITH MODEL EVALUATOR\n",
    "#######\n",
    "\n",
    "\n",
    "\n",
    "#######\n",
    "def ResetTradeHist(name = 'dfTradeHist', cols = ['DateTime','Ticker','Volume','Value','TimingState']):\n",
    "    \"\"\" Reset the trade history dataframe and remakes its columns (cols, if specified). \"\"\"    \n",
    "    globals()[name] = pd.DataFrame(columns = cols)\n",
    "    return\n",
    "#######\n",
    "\n",
    "#######\n",
    "def MakeTrade (dateTime, volume, ticker, history = None, buy = True, timingState = 'Close', declare = False) -> [[float, str, float, float, str], pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Simulates making a specified trade and records it (using a sub-routine). Ticker price dataframe must have 'df_' prefix\n",
    "    \"\"\"\n",
    "    # Default history input save status (take dfTradeHist as default storage dataframe)\n",
    "    isDefaultHist = False\n",
    "    if history is None:\n",
    "        global dfTradeHist\n",
    "        history = dfTradeHist.copy()\n",
    "        isDefaultHist = True\n",
    "    \n",
    "    \n",
    "    datasetName = 'df_'+ticker\n",
    "\n",
    "    # The 1st part gets the global dataset, 2nd searches the datetime and 3rd gets the timings state price\n",
    "    unitValue = globals()[datasetName][ globals()[datasetName]['Date'] == dateTime ][timingState].values[0]\n",
    "    \n",
    "    totalValue = -unitValue * volume\n",
    "    transaction = 'bought'\n",
    "    \n",
    "    if not buy:\n",
    "        transaction = 'sold'\n",
    "\n",
    "    if declare == True:\n",
    "        print(f\"{volume} {ticker} stock(s) {transaction} at total price {-totalValue} (unit price {unitValue}) at {dateTime}\")\n",
    "\n",
    "    if not buy:\n",
    "        totalValue = -totalValue\n",
    "\n",
    "    result = [dateTime, ticker, volume, totalValue, timingState]\n",
    "    \n",
    "    ## Store in history\n",
    "    history = RecordAction(result, history)\n",
    "\n",
    "    # Default history location (if None then it won't update the original dataframe otherwise)\n",
    "    if isDefaultHist:\n",
    "        dfTradeHist = history.copy()\n",
    "    \n",
    "    return [[dateTime, ticker, volume, totalValue, timingState], history]\n",
    "#######\n",
    "#######\n",
    "def RecordAction(action, dataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Records action taken into a history dataframe.\n",
    "    Note: Make sure the action row size is the same as the column size in the dataFrame.\n",
    "    For each column take action indices and add to the existing history dataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    # A slightly different process for first action recorded to avoid python warning\n",
    "    if dataFrame.size == 0: # If no data\n",
    "        \n",
    "        print('Recording first input into trade history')\n",
    "        dataFrame = pd.DataFrame(columns = dataFrame.columns)\n",
    "\n",
    "    # Adding all actions into a dictionary to append to dataFrame\n",
    "    appendDict = {}\n",
    "    \n",
    "    for i in range(0,len(action)):\n",
    "        appendDict[dataFrame.columns[i]] = action[i]\n",
    "    appendSeries = pd.Series(appendDict)    \n",
    "    \n",
    "    return pd.concat([dataFrame,appendSeries.to_frame().T], ignore_index=True)\n",
    "#######\n",
    "#######\n",
    "def EvaluateModel(dataFrame, *arguments, **keywords) -> float:\n",
    "    \"\"\"\n",
    "    This function is supposed to take the trade/call history and evaluate its properties,\n",
    "    such as success (profit/loss) and other properties (e.g. profit vs. loss freq. etc.).\n",
    "    \"\"\"\n",
    "    # Additional things to implement:\n",
    "    # Exchange rate of currency effects\n",
    "    # Trading costs\n",
    "    \n",
    "    \n",
    "    print('------------------')\n",
    "    print('Evaluating current algo model...')\n",
    "    \n",
    "    # Stratifying data based on ticker\n",
    "    stratDF = dataFrame.groupby('Ticker').apply(lambda x: x)\n",
    "    stratDF = stratDF.drop(columns=['Ticker'])\n",
    "\n",
    "    # Extended output\n",
    "    if any(val == 'Extended' for val in keywords.values()):\n",
    "        print('List of trades to be evaluated:')\n",
    "        print(stratDF)\n",
    "\n",
    "\n",
    "    # Make a DF of compiled stats\n",
    "    basicStatsCols = ['LongVol','ShortVol','RemainVol','Cost','Income','Profit']\n",
    "    compileDF = pd.DataFrame(columns = basicStatsCols)\n",
    "    \n",
    "\n",
    "    # Takes list of the (multi-level) index of DF (tuple) and converts to\n",
    "    # dictionary for unique 'Ticker' key \n",
    "    print('List of tickers traded:')\n",
    "    for key in dict(stratDF.index.tolist()).keys():\n",
    "        print(key)\n",
    "        # key is the different ticker names\n",
    "\n",
    "    # Implement the evaluation for each ticker traded here\n",
    "    for key in dict(stratDF.index.tolist()).keys():\n",
    "        # stratDF.loc[key] is the different ticker trades each in own DF\n",
    "        tickerDF = stratDF.loc[key]\n",
    "\n",
    "        # Extended output\n",
    "        for value in keywords.values():\n",
    "            if value == 'Verbose':\n",
    "                print('Trade history of ticker ' + key +':')\n",
    "                print(tickerDF)\n",
    "\n",
    "        sumVal = tickerDF['Value'].sum()\n",
    "        \n",
    "        ## Basic stats\n",
    "        sumShortVol = 0\n",
    "        sumCost = 0 # Cost is for purchasing the share etc.\n",
    "        sumLongVol = 0\n",
    "        sumIncome = 0 # Income is obtained by selling\n",
    "        for i in range(0,len(tickerDF['Value'])):\n",
    "            if tickerDF['Value'].iloc[i] > 0:\n",
    "                sumShortVol = sumShortVol + tickerDF['Volume'].iloc[i]\n",
    "                sumIncome = sumIncome + tickerDF['Value'].iloc[i]\n",
    "            else:\n",
    "                sumLongVol = sumLongVol + tickerDF['Volume'].iloc[i]\n",
    "                sumCost = sumCost - tickerDF['Value'].iloc[i]\n",
    "        profit = sumIncome - sumCost\n",
    "        specProfit = profit*2/(sumShortVol+sumLongVol)\n",
    "    \n",
    "        # To implement a existing position closing system (to evaluate the model more accurately)\n",
    "        remainVol = sumLongVol - sumShortVol\n",
    "        if remainVol != 0:\n",
    "            print(Fore.RED)\n",
    "            print(' !!!!!!!!!!')\n",
    "            print('There is an existing open position in ' + key + '! This may impact the accuracy of the model evaluation.')\n",
    "            if value == 'Extended':\n",
    "                if remainVol > 0:\n",
    "                    print('Open position size: ' + str(remainVol) + ' shares long.')\n",
    "                else:\n",
    "                    print('Open position size: ' + str(-remainVol) + ' shares short.')\n",
    "            \n",
    "            print('Existing positions will be closed.')\n",
    "            print(' !!!!!!!!!!')\n",
    "            print(Fore.BLACK)\n",
    "        \n",
    "            # Closing existing positions using last unit price\n",
    "            datasetName = 'df_' + key\n",
    "            \n",
    "            # Portfolio value is positive if long (remain vol > 0)\n",
    "            portValue = remainVol * globals()[datasetName]['Close'][len(globals()[datasetName]['Close'])-1]\n",
    "            \n",
    "            finalProfit = profit + portValue\n",
    "            if portValue > 0:\n",
    "                finalCost = sumCost\n",
    "                finalIncome = sumIncome + portValue\n",
    "            else:\n",
    "                finalCost = sumCost + portValue\n",
    "                finalIncome = sumIncome\n",
    "    \n",
    "        ## Advanced stats \n",
    "\n",
    "\n",
    "\n",
    "        # Adding all basic stats for each ticker into an array then converting into dictionary to append to dataFrame\n",
    "        # basicStats uses 'final' position if unclosed     \n",
    "        basicStats = []        \n",
    "        if remainVol != 0:\n",
    "            basicStats = [sumLongVol, sumShortVol,remainVol,finalCost,finalIncome,finalProfit]\n",
    "        else:\n",
    "            basicStats = [sumLongVol, sumShortVol,remainVol,sumCost,sumIncome,profit]\n",
    "\n",
    "        # Append dictionary for dataFrame\n",
    "        appendDict = {}\n",
    "        for i in range(0,len(basicStats)):\n",
    "            appendDict[compileDF.columns[i]] = basicStats[i]\n",
    "\n",
    "        compileDF = compileDF._append(appendDict, ignore_index=True)\n",
    "    \n",
    "        # Currently only evaluates the profit/loss levels (not including closing existing positions)\n",
    "        print('===================\\n\\nCalculating basic evaluation stats in ticker '+key+':')\n",
    "        \n",
    "        print('Current profit/loss stats (not closing existing positions):')\n",
    "        print('Total profit from all trades: ' + str(sumVal))\n",
    "        print('Total cost: ' + str(sumCost))\n",
    "        print('Total income: ' + str(sumIncome))\n",
    "        print('Total profit: ' + str(profit))\n",
    "        \n",
    "        print('Profit Margin: ' + str(profit/sumCost))\n",
    "        print('Specific Profit (per volume traded): ' + str(specProfit)) # Calculated as avg. of long and short vol. (if shares outstanding)\n",
    "    \n",
    "        if remainVol != 0:\n",
    "            print(Fore.BLUE)\n",
    "            print('Current profit/loss stats (after closing existing positions):')\n",
    "            print('Remaining portfolio value (before close): ' + str(portValue))\n",
    "            print('Final cost: ' + str(compileDF['Cost'].sum()))\n",
    "            print('Final income: ' + str(compileDF['Income'].sum()))\n",
    "            print('Final profit from all trades: ' + str(compileDF['Profit'].sum()))\n",
    "            \n",
    "            print('Final Profit Margin: ' + str(compileDF['Profit'].sum()/compileDF['Cost'].sum()))\n",
    "            print(Fore.BLACK)\n",
    "        \n",
    "        print('===================')\n",
    "\n",
    "\n",
    "    if any(val == 'AddEval' for val in keywords.values()):\n",
    "        for input in arguments: # Do not need to use *args this way but I chose to, its fine until I want to use more than 1 args dataframe\n",
    "            if isinstance(input, pd.DataFrame):\n",
    "\n",
    "                ## TO ADD TRADING ANALYTICS HERE, THIS SHOULD ONLY TAKE IN OUTPUT FROM ELSEWHERE\n",
    "                ## SHOULD CREATE FUNCTIONS TO DIRECTLY MODIFY ACCURACY MATRIX INSTEAD OF MAKING WITHIN EACH MODEL\n",
    "                ## SHOULD ALSO CREATE STOCK ANALYSER METHOD SEPARATELY\n",
    "\n",
    "            \n",
    "                print(Fore.GREEN)\n",
    "                print('#################################\\nCalculating additional model evaluations from given accuracy dataFrame:')\n",
    "                print('Model efficiency stats:')\n",
    "                print('Trade accuracy (trades being incorrect by instance, not volume?): (NOT IMPLEMENTED YET)')\n",
    "                print('Trade efficiency (how many trades are not correct?): (NOT IMPLEMENTED YET)')\n",
    "                print('Model Loss (how far are trades from actual optimal points): (NOT IMPLEMENTED YET)')\n",
    "                print('#################################')\n",
    "                print(Fore.BLACK)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print('------------------')\n",
    "    return 1\n",
    "\n",
    "df_NVDA = PrepDataFrame(df_NVDA)\n",
    "df_MSFT = PrepDataFrame(df_MSFT)\n",
    "df_KO = PrepDataFrame(df_KO)\n",
    "\n",
    "[NVDA_L1,_] = MakeTrade('2019-07-01 00:00:00', 10.5, 'NVDA')\n",
    "[MSFT_L1,_] = MakeTrade('2019-10-01 00:00:00', 6, 'MSFT')\n",
    "[NVDA_S1,_] = MakeTrade('2022-07-01 00:00:00', 10.5, 'NVDA', buy = False)\n",
    "[NVDA_S2,_] = MakeTrade('2023-06-01 00:00:00', 100, 'NVDA', buy = False)\n",
    "[NVDA_L2,_] = MakeTrade('2023-08-01 00:00:00', 99, 'NVDA', buy = True)\n",
    "[MSFT_S1,_] = MakeTrade('2022-07-01 00:00:00', 6, 'MSFT', history = None, buy = False)\n",
    "\n",
    "num = EvaluateModel(dfTradeHist, depth = 'AddEval', debug = 'Simple')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b4f61f-d884-45ea-bf1d-8431a33ba96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating sub-routines for models here\n",
    "\n",
    "# Here we will create a sub-routine that provides conditions for specified trades when a condition is met.\n",
    "# The basic one will be when a set of values becomes positive (from a negative/zero value).\n",
    "\n",
    "\n",
    "def WhenPositive(dataset,searchData,indexData):\n",
    "    \"\"\"\n",
    "    This function returns set of independent variables (outData, e.g. date/time) in the selected data\n",
    "    to be searched (indexData) from the dataset, when searchData (e.g. price) turns positive from negative.\n",
    "    searchData and indexData are strings of the column names.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Indexed (observed) dataset (independent var.)\n",
    "    obsDataset = dataset[indexData]\n",
    "\n",
    "    # Searched dataset (dependent var.)\n",
    "    searchDataset = dataset[searchData]\n",
    "\n",
    "    # Outputted dataset array (indep. var.)\n",
    "    outDataset = []\n",
    "\n",
    "    # Goes through all points to get when turn positive\n",
    "    for i in range(1,len(obsDataset)):\n",
    "        # range starts from 1 because need the i-1'th datapoint\n",
    "        # If negative, irrelevant so skip step\n",
    "        if searchDataset.iloc[i] <= 0:\n",
    "            continue\n",
    "\n",
    "        # If positive but last step negative, record\n",
    "        if searchDataset.iloc[i-1] < 0:\n",
    "            outDataset = outDataset + [obsDataset.iloc[i]]\n",
    "\n",
    "    # Note output is an array for efficiency\n",
    "    return outDataset\n",
    "\n",
    "def WhenNegative(dataset,searchData,indexData):\n",
    "    \"\"\"\n",
    "    This function returns set of independent variables (outData, e.g. date/time) in the selected data\n",
    "    to be searched (indexData) from the dataset, when searchData (e.g. price) turns negative from positive.\n",
    "    searchData and indexData are strings of the column names.\n",
    "    Same as WhenPositive but reverse as you cant just use -dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Indexed (observed) dataset (independent var.)\n",
    "    obsDataset = dataset[indexData]\n",
    "\n",
    "    # Searched dataset (dependent var.)\n",
    "    searchDataset = dataset[searchData]\n",
    "\n",
    "    # Outputted dataset array (indep. var.)\n",
    "    outDataset = []\n",
    "\n",
    "    # Goes through all points to get when turn negative\n",
    "    for i in range(1,len(obsDataset)-1):\n",
    "        # range starts from 1 because need the i-1'th datapoint\n",
    "        # range ends at len(obsDataset)-1 if last datapoint is 'live' and pending update\n",
    "        # If positive, irrelevant so skip step\n",
    "        if searchDataset.iloc[i] >= 0:\n",
    "            continue\n",
    "\n",
    "        # If negative but last step positive, record\n",
    "        if searchDataset.iloc[i-1] > 0:\n",
    "            outDataset = outDataset + [obsDataset.iloc[i]]\n",
    "\n",
    "    # Note output is an array for efficiency\n",
    "    return outDataset\n",
    "\n",
    "def JumpChecker(dataset, searchData, indexData, jumpThresh, jumpPeriod = 1, signalPeriod = 1):\n",
    "    \"\"\"\n",
    "    This function returns set of independent variables (outData, e.g. date/time) in the selected data\n",
    "    to be searched (indexData) from the dataset, when searchData (e.g. price) jumps (or changes) a\n",
    "    certain percentage (jumpThresh) on average of a (signalPeriod) time period, from its last (jumpPeriod)\n",
    "    time periods ago.\n",
    "    searchData and indexData are strings of the column names.\n",
    "    \"\"\"\n",
    "\n",
    "    # Error input guard (fixes negative input values for real numbers)\n",
    "    jumpThresh = abs(jumpThresh)\n",
    "    jumpPeriod = abs(jumpPeriod)\n",
    "    \n",
    "    # Indexed (observed) dataset (independent var.) - dataset[indexData]\n",
    "    # Searched dataset (dependent var.) - dataset[searchData]\n",
    "\n",
    "    # Outputted dataset array pair (independent var., and bool) [['Time'],['Jump?']]\n",
    "    outDataset = []\n",
    "    jumpTime = []\n",
    "    isJump = []\n",
    "    \n",
    "    # Get rolling simple moving average of dataset (SMA as signal safety vs instant drop/jumps or spread drop/jumps)\n",
    "    copyDataset = dataset.copy() # Make shallow copy to not bloat original dataset\n",
    "    copyDataset['SMA ' + searchData] = copyDataset[searchData].rolling(window = signalPeriod).mean()\n",
    "    \n",
    "    # Goes through all points to get when turn negative\n",
    "    for i in range(jumpPeriod,len(dataset[indexData])-1):\n",
    "        \n",
    "        # Range starts from jumpPeriod because SMA starts from jumpPeriod-1'th datapoint (as index starts at 0),\n",
    "        # but need the value before jumpPeriod (so +1) since that is the drop signal datapoint. \n",
    "        # Range ends at len(obsDataset)-1 if last datapoint is 'live' and pending update\n",
    "\n",
    "\n",
    "        # Error guard function (0 val input), skip step\n",
    "        if dataset[searchData][i-jumpPeriod] == 0:\n",
    "            continue\n",
    "\n",
    "        # Jump ratio is the [amount at time-index i] - [amount at i-jumpPeriod (pre-jump) time] / pre-jump value (normalisation) \n",
    "        jumpRatio = (copyDataset['SMA ' + searchData][i] - copyDataset['SMA ' + searchData][i-jumpPeriod])/copyDataset['SMA ' + searchData][i-jumpPeriod]\n",
    "        #jumpRatio = (dataset[searchData][i] - dataset[searchData][i-jumpPeriod])/dataset[searchData][i-jumpPeriod]\n",
    "        \n",
    "        # Check for value jump (above jumpThresh) or drop (below -jumpThresh), if so, record point\n",
    "        if jumpRatio > jumpThresh:\n",
    "            #print('Value jump detected: ' + str(jumpRatio*100) + '%.')\n",
    "            # Record data (as jump)\n",
    "            jumpTime = jumpTime + [dataset[indexData][i]]\n",
    "            isJump = isJump + [True]\n",
    "\n",
    "            #outDataset = outDataset + [[dataset[indexData][i], True]]\n",
    "\n",
    "        elif jumpRatio < -jumpThresh:\n",
    "            #print('Value drop detected: ' + str(-jumpRatio*100) + '%.')\n",
    "            # Record data (as drop)\n",
    "            jumpTime = jumpTime + [dataset[indexData][i]]\n",
    "            isJump = isJump + [False]\n",
    "            \n",
    "            #outDataset = outDataset + [[dataset[indexData][i], False]]\n",
    "\n",
    "    outDataset = [jumpTime, isJump]\n",
    "    # Note output is an array for efficiency\n",
    "    return outDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0db583-0d19-41a3-aee1-5ca37928f374",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing playground to add into the model evaluator\n",
    "\n",
    "\n",
    "# Note: using frozenset() for checks can make it faster for big datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be29d90-8b86-41ca-b435-1ffabfd07d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def SMIFlipTradeModel(ticker, fast_period, slow_period, signal_period, timingState = 'Close'):\n",
    "    \"\"\"This trade model trades when SMIIO of ticker becomes ('flips') to positive (long) and negative (short).\"\"\"\n",
    "\n",
    "    global histCols\n",
    "    # Create trade history dataframe\n",
    "    tradeHist = pd.DataFrame(columns = histCols)\n",
    "    \n",
    "    # Get and prep relevant dataFrame\n",
    "    df_ticker = PrepDataFrame(globals()['df_'+ticker])\n",
    "\n",
    "    # Apply SMI model to get result dataframe (and add date column)\n",
    "    df_smi = ta.momentum.smi(df_ticker['Close'],fast_period,slow_period,signal_period)\n",
    "    df_smi['Date'] = df_ticker['Date'].copy()\n",
    "    \n",
    "    smiConfigName = \"_{}_{}_{}\".format(fast_period,slow_period,signal_period)\n",
    "    # SMI + smi_name is the SMI of the stock\n",
    "    # SMIs + smi_name is the indicator made from signal line\n",
    "    # SMIo + smi_name is the oscillator made by SMI - SMIs\n",
    "    \n",
    "    smiTypeNames = [\"SMI\"+smiConfigName,\"SMIs\"+smiConfigName,\"SMIo\"+smiConfigName]\n",
    "    \n",
    "    # Removing NaN values\n",
    "    df_smi.fillna(0, inplace = True) \n",
    "    \n",
    "    buyFlip = WhenPositive(df_smi,smiTypeNames[2],'Date')\n",
    "    sellFlip = WhenNegative(df_smi,smiTypeNames[2],'Date')\n",
    "\n",
    "    # Currently only trade one stock per instance, can be made variable.\n",
    "    for i in range(0,len(buyFlip)):\n",
    "        [maketrade, tradeHist] = MakeTrade(buyFlip[i].strftime('%Y-%m-%d %H:%M:%S'), 1, ticker, history = tradeHist)\n",
    "    \n",
    "    for i in range(0,len(sellFlip)):\n",
    "        [maketrade, tradeHist] = MakeTrade(sellFlip[i].strftime('%Y-%m-%d %H:%M:%S'), 1, ticker, history = tradeHist, buy = False)\n",
    "    \n",
    "    eval = EvaluateModel(tradeHist, depth = 'AddEval', debug = 'Extended')\n",
    "    \n",
    "    return tradeHist\n",
    "\n",
    "def SuddenChangeTradeModel(ticker, changeThresh = 0.05, changePeriod = 1, revertRatio = 0.9, safetyPeriod = 0, timingState = 'Close'):\n",
    "    \"\"\"\n",
    "    This model trades when a ticker suddenly changes up (short) or down (long) by a certain amount\n",
    "    (Thresh), and 'reverts' position when it goes back up.\n",
    "    \n",
    "    Details: Can revert the up or down jumps partially using a setting (revertRatio), and can add safety factors\n",
    "    to the initation signal (safetyPeriod). Currently only takes one ticker.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create trade history (global, local seems to make issues)\n",
    "    global histCols\n",
    "    tradeHist = pd.DataFrame(columns = histCols)\n",
    "    \n",
    "    # Get relevant dataFrame and pre-process it\n",
    "    df_ticker = globals()['df_'+ticker].copy()\n",
    "    df_ticker = PrepDataFrame(df_ticker)\n",
    "\n",
    "    # Obtain timings of significant price changes ([Date of change ending, bool if its jump (up)])\n",
    "    changeTimings = JumpChecker(df_ticker, timingState, 'Date', changeThresh, jumpPeriod = changePeriod, signalPeriod = safetyPeriod)\n",
    "    # Make dummy ticker to establish repurchase date\n",
    "    df_dummy = copy.deepcopy(df_ticker)\n",
    "    \n",
    "    # Currently only trade one stock per instance, can be made variable.\n",
    "    for i in range(0,len(changeTimings[0])):\n",
    "        \n",
    "        # If jump\n",
    "        if changeTimings[1][i]:\n",
    "            # Sell now, buy later\n",
    "            [makeTrade, tradeHist] = MakeTrade(changeTimings[0][i].strftime('%Y-%m-%d %H:%M:%S'), 1, ticker, history = tradeHist, buy = False)\n",
    "\n",
    "        else: # If drop\n",
    "            # Buy now, sell later\n",
    "            [makeTrade, tradeHist] = MakeTrade(changeTimings[0][i].strftime('%Y-%m-%d %H:%M:%S'), 1, ticker, history = tradeHist)\n",
    "\n",
    "    \n",
    "    # Get the time and value of stock pre-jump after all 'procedure starting' trades are done (to be used for revert trades)\n",
    "    # Sudden change is generally referred to as Jump here for simplicity\n",
    "    # This is done at end to not enlarge list during iterations\n",
    "\n",
    "    # This is a list comprehension; result (preJumpIndex) is left (i - changePeriod) for output (i, _)\n",
    "    # (i is index num, x/t is value itself) in input variable. Can also add boolean\n",
    "    # condition after (i.e. if tradeHist['Date'] = 01-01-2023)\n",
    "    jumpTime = [x for _, x in enumerate(tradeHist['Date'])] # Note x is in str not timestamp\n",
    "    jumpIndex = [i for i, t in enumerate(df_ticker['Date']) if t.strftime('%Y-%m-%d %H:%M:%S') in jumpTime]\n",
    "    jumpValue = [df_ticker[timingState][i] for i in jumpIndex]\n",
    "    preJumpValue = [df_ticker[timingState][i - changePeriod] for i in jumpIndex]\n",
    "\n",
    "    revertValue = [jumpValue[i] + revertRatio*(preJumpValue[i] - jumpValue[i]) for i in range(0,len(jumpValue))]\n",
    "\n",
    "    # Get (spot - revert) values and check for negative/positive first swap point for each initiation trade and\n",
    "    # make a reversion trade at that point.\n",
    "    # We will put each reversion point of a trade on the initiation trade time in the dataframe\n",
    "    df_dummy['ReversionVal'] = None\n",
    "    df_dummy['ReversionDate'] = None\n",
    "    df_dummy['UnclosedTrade'] = None # To highlight unclosed trades\n",
    "    \n",
    "    for i in range(0,len(jumpIndex)):\n",
    "        # i is out of the number of jumps traded, j is the index (in the dataframe) where the trade is done.\n",
    "        j = jumpIndex[i]\n",
    "\n",
    "        # We use .loc to get j'th point in 'Reversion' column as it takes the variable in memory and not its mirror/copy.\n",
    "        df_dummy.loc[j,('ReversionVal')] = revertValue[i]\n",
    "        # df_dummy['Reversion'][i] is CHAINED INDEXING and won't work because it calls\n",
    "        # df_dummy.__getitem__('Reversion).__setitem__(i) = ... which may not be applied to df_dummy location\n",
    "        # in memory layout (as getitem) and be thrown out immediately. But, .loc dodges this by having __setitem__ only.\n",
    "        # Note: this wouldnt be an issue if the chained indexing happened on the other side (unless doing assignment?).\n",
    "\n",
    "        # The spot - reversion values (i'th value happens at index j within dataframe)\n",
    "        df_dummy['DistToRev'] = None\n",
    "        df_dummy['DistToRev'] = df_dummy['Close'][j:] - revertValue[i]\n",
    "        \n",
    "    \n",
    "        # Distance (of value at first trade) to reversion point is positive if jump (as immediately greater than\n",
    "        # reversion point), and negative if drop (immediately below reversion point).\n",
    "        # Thus, depending on first value we know what if buy or sell first, then select if WhenPositive or WhenNegative\n",
    "        if df_dummy['DistToRev'][j] > 0:\n",
    "            # Buy back now\n",
    "            # To scan and find the first point distance-to-reversion value changes sign (becomes negative)\n",
    "            scanRange = range( j, len(df_dummy['DistToRev']) )\n",
    "            reverseTrade = WhenNegative(df_dummy.iloc[scanRange],'DistToRev','Date')\n",
    "            if len(reverseTrade) != 0:\n",
    "                [makeTrade, tradeHist] = MakeTrade(reverseTrade[0].strftime('%Y-%m-%d %H:%M:%S'), 1, ticker, history = tradeHist)\n",
    "            else:\n",
    "                print('Unclosed short detected at time: ' + df_dummy['Date'][j].strftime('%Y-%m-%d %H:%M:%S'))\n",
    "                print('Unclosed trades can cause significant losses! Initial trade elected to be cancelled.')\n",
    "                df_dummy.loc[j,('UnclosedTrade')] = df_dummy['Close'][j]\n",
    "                [makeTrade, tradeHist] = MakeTrade(changeTimings[0][i].strftime('%Y-%m-%d %H:%M:%S'), 1, ticker, history = tradeHist)\n",
    "\n",
    "\n",
    "        elif df_dummy['DistToRev'][j] < 0:\n",
    "            # Sell back now\n",
    "            # To scan and find the first point distance-to-reversion value changes sign (becomes positive)\n",
    "            scanRange = range( j, len(df_dummy['DistToRev']) )\n",
    "            reverseTrade = WhenPositive(df_dummy.loc[scanRange],'DistToRev','Date')\n",
    "            if len(reverseTrade) != 0:\n",
    "                [makeTrade, tradeHist] = MakeTrade(reverseTrade[0].strftime('%Y-%m-%d %H:%M:%S'), 1, ticker, history = tradeHist, buy = False)\n",
    "            else:\n",
    "                print('Unclosed long detected at time: ' + df_dummy['Date'][j].strftime('%Y-%m-%d %H:%M:%S'))\n",
    "                print('Unclosed trades can cause significant losses! Initial trade elected to be cancelled.')\n",
    "                df_dummy.iloc[j,('UnclosedTrade')] = df_dummy['Close'][j]\n",
    "                [makeTrade, tradeHist] = MakeTrade(changeTimings[0][i].strftime('%Y-%m-%d %H:%M:%S'), 1, ticker, history = tradeHist, buy = False)\n",
    "\n",
    "        else:\n",
    "            raise ValueError('The immediate distance of price to reversion trade point should not be 0 or NaN.')\n",
    "\n",
    "    # Display graphical data of the model (when trades were done, size of trades). (Also try to display the reversion datasets.)\n",
    "    print(df_dummy.to_string())\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(df_dummy['Date'],df_dummy['ReversionVal'])\n",
    "    plt.plot(df_dummy['Date'],df_dummy['Close'])\n",
    "    plt.plot(df_dummy['Date'],df_dummy['UnclosedTrade'])\n",
    "    \n",
    "\n",
    "    eval = EvaluateModel(tradeHist, df_dummy, depth = 'AddEval', debug = 'Extended')\n",
    "\n",
    "    # AFTER THIS CREATE A MODEL CLASS AND USE IT TO CREATE A CUSTOM MODEL TYPE VARIABLE WITH CALLABLE OUTPUT VALUES\n",
    "    # AND CUSTOMISABLE INPUTS\n",
    "    \n",
    "    return tradeHist\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6fb804-56fa-498b-8ab1-75ccafdd95cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = SuddenChangeTradeModel('NVDA', 0.1, 10, 0.75, 2)\n",
    "\n",
    "#data = SMIFlipTradeModel('NVDA', 3, 10, 5)\n",
    "\n",
    "\n",
    "###\n",
    "###\n",
    "# The code seems to work as intended but I will check again, but the method seems to not work. There could be at least 3-5 reasons.\n",
    "# 1) Selling the buying (and vice versa) does not work if the stock trajectory long-term is upward as there will be cases where you cannot sell again\n",
    "# (and when forced to close position, are at a loss).\n",
    "# 2) Linked to 1) the periodicity of the model application means if applied at the wrong timeframe (granularity) i.e. each datapoint is day and\n",
    "# not hour or 15 mins, it causes loss as the trajectory of the model is more refined and there will be more dips and peaks.\n",
    "# 3) The code should be also tested for models of different trajectories (long-term, not related to granularity), as perhaps that influences the\n",
    "# outcome more than the effect of granularity.\n",
    "# 4) A general loss-stop missing, maybe the biggest losses are due to a lack of loss-stop method and a bleed in the earlier trades.\n",
    "# 5) Perhaps the method itself is statistically bad/incorrect (i.e. when a jump happens the stock is likelier than not to keep going up and not\n",
    "# reverting)\n",
    "###\n",
    "# Note that removing unclosed trades fixes all issues as hypothesized. Max. profit margin is approx. jumpRatio*reversionRatio. But issue is we cannot\n",
    "# know in advance if there will be unclosed trades unless we know the trajectory of the stock or if we use a stop-loss\n",
    "# need to record if trade is incomplete, add statistical modelling as well to assess shortfall?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ace4030-e4db-41a8-982a-97e04e65bb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "## THIS CODE IS SHELVED, NO NEED TO USE (CODE DISABLED, CAN REENABLE WITH THE BOOL SETTINGS BELOW)\n",
    "\n",
    "\n",
    "## Here we scrape data from websites or from yahoo finance api\n",
    "\n",
    "# Have them here at the end as they are not being used currently but may have some use\n",
    "# Examples of future use are: sentiment analysis, or low temporal resolution results\n",
    "# Also raw code so I do not need to immediately go through using data manifest class\n",
    "\n",
    "#############\n",
    "# User settings (to avoid looking for lines and changing manually)\n",
    "# Enable various scraping mechanisms\n",
    "enableSelenium = False\n",
    "enableyFinance = False\n",
    "enableTVS = False\n",
    "enableAlphavantage = False\n",
    "\n",
    "# Quit chrome after selenium use complete\n",
    "chromeQuit = True\n",
    "\n",
    "# Export data to a file?\n",
    "export = True\n",
    "\n",
    "###########\n",
    "# Taking direct data using Alphavantage's API of intraday and daily values\n",
    "if enableAlphavantage:\n",
    "    # Alphavantage API key (intraday upto a month length each time, limited daily request, free), no scraping\n",
    "    # Something like '69SFCX93J1H8V9K0'\n",
    "\n",
    "    # Documentation for API here: https://www.alphavantage.co/documentation/\n",
    "\n",
    "    ### Extract data manifest from folder and ignore existing datasets in API request\n",
    "    # Load manifest file (if exists, if not, create empty file)\n",
    "    try:\n",
    "        dataManifestFile = open(r'StockHistData\\dataManifest.txt',\"r\")\n",
    "    except FileNotFoundError:\n",
    "        dataManifestFile = open(r'StockHistData\\dataManifest.txt',\"w\")\n",
    "\n",
    "    # EXTRACT JSON AND \n",
    "    #textdata = dataManifestFile.read()\n",
    "\n",
    "    \n",
    "    #print(textdata) # NEED TO HAVE CODE TO CONVERT OUTPUT STRAIGHT INTO A DF OR\n",
    "    dataManifestFile.close()\n",
    "    \n",
    "    # Here we scrape past intraday stock data (not interested in testing current prices as can test it later by making it past :D )\n",
    "    dataManifestFile = open(r'StockHistData\\dataManifest.txt',\"a\")\n",
    "    \n",
    "    for symbol in dltickers:\n",
    "        for month in dlmonths:\n",
    "            for interval in dlintervals:\n",
    "                # Check data file here\n",
    "                ## NEED TO CHECK FILE DATA HERE, IF NO FILE, NEED TO CATCH NOFILEERROR AND CREATE NEW ONE AND BLANK MIDF\n",
    "                # READ FILE, IF DOESN'T EXIST, DOWNLOAD DATA AND SAVE AND ADD TO MANIFEST\n",
    "                \n",
    "                alphaURL = rf\"https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY&symbol={symbol}&interval={interval}min&month={month}&outputsize=full&apikey={alphaAPIkey} \"\n",
    "                print(alphaURL)\n",
    "                r = requests.get(alphaURL)\n",
    "                data = r.json()\n",
    "\n",
    "                # We add this dataset to the manifest, and save the data itself\n",
    "                #dataManifestFile.write()\n",
    "    \n",
    "    dataManifestFile.close()\n",
    "                \n",
    "    scrapeDF = pd.DataFrame.from_dict(data, orient='columns')\n",
    "    print(scrapeDF)\n",
    "    # Check if dataset exists before making request (to avoid wasting limited daily calls)\n",
    "\n",
    "    \n",
    "    #dataManifestFile.close()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "##########\n",
    "# Selenium scraper to get intra-day prices (not completed so doesn't get intraday prices)\n",
    "# May need higher level expertise to extract data from jscript objects \n",
    "if enableSelenium:\n",
    "    DRIVERPATH = r\"D:\\Finance Study\\chromedriver-win64\\chromedriver-win64\\ \"\n",
    "    \n",
    "    # Set Chrome options\n",
    "    options = Options()\n",
    "    options.headless = False #True # Enable headless mode (no GUI)\n",
    "    options.add_argument(\"--window-size=1920,1200\")  # Set the window size\n",
    "    \n",
    "    \n",
    "    # Init Chrome driver (I guess it's a semi-manual task?)\n",
    "    driver = webdriver.Chrome()#executable_path = DRIVERPATH)\n",
    "    \n",
    "    # Navigate to the desired page\n",
    "    for url in yUrls:\n",
    "        print(\"==================\")\n",
    "        driver.get(r''+url)\n",
    "        time.sleep(5)\n",
    "    \n",
    "    \n",
    "    # Testing here (to develop interaction code here)\n",
    "    #print(driver.page_source)\n",
    "    \n",
    "    \n",
    "    # Good practice to quit when done\n",
    "    if chromeQuit: driver.quit()\n",
    "\n",
    "##########\n",
    "# yFinance to scrape Yahoo Finance\n",
    "if enableyFinance:\n",
    "    # yFinance (Yahoo Finance Historical Data (daily))\n",
    "    # Ticker object array\n",
    "    tickObjArr = [yf.Ticker(ticker) for ticker in dltickers]\n",
    "    \n",
    "    # Fetch historical data\n",
    "    tframe = \"5d\"#\"1mo\"#\"1y\"\n",
    "    histData = [tickObj.history(period = tframe) for tickObj in tickObjArr]\n",
    "    for i in range(len(histData)):\n",
    "        print(\"Historical data for \" + tickObjArr[i].ticker + \":\")\n",
    "        print(histData[i])\n",
    "    \n",
    "    \n",
    "    # Fetch basic financial data\n",
    "    finData = [tickObj.financials for tickObj in tickObjArr]\n",
    "    for i in range(len(finData)):\n",
    "        print(\"Basic Financial data for \" + tickObjArr[i].ticker + \":\")\n",
    "        print(finData[i])\n",
    "    \n",
    "    # Fetch stock actions like dividends and splits\n",
    "    actionData = [tickObj.actions for tickObj in tickObjArr]\n",
    "    for i in range(len(actionData)):\n",
    "        print(\"\\nStock Actions for \" + tickObjArr[i].ticker +  \":\")\n",
    "        print(actionData[i])\n",
    "    \n",
    "    # Using soup\n",
    "    yUrls = [ f'https://finance.yahoo.com/quote/{ticker}/' for ticker in dltickers\n",
    "    ]\n",
    "    #print(urls)\n",
    "    r = requests.get(url=yUrls[0], headers=user_header)\n",
    "    #print(r.content)\n",
    "    \n",
    "    soup = BeautifulSoup(r.content, 'html5lib') # If this line causes an error, run 'pip install html5lib' or install html5lib\n",
    "    #print(soup.prettify())\n",
    "    #table = soup.find('div',)\n",
    "    \n",
    "    company = soup.find('h1', {'class': 'yf-xxbei9'}).text\n",
    "    #print(company)\n",
    "    closePrice = soup.find('div', {'class': 'stx-btn-panel stx-show'})\n",
    "    print(closePrice)\n",
    "    closePrice = soup.find('span', {'class': 'stx-ico-close'})\n",
    "    print(closePrice)\n",
    "\n",
    "\n",
    "\n",
    "##########\n",
    "# Tradingview Scraper\n",
    "if enableTVS:\n",
    "    \n",
    "    # Ideas scraper\n",
    "    # Ideas are the tab in the webpage with articles of sorts\n",
    "    # Initialize the Ideas scraper with default parameters\n",
    "    \n",
    "    # Default: export_result=False, export_type='json'\n",
    "    ideas_scraper = Ideas(\n",
    "      export_result=True,  # Set to True to save the results\n",
    "      export_type='csv'    # Specify the export type (json or csv)\n",
    "    )\n",
    "    \n",
    "    # Default symbol: 'BTCUSD'\n",
    "    # Scrape ideas for the NVDA symbol, from page 1 to page 1\n",
    "    ideas = ideas_scraper.scrape(\n",
    "      symbol=\"NVDA\",\n",
    "      startPage=1,\n",
    "      endPage=1,\n",
    "      sort=\"popular\"  #  Could be 'popular' or 'recent'\n",
    "    )\n",
    "    \n",
    "    #print(\"Ideas:\", ideas)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##########\n",
    "    # Indicators\n",
    "    from tradingview_scraper.symbols.technicals import Indicators\n",
    "    \n",
    "    # Scrape all indicators for the BTCUSD symbol\n",
    "    indicators_scraper = Indicators(export_result=True, export_type='json')\n",
    "    indicators = indicators_scraper.scrape(\n",
    "        symbol=\"BTCUSD\",\n",
    "        timeframe=\"4h\",\n",
    "        allIndicators=True\n",
    "    )\n",
    "    #print(\"All Indicators:\", indicators)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
