{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc6fdd67-383f-4bc2-bbca-2bfb8e819e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile __init__.py\n",
    "###### -- Python Script in Jupyter to access, create and test financial data and models\n",
    "###### -- By Ahmed Asiliskender, initial write date 25 June 2024\n",
    "###### -- May also access MATLAB scripts through here and .py files.\n",
    "\n",
    "### Here we initialise important libraries and variables.\n",
    "\n",
    "## To download packages using pip\n",
    "import sys #! allows to use command terminal code in here\n",
    "#!{sys.executable} --version\n",
    "#!pip install html5lib\n",
    "#!pip install bs4\n",
    "#!pip install yfinance\n",
    "#!pip install tradingview-scraper\n",
    "#!pip install --upgrade --no-cache tradingview-scraper\n",
    "#!pip install selenium\n",
    "#!pip install sqlalchemy\n",
    "#!pip install python-dotenv\n",
    "#!pip install pandas-ta\n",
    "#!pip install pytest\n",
    "#!pip install python-on-whales\n",
    "# Security testing\n",
    "#!pip install bandit\n",
    "\n",
    "# Cmd terminal environment install (psycopg2)\n",
    "#!pip install psycopg2-binary \n",
    "# Conda environment install\n",
    "#!conda install -c anaconda psycopg2 \n",
    "\n",
    "\n",
    "\n",
    "# Import pandas (python data analysis lib) and data analysis packages\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "#user_header = {'user-agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) \\\n",
    "#                                AppleWebKit/537.36 (KHTML, like Gecko) \\\n",
    "#                                Chrome/122.0.0.0 Safari/537.36'}\n",
    "\n",
    "# Webscrape libs\n",
    "from bs4 import BeautifulSoup\n",
    "import yfinance as yf\n",
    "import tradingview_scraper as tvs\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from tradingview_scraper.symbols.ideas import Ideas\n",
    "\n",
    "# Other libs (system, graphical or time-compute analysis)\n",
    "import os\n",
    "from io import StringIO\n",
    "import psycopg2\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from dotenv.main import set_key\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy import types as sqltype\n",
    "import sqlalchemy.exc as sqlexc\n",
    "from colorama import Fore, Back, Style\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import time\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from ast import literal_eval\n",
    "import json\n",
    "import unittest\n",
    "import pytest\n",
    "\n",
    "#import warnings\n",
    "\n",
    "\n",
    "# My libs\n",
    "#from arcanequant.quantlib import *\n",
    "\n",
    "from arcanequant.quantlib import DataManifest\n",
    "from arcanequant.quantlib import DownloadIntraday # NEED TO MOVE THIS TO DATA MANAGER\n",
    "from arcanequant.quantlib import SetKeysQuery, DropKeysQuery, ExecuteSQL\n",
    "\n",
    "# Paid APIs, (not used, left here)\n",
    "\n",
    "# Bloomberg (not free)\n",
    "#!pip install blpapi --index-url=https://blpapi.bloomberg.com/repository/releases/python/simple/ blpapi\n",
    "#!pip install xbbg\n",
    "#from xbbg import blp\n",
    "#blp.bdh( tickers='SPX Index', flds=['High', 'Low', 'Last_Price'], start_date='2018-10-10', end_date='2018-10-20')\n",
    "#blp.bdp('AAPL US Equity', 'Eqy_Weighted_Avg_Px', VWAP_Dt='20181224')\n",
    "#blp.bdp(tickers='NVDA US Equity', flds=['Security_Name', 'GICS_Sector_Name'])\n",
    "\n",
    "\n",
    "### RAPID API FOR INTRADAYS (not free)\n",
    "#import http.client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a8de56-48a1-4076-a32d-4502bacb6379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77f18e65-bb22-43e3-b8da-4b6e68aff79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#conn = http.client.HTTPSConnection(\"yahoo-finance127.p.rapidapi.com\")\n",
    "\n",
    "#headers = {\n",
    "#    'x-rapidapi-key': \"2e7bf1e71cmsh8f7a5babc8f5197p1f02bejsn72107f046a86\",\n",
    "#    'x-rapidapi-host': \"yahoo-finance127.p.rapidapi.com\"\n",
    "#}\n",
    "\n",
    "#conn.request(\"GET\", \"/finance-analytics/nvda\", headers=headers)\n",
    "\n",
    "#res = conn.getresponse()\n",
    "#data = res.read()\n",
    "\n",
    "#print(data.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f16ac43-8712-42fd-8907-04f335f8ef4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main\n"
     ]
    }
   ],
   "source": [
    "#%%writefile main.py\n",
    "\n",
    "def main():\n",
    "    print('main')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f6a8f6-0f27-450c-b312-8ae6e689abde",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4cf7b04-0dd2-44d7-925f-746fa44e2c6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Manifest Initialised\n",
      "Loading Manifest Data\n",
      "Load path/name: data/StockHistData/dataManifest.json\n",
      "Connecting to engine: Engine(postgresql+psycopg2://postgres:***@localhost:5432/marketdata)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "DownloadIntraday() got multiple values for argument 'saveMode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m checkManifest\u001b[38;5;241m.\u001b[39mconnectSQL(SQLloginfilename)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m##### Sourcing market data\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m DownloadIntraday(savepath, dltickers, dlintervals, dlmonths, alphaAPIkey, saveMode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboth\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: DownloadIntraday() got multiple values for argument 'saveMode'"
     ]
    }
   ],
   "source": [
    "## Here we create settings for the database building/development:\n",
    "# - Tickers to download\n",
    "# - Months of intraday data to request\n",
    "# - Time intervals (granularity/resolution)\n",
    "# - API request key (taken from file)\n",
    "\n",
    "savepath = r'data/StockHistData/'\n",
    "\n",
    "# Tickers to request, make sure it is the correct one (there is a search API call to check)\n",
    "dltickers = [\"GOOG\"]\n",
    "\n",
    "# Months to request (string, \"year-month\") i.e. 2020-02\n",
    "dlmonths = [\"2025-06\"] # Need to add functionality to take whole years\n",
    "\n",
    "# Options for time resolution: 1, 5, 15, 30, 60\n",
    "dlintervals = [1]\n",
    "\n",
    "##### Alphavantage API key for data acquisition\n",
    "# .env file method\n",
    "env_path = Path(\".\") / \"APIkey.env\" # Environment variables file must be same folder as this code (#REGEXSEARCHFOR API IF NO FILE)\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "alphaAPIkey = os.getenv(\"ALPHA_API_KEY\") #MAYBE TRY .TXT IF NO .ENV\n",
    "# .txt file method\n",
    "#with open('APIkey.txt',) as keyfile:\n",
    "#    alphaAPIkey = keyfile.read()\n",
    "\n",
    "\n",
    "##### Setting environment variables for the SQL login details\n",
    "SQLloginfilename = \"SQLlogin\" # Default filename \"SQLlogin\" \n",
    "#SQLdetails_path = Path(\".\") / f\"{SQLloginfilename}.env\"\n",
    "#set_key(SQLdetails_path, 'DRIVER', 'hello') #dotenv.set_key\n",
    "#set_key(SQLdetails_path, 'DIALECT', 'mate')\n",
    "#set_key(SQLdetails_path, 'ENV_USER', 'put')\n",
    "#set_key(SQLdetails_path, 'PASSWORD', 'your')\n",
    "#set_key(SQLdetails_path, 'HOST_MACHINE', 'own')\n",
    "#set_key(SQLdetails_path, 'PORT', 'stuff')\n",
    "#set_key(SQLdetails_path, 'DBNAME', 'here')\n",
    "\n",
    "\n",
    "checkManifest = DataManifest()\n",
    "checkManifest.loadManifest(loadFrom = 'direct', path = savepath)\n",
    "checkManifest.connectSQL(SQLloginfilename)\n",
    "\n",
    "##### Sourcing market data\n",
    "DownloadIntraday(savepath, dltickers, dlintervals, dlmonths, alphaAPIkey, saveMode='both', verbose=True) # TODO: ADD SAVING TO BOTH STORAGE\n",
    "\n",
    "#checkManifest = DataManifest()\n",
    "#checkManifest.loadManifest(path = savepath)\n",
    "#checkManifest.connectSQL(SQLloginfilename)\n",
    "\n",
    "#checkManifest.validateManifest(fastValidate = False, show = False)\n",
    "#print(checkManifest.DF) # Maybe I can add method to show manifest in a more compact form\n",
    "#checkManifest.saveManifest(saveTo='direct',savePath=checkManifest.directory)\n",
    "\n",
    "# ADD CODE TO CONVERT SQL TABLE TO THE DF FORM (MULTIINDEX)c\n",
    "\n",
    "# PSEUDOCODE FOR MANIFEST:\n",
    "# DROP MANIFESTID\n",
    "# MAKE MULTIINDEX ONCE AGAIN FROM STOCKS, INTERVAL COLUMNS\n",
    "# RENAME COLUMN NAME TO MONTH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fdb622-1758-4fbb-9354-9ed17cfbcdbd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pytest\n",
    "from python_on_whales import docker\n",
    "import time\n",
    "\n",
    "\n",
    "# Run a container\n",
    "#docker.run(\"hello-world\")\n",
    "\n",
    "# Pull an image\n",
    "#docker.pull(\"postgres:15\")\n",
    "\n",
    "\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def test_postgres():\n",
    "    container = docker.run(\n",
    "        \"postgres:15\",\n",
    "        detach=True,\n",
    "        name=\"test_pg\",\n",
    "        envs={\"POSTGRES_PASSWORD\": \"test\", \"POSTGRES_USER\": \"test\", \"POSTGRES_DB\": \"testdb\"},\n",
    "        publish=[\"5433:5432\"]\n",
    "    )\n",
    "    time.sleep(3)  # wait for the DB to boot\n",
    "\n",
    "    yield \"postgresql://test:test@localhost:5433/testdb\"\n",
    "\n",
    "    docker.container.remove(\"test_pg\", force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c77da0-37fc-4441-8a12-00d6acf59eef",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#%%writefile DockerInstallWrapper.py\n",
    "# Docker Installation Process\n",
    "import os\n",
    "\n",
    "def install_docker():\n",
    "    import subprocess\n",
    "    import shutil\n",
    "\n",
    "    docker_path = shutil.which(\"docker\")\n",
    "    if docker_path:\n",
    "        print(f\"Docker already installed, located at: {docker_path}\")\n",
    "        return\n",
    "    else:\n",
    "        print(\"Docker not found in PATH\")\n",
    "\n",
    "    # Windows installation procedure\n",
    "    if os.name == \"nt\":\n",
    "        dockerInstaller_path = shutil.which(\"dockerinstaller\")    \n",
    "        if dockerInstaller_path:\n",
    "            print(f\"Docker installer found at: {dockerInstaller_path}\")\n",
    "        else:\n",
    "            print(\"Downloading docker installer...\")\n",
    "            subprocess.run([\"powershell\", \"-Command\", \"Invoke-WebRequest -Uri 'https://desktop.docker.com/win/main/amd64/Docker Desktop Installer.exe' -OutFile 'DockerInstaller.exe'\"], shell=True)\n",
    "        \n",
    "        quietChoice = input(\"Do you want to install quietly (without GUI, automatically)? (y/n): \").strip().lower()\n",
    "        if quietChoice == 'y':\n",
    "            print(\"Installing docker quietly...\")\n",
    "            subprocess.run([\"powershell\", \"-Command\", 'Start-Process -FilePath \"DockerInstaller.exe\" -ArgumentList \"install --quiet\" -Verb RunAs -Wait'], shell=True)\n",
    "        else:\n",
    "            print(\"Installing docker...\")\n",
    "            subprocess.run([\"powershell\", \"-Command\", 'Start-Process -FilePath \"DockerInstaller.exe\" -ArgumentList \"install\" -Verb RunAs -Wait'], shell=True)\n",
    "\n",
    "    # Linux/macOS installation procedure\n",
    "    elif os.name == \"posix\":\n",
    "        print(\"Installing docker...\")\n",
    "        subprocess.run(\"curl -fsSL https://get.docker.com | sh\", shell=True)\n",
    "        subprocess.run(\"sudo usermod -aG docker $USER\", shell=True)\n",
    "\n",
    "    prompt_restart()\n",
    "\n",
    "def prompt_restart():\n",
    "    choice = input(\"Docker installation complete. Do you want to restart now? (y/n): \").strip().lower()\n",
    "    if choice == 'y':\n",
    "        force_restart()\n",
    "    else:\n",
    "        print(\"Restart skipped. You may need to restart manually for changes to take full effect.\")\n",
    "\n",
    "def force_restart():\n",
    "    print(\"Restarting device.\")\n",
    "    if os.name == \"nt\":  # Windows\n",
    "        subprocess.run([\"powershell\", \"Restart-Computer -Force\"], shell=True)\n",
    "    elif os.name == \"posix\":  # Linux/macOS\n",
    "        subprocess.run(\"sudo shutdown -r now\", shell=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    installChoice = input(\"Do you want to install Docker to gain access to SQL functionality? (y/n): \").strip().lower()\n",
    "    \n",
    "    if installChoice == 'y':\n",
    "        install_docker()\n",
    "    else:\n",
    "        print(\"Install skipped. You need to install docker to be able to use SQL functionality.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a37bb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================= CSV FORM =========================\n",
      "                                                           Meta Data\n",
      "1. Information     Intraday (1min) open, high, low, close prices ...\n",
      "2. Symbol                                                       MSFT\n",
      "3. Last Refreshed                                2022-01-31 19:58:00\n",
      "4. Interval                                                     1min\n",
      "5. Output Size                                             Full size\n",
      "6. Time Zone                                              US/Eastern\n",
      "========================= INPUTTABLE SQL FORM =========================\n",
      "                                              1. Information Ticker  \\\n",
      "Meta Data  Intraday (1min) open, high, low, close prices ...   MSFT   \n",
      "\n",
      "                      DateTime Interval 5. Output Size 6. Time Zone 7. Month  \n",
      "Meta Data  2022-01-31 19:58:00        1      Full size   US/Eastern  2022-01  \n",
      "========================= REVERTED =========================\n",
      "                                                           Meta Data\n",
      "1. Information     Intraday (1min) open, high, low, close prices ...\n",
      "2. Symbol                                                       MSFT\n",
      "3. Last Refreshed                                2022-01-31 19:58:00\n",
      "4. Interval                                                0    1min\n",
      "5. Output Size                                             Full size\n",
      "6. Time Zone                                              US/Eastern\n",
      "index                                                      Meta Data\n"
     ]
    }
   ],
   "source": [
    "# TEST SQL SYNC\n",
    "# TEST SQL SAVE\n",
    "import pandas as pd\n",
    "### INITIAL STATE\n",
    "x = pd.read_csv(r\"C:\\Users\\a_asi\\FinanceProject\\ArcaneQuant\\data\\StockHistData\\MSFT\\MSFT_1_2022-01_meta.csv\", index_col = 0) #index_col = 0, \n",
    "print('========================= CSV FORM =========================')\n",
    "print(x)\n",
    "\n",
    "### CONVERTING TO SQL FORM\n",
    "y=x\n",
    "y.rename(index={'2. Symbol':'Ticker', '3. Last Refreshed':'DateTime','4. Interval':'Interval'},inplace=True)\n",
    "y.loc['Interval'] = 1\n",
    "\n",
    "\n",
    "\n",
    "y.loc['7. Month'] = '2022-01'\n",
    "y = y.T\n",
    "print('========================= INPUTTABLE SQL FORM =========================')\n",
    "# This gets put into SQLSave, but I have to add code to SQLSave that if saving to metaTable, convert name of Ticker, Interval to 2. Symbol, 4. Interval\n",
    "# , and also create a view combining DateID/TimeID and call it 3. Last Refreshed. This way, when loading, just call the view with the month, ticker, interval filter, and drop month column to get original .csv format\n",
    "print(y) \n",
    "### NOTE: DONT NEED TO MAKE MULTIINDEX, \n",
    "\n",
    "\n",
    "# CHANGING SQL ACCEPTABLE TO META\n",
    "xx = y\n",
    "yy = xx.reset_index()\n",
    "\n",
    "# CHANGE ROW TO META DATA AND TRANSPOSE\n",
    "zz = yy\n",
    "zz.drop(columns=['7. Month'], inplace=True)\n",
    "zz = zz.T\n",
    "zz.loc['Interval'] = zz.loc['Interval'].to_string()+'min'\n",
    "zz.rename(index={'Ticker':'2. Symbol', 'DateTime':'3. Last Refreshed','Interval':'4. Interval'},inplace=True)\n",
    "zz.rename(columns={0:\"Meta Data\"},inplace=True)\n",
    "print('========================= REVERTED =========================')\n",
    "print(zz.sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbcae28-02a3-4ddd-bec3-e1c9b65d20f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bacd1cf1-236e-44db-84c7-385f1d7b27e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataManifest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m savepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/StockHistData/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      5\u001b[0m SQLloginfilename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSQLlogin\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# Default filename \"SQLlogin\" \u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m checkManifest \u001b[38;5;241m=\u001b[39m DataManifest()\n\u001b[0;32m      8\u001b[0m checkManifest\u001b[38;5;241m.\u001b[39mloadManifest(loadFrom \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdirect\u001b[39m\u001b[38;5;124m'\u001b[39m, path \u001b[38;5;241m=\u001b[39m savepath)\n\u001b[0;32m      9\u001b[0m checkManifest\u001b[38;5;241m.\u001b[39mvalidateManifest()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DataManifest' is not defined"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "#from quantlib import SQLSave\n",
    "# Testing manifest saving\n",
    "savepath = r'data/StockHistData/'\n",
    "SQLloginfilename = \"SQLlogin\" # Default filename \"SQLlogin\" \n",
    "\n",
    "checkManifest = DataManifest()\n",
    "checkManifest.loadManifest(loadFrom = 'direct', path = savepath)\n",
    "checkManifest.validateManifest()\n",
    "checkManifest.saveManifest()\n",
    "#checkManifest.connectSQL(SQLloginfilename)\n",
    "# Saving market data from csv needs tagging of Ticker and Interval\n",
    "df1 = checkManifest.loadData_fromcsv('KO',5,'2024-04')\n",
    "df1['Ticker'] = 'KO'\n",
    "df1['Interval'] = 5\n",
    "\n",
    "SQLSave(df1, checkManifest.SQLengine, 'marketTable', echo = True)\n",
    "\n",
    "\n",
    "#SQLEstablish(checkManifest.SQLengine)\n",
    "#SQLRepair(dataManifest = checkManifest)\n",
    "\n",
    "t0 = time()\n",
    "# Testing manifest loading\n",
    "print('manifest/DM')\n",
    "zz = ExtractData(checkManifest,  'manifest',  start = 'all', end = '2022-11', fromSQL = False, condition = None)\n",
    "print(zz)\n",
    "t1 = time()\n",
    "print(f\"Total time elapsed : {t1-t0} seconds\")\n",
    "\n",
    "print('manifest/SQL')\n",
    "zz2 = ExtractData(checkManifest, 'manifest',  start = '2000-01', end = '2000-05', fromSQL = True, condition = None)\n",
    "print(zz2)\n",
    "t2 = time()\n",
    "print(f\"Total time elapsed : {t2-t1} seconds\")\n",
    "\n",
    "print('market/DM')\n",
    "# Testing stock data loading\n",
    "xx = ExtractData(checkManifest, 'market',  start = 'all', end = '2022-11', fromSQL = False, condition = None)\n",
    "print(xx)\n",
    "t3 = time()\n",
    "print(f\"Total time elapsed : {t3-t2} seconds\")\n",
    "\n",
    "print('manifest/SQL')\n",
    "yy = ExtractData(checkManifest, 'market',  start = 'all', end = '2022-11', fromSQL = True, condition = None)\n",
    "print(yy)\n",
    "t4 = time()\n",
    "print(f\"Total time elapsed : {t4-t3} seconds\")\n",
    "\n",
    "# Testing stock data saving\n",
    "#SQLSave(xx, checkManifest.SQLengine, 'marketTable', echo = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bb47b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Manifest Initialised\n",
      "Connecting to engine: Engine(postgresql+psycopg2://postgres:***@localhost:5432/marketdata)\n",
      "Loading Manifest Data\n",
      "Load path/name: data/StockHistData/dataManifest.json\n",
      "load csv\n",
      "Loading data file: KO_15_2022-01.csv\n",
      "Loading meta data file: KO_15_2022-01_meta.csv\n",
      "load sql\n",
      "Extracting data from SQL database\n",
      "Direct Test: True, True (meta)\n",
      "load DF/SQLDF\n",
      "Loading Manifest Data\n",
      "Load path/name: data/StockHistData/dataManifest.json\n",
      "load metaDF/metaSQLDF\n",
      "Loading Manifest Data\n",
      "Load path/name: data/StockHistData/dataManifest.json\n",
      "Inputted DataFrame has no ticker, interval and/or month column. From .csv or already processed. Fixing index and returning input.\n",
      "ExtractDF Test: True, True (meta)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\a_asi\\FinanceProject\\ArcaneQuant\\arcanequant\\quantlib\\DataManager.py:482: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  return df[condition(df)]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from arcanequant.quantlib import *\n",
    "\n",
    "savepath = r'data/StockHistData/'\n",
    "SQLloginfilename = \"SQLlogin\" # Default filename \"SQLlogin\" \n",
    "\n",
    "checkManifest = DataManifest()\n",
    "checkManifest.connectSQL(SQLloginfilename)\n",
    "checkManifest.loadManifest(loadFrom='direct', path = savepath)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b02cec72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month            2022-01  2022-02  2022-03  2022-04  2022-05  2022-06  \\\n",
      "Ticker Interval                                                         \n",
      "ARM    1               0        0        0        0        0        0   \n",
      "       5               0        0        0        0        0        0   \n",
      "       15              0        0        0        0        0        0   \n",
      "       30              0        0        0        0        0        0   \n",
      "       60              0        0        0        0        0        0   \n",
      "\n",
      "Month            2022-07  2022-08  2022-09  2022-10  ...  2024-09  2024-10  \\\n",
      "Ticker Interval                                      ...                     \n",
      "ARM    1               0        0        0        0  ...        1        1   \n",
      "       5               0        0        0        0  ...        1        1   \n",
      "       15              0        0        0        0  ...        1        1   \n",
      "       30              0        0        0        0  ...        1        1   \n",
      "       60              0        0        0        0  ...        1        1   \n",
      "\n",
      "Month            2024-11  2024-12  2025-01  2025-02  2025-03  2025-04  \\\n",
      "Ticker Interval                                                         \n",
      "ARM    1               1        1        1        0        0        0   \n",
      "       5               1        1        1        0        0        0   \n",
      "       15              1        1        1        0        0        0   \n",
      "       30              1        1        1        0        0        0   \n",
      "       60              1        1        1        0        0        0   \n",
      "\n",
      "Month            2025-05  2025-06  \n",
      "Ticker Interval                    \n",
      "ARM    1               0        0  \n",
      "       5               0        0  \n",
      "       15              0        0  \n",
      "       30              0        0  \n",
      "       60              0        0  \n",
      "\n",
      "[5 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "print(checkManifest.DF[checkManifest.DF['2022-01']==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8af0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Manifest Initialised\n",
      "Connecting to engine: Engine(postgresql+psycopg2://postgres:***@localhost:5432/marketdata)\n",
      "Loading Manifest Data\n",
      "Loading manifest view from engine: Engine(postgresql+psycopg2://postgres:***@localhost:5432/marketdata)\n",
      "                 2022-01  2022-02  2022-03  2022-04  2022-05  2022-06  \\\n",
      "Ticker Interval                                                         \n",
      "ARM    1               0        0        0        0        0        0   \n",
      "       5               0        0        0        0        0        0   \n",
      "       15              0        0        0        0        0        0   \n",
      "       30              0        0        0        0        0        0   \n",
      "       60              0        0        0        0        0        0   \n",
      "GOOG   1               1        1        1        1        1        1   \n",
      "       5               1        1        1        1        1        1   \n",
      "       15              1        1        1        1        1        1   \n",
      "       30              1        1        1        1        1        1   \n",
      "       60              1        1        1        1        1        1   \n",
      "KO     1               1        1        1        1        1        1   \n",
      "       5               1        1        1        1        1        1   \n",
      "       15              1        1        1        1        1        1   \n",
      "       30              1        1        1        1        1        1   \n",
      "       60              1        1        1        1        1        1   \n",
      "MSFT   1               1        1        1        1        1        1   \n",
      "       5               1        1        1        1        1        1   \n",
      "       15              1        1        1        1        1        1   \n",
      "       30              1        1        1        1        1        1   \n",
      "       60              1        1        1        1        1        1   \n",
      "NVDA   1               1        1        1        1        1        1   \n",
      "       5               1        1        1        1        1        1   \n",
      "       15              1        1        1        1        1        1   \n",
      "       30              1        1        1        1        1        1   \n",
      "       60              1        1        1        1        1        1   \n",
      "\n",
      "                 2022-07  2022-08  2022-09  2022-10  ...  2024-09  2024-10  \\\n",
      "Ticker Interval                                      ...                     \n",
      "ARM    1               0        0        0        0  ...        1        1   \n",
      "       5               0        0        0        0  ...        1        1   \n",
      "       15              0        0        0        0  ...        1        1   \n",
      "       30              0        0        0        0  ...        1        1   \n",
      "       60              0        0        0        0  ...        1        1   \n",
      "GOOG   1               1        1        1        1  ...        0        0   \n",
      "       5               1        1        1        1  ...        0        0   \n",
      "       15              1        1        1        1  ...        0        0   \n",
      "       30              1        1        1        1  ...        0        0   \n",
      "       60              1        1        1        1  ...        0        0   \n",
      "KO     1               1        1        1        1  ...        1        1   \n",
      "       5               1        1        1        1  ...        1        1   \n",
      "       15              1        1        1        1  ...        1        1   \n",
      "       30              1        1        1        1  ...        1        1   \n",
      "       60              1        1        1        1  ...        1        1   \n",
      "MSFT   1               1        1        1        1  ...        1        1   \n",
      "       5               1        1        1        1  ...        1        1   \n",
      "       15              1        1        1        1  ...        1        1   \n",
      "       30              1        1        1        1  ...        1        1   \n",
      "       60              1        1        1        1  ...        1        1   \n",
      "NVDA   1               1        1        1        1  ...        1        1   \n",
      "       5               1        1        1        1  ...        1        1   \n",
      "       15              1        1        1        1  ...        1        1   \n",
      "       30              1        1        1        1  ...        1        1   \n",
      "       60              1        1        1        1  ...        1        1   \n",
      "\n",
      "                 2024-11  2024-12  2025-01  2025-02  2025-03  2025-04  \\\n",
      "Ticker Interval                                                         \n",
      "ARM    1               1        1        1        0        0        0   \n",
      "       5               1        1        1        0        0        0   \n",
      "       15              1        1        1        0        0        0   \n",
      "       30              1        1        1        0        0        0   \n",
      "       60              1        1        1        0        0        0   \n",
      "GOOG   1               0        0        1        1        1        1   \n",
      "       5               0        0        1        1        1        1   \n",
      "       15              0        0        1        1        1        1   \n",
      "       30              0        0        1        1        1        1   \n",
      "       60              0        0        1        1        1        1   \n",
      "KO     1               1        1        1        0        0        0   \n",
      "       5               1        1        1        0        0        0   \n",
      "       15              1        1        1        0        0        0   \n",
      "       30              1        1        1        0        0        0   \n",
      "       60              1        1        1        0        0        0   \n",
      "MSFT   1               1        1        1        0        0        0   \n",
      "       5               1        1        1        0        0        0   \n",
      "       15              1        1        1        0        0        0   \n",
      "       30              1        1        1        0        0        0   \n",
      "       60              1        1        1        0        0        0   \n",
      "NVDA   1               1        1        1        0        0        0   \n",
      "       5               1        1        1        0        0        0   \n",
      "       15              1        1        1        0        0        0   \n",
      "       30              1        1        1        0        0        0   \n",
      "       60              1        1        1        0        0        0   \n",
      "\n",
      "                 2025-05  2025-06  \n",
      "Ticker Interval                    \n",
      "ARM    1               0        0  \n",
      "       5               0        0  \n",
      "       15              0        0  \n",
      "       30              0        0  \n",
      "       60              0        0  \n",
      "GOOG   1               1        1  \n",
      "       5               1        0  \n",
      "       15              1        0  \n",
      "       30              1        0  \n",
      "       60              1        0  \n",
      "KO     1               0        0  \n",
      "       5               0        0  \n",
      "       15              0        0  \n",
      "       30              0        0  \n",
      "       60              0        0  \n",
      "MSFT   1               0        0  \n",
      "       5               0        0  \n",
      "       15              0        0  \n",
      "       30              0        0  \n",
      "       60              0        0  \n",
      "NVDA   1               0        0  \n",
      "       5               0        0  \n",
      "       15              0        0  \n",
      "       30              0        0  \n",
      "       60              0        0  \n",
      "\n",
      "[25 rows x 42 columns]\n",
      "Loading Manifest Data\n",
      "Load path/name: data/StockHistData/dataManifest.json\n",
      "Month            2022-01  2022-02  2022-03  2022-04  2022-05  2022-06  \\\n",
      "Ticker Interval                                                         \n",
      "ARM    1               0        0        0        0        0        0   \n",
      "       5               0        0        0        0        0        0   \n",
      "       15              0        0        0        0        0        0   \n",
      "       30              0        0        0        0        0        0   \n",
      "       60              0        0        0        0        0        0   \n",
      "GOOG   1               1        1        1        1        1        1   \n",
      "       5               1        1        1        1        1        1   \n",
      "       15              1        1        1        1        1        1   \n",
      "       30              1        1        1        1        1        1   \n",
      "       60              1        1        1        1        1        1   \n",
      "KO     1               1        1        1        1        1        1   \n",
      "       5               1        1        1        1        1        1   \n",
      "       15              1        1        1        1        1        1   \n",
      "       30              1        1        1        1        1        1   \n",
      "       60              1        1        1        1        1        1   \n",
      "MSFT   1               1        1        1        1        1        1   \n",
      "       5               1        1        1        1        1        1   \n",
      "       15              1        1        1        1        1        1   \n",
      "       30              1        1        1        1        1        1   \n",
      "       60              1        1        1        1        1        1   \n",
      "NVDA   1               1        1        1        1        1        1   \n",
      "       5               1        1        1        1        1        1   \n",
      "       15              1        1        1        1        1        1   \n",
      "       30              1        1        1        1        1        1   \n",
      "       60              1        1        1        1        1        1   \n",
      "\n",
      "Month            2022-07  2022-08  2022-09  2022-10  ...  2024-09  2024-10  \\\n",
      "Ticker Interval                                      ...                     \n",
      "ARM    1               0        0        0        0  ...        1        1   \n",
      "       5               0        0        0        0  ...        1        1   \n",
      "       15              0        0        0        0  ...        1        1   \n",
      "       30              0        0        0        0  ...        1        1   \n",
      "       60              0        0        0        0  ...        1        1   \n",
      "GOOG   1               1        1        1        1  ...        0        0   \n",
      "       5               1        1        1        1  ...        0        0   \n",
      "       15              1        1        1        1  ...        0        0   \n",
      "       30              1        1        1        1  ...        0        0   \n",
      "       60              1        1        1        1  ...        0        0   \n",
      "KO     1               1        1        1        1  ...        1        1   \n",
      "       5               1        1        1        1  ...        1        1   \n",
      "       15              1        1        1        1  ...        1        1   \n",
      "       30              1        1        1        1  ...        1        1   \n",
      "       60              1        1        1        1  ...        1        1   \n",
      "MSFT   1               1        1        1        1  ...        1        1   \n",
      "       5               1        1        1        1  ...        1        1   \n",
      "       15              1        1        1        1  ...        1        1   \n",
      "       30              1        1        1        1  ...        1        1   \n",
      "       60              1        1        1        1  ...        1        1   \n",
      "NVDA   1               1        1        1        1  ...        1        1   \n",
      "       5               1        1        1        1  ...        1        1   \n",
      "       15              1        1        1        1  ...        1        1   \n",
      "       30              1        1        1        1  ...        1        1   \n",
      "       60              1        1        1        1  ...        1        1   \n",
      "\n",
      "Month            2024-11  2024-12  2025-01  2025-02  2025-03  2025-04  \\\n",
      "Ticker Interval                                                         \n",
      "ARM    1               1        1        1        0        0        0   \n",
      "       5               1        1        1        0        0        0   \n",
      "       15              1        1        1        0        0        0   \n",
      "       30              1        1        1        0        0        0   \n",
      "       60              1        1        1        0        0        0   \n",
      "GOOG   1               0        0        1        1        1        1   \n",
      "       5               0        0        1        1        1        1   \n",
      "       15              0        0        1        1        1        1   \n",
      "       30              0        0        1        1        1        1   \n",
      "       60              0        0        1        1        1        1   \n",
      "KO     1               1        1        1        0        0        0   \n",
      "       5               1        1        1        0        0        0   \n",
      "       15              1        1        1        0        0        0   \n",
      "       30              1        1        1        0        0        0   \n",
      "       60              1        1        1        0        0        0   \n",
      "MSFT   1               1        1        1        0        0        0   \n",
      "       5               1        1        1        0        0        0   \n",
      "       15              1        1        1        0        0        0   \n",
      "       30              1        1        1        0        0        0   \n",
      "       60              1        1        1        0        0        0   \n",
      "NVDA   1               1        1        1        0        0        0   \n",
      "       5               1        1        1        0        0        0   \n",
      "       15              1        1        1        0        0        0   \n",
      "       30              1        1        1        0        0        0   \n",
      "       60              1        1        1        0        0        0   \n",
      "\n",
      "Month            2025-05  2025-06  \n",
      "Ticker Interval                    \n",
      "ARM    1               0        0  \n",
      "       5               0        0  \n",
      "       15              0        0  \n",
      "       30              0        0  \n",
      "       60              0        0  \n",
      "GOOG   1               1        1  \n",
      "       5               1        0  \n",
      "       15              1        0  \n",
      "       30              1        0  \n",
      "       60              1        0  \n",
      "KO     1               0        0  \n",
      "       5               0        0  \n",
      "       15              0        0  \n",
      "       30              0        0  \n",
      "       60              0        0  \n",
      "MSFT   1               0        0  \n",
      "       5               0        0  \n",
      "       15              0        0  \n",
      "       30              0        0  \n",
      "       60              0        0  \n",
      "NVDA   1               0        0  \n",
      "       5               0        0  \n",
      "       15              0        0  \n",
      "       30              0        0  \n",
      "       60              0        0  \n",
      "\n",
      "[25 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "from arcanequant.quantlib import *\n",
    "\n",
    "\n",
    "# Testing manifest load\n",
    "savepath = r'data/StockHistData/'\n",
    "SQLloginfilename = \"SQLlogin\" # Default filename \"SQLlogin\" \n",
    "\n",
    "checkManifest = DataManifest()\n",
    "checkManifest.connectSQL(SQLloginfilename)\n",
    "checkManifest.loadManifest(loadFrom='database', path = savepath)\n",
    "print(checkManifest.DF)\n",
    "checkManifest.loadManifest(loadFrom='direct', path = savepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1680e275",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da94734",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(checkManifest.DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2734c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we read and post-process the .csv data and do a time-series analysis (rolling mean and std. deviation)\n",
    "# Intention is to figure out a good window size by assessing the variance and if it follows a chi-sq. distribution\n",
    "# The assumption here is the stock values are normally distributed\n",
    "# The issue is the stock mean/variance may change over time, how do I account for this?\n",
    "\n",
    "#### TO DO:\n",
    "# We are supposed to assess the % change of stock values\n",
    "# I will calculate this, and also its variance, I will do this for different resolutions and in a rolling window (I am thinking of sizes, 5, 10, 20, 50, 100)\n",
    "# Should I calculate a global (or yearly or monthly) variance and compare this to the window to see if theres a match for specific sizes (which correspond to same timeframe)?\n",
    "# Once I do this I can really go into the trade models I made previously\n",
    "\n",
    "##### THOUGHTS:\n",
    "# Is there a point in hypothesis testing a 'true' variance or mean for growth stocks? perhaps other than to test for a non-constant mean model?\n",
    "# There is point in hypothesis testing dividend stocks/ETFs (i.e. FTSE 100 Vanguard ETF)\n",
    "# Is it reasonable to model the mean (or exp. stock value) as fct of time in context of a fourier transform\n",
    "# (as many growth+dividend stocks have cyclical behaviour around quarterlies, typically goes up then down later) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_MSFT_test = checkManifest.loadData_fromcsv(\"MSFT\", 15, \"2024-08\")\n",
    "data_MSFT_test.set_index(pd.to_datetime(data_MSFT_test['DateTime']), inplace=True)\n",
    "\n",
    "\n",
    "############\n",
    "############\n",
    "# Modelling prices:\n",
    "# Get nominal price for each time, use to get percent change from last instance\n",
    "# Convert to histogram/empirical distribution and model the distribution\n",
    "\n",
    "# TO DO:\n",
    "# OBTAIN HISTOGRAM\n",
    "# I EXPECT OUTLIERS TO PROVIDE INFORMATION (PERHAPS W.R.T. INTER-DAY PRICE OR HIGHER VOLATILITY AT TRADING DAY START/END)\n",
    "\n",
    "# THE BIN RESOLUTION FOR HISTOGRAMS ARE IMPORTANT IN ASSESSING HOW CLOSELY THE DISTRIBUTION MATCHES THE DATA, MAYBE.\n",
    "# IF TOO COARSE, THE BIN CENTRE WILL BE TOO HIGH, OTHERWISE IT WILL BE TOO FLAT (AND UNEVEN)\n",
    "# IS THE BIN REPRESENTATION A BETTER ASSESSMENT OF THE RESIDUAL OF THE DATA <-> DISTRIBUTION FIT, OR CAN THE RAW DATA BE COMPARED DIRECTLY TO THE DISTRIBUTION?\n",
    "# THE LATTER SOUNDS LIKE IT DOESNT MAKE SENSE AS THE PDF IS TO DO WITH \"PROBABILITY DENSITY\" SO WE MUST USE BINS TO COLLECT REGIONS IN THE PRICE (X) DOMAIN\n",
    "# IF SO I MUST STUDY WHAT BIN SIZE IS OPTIMAL FOR RESIDUAL, IF THE OPTIMAL BIN SIZE CHANGES FOR DIFFERENT DISTRIBUTIONS (PROBABLY YES) OR FOR OUTLIER EFFECTS (ALSO YES)\n",
    "\n",
    "# TO CONSIDER:\n",
    "# MODELLING EACH TIME INSTANCE WITH ITS OWN LIMITED-DOMAIN DISTRIBUTION BASED ON MAX-MIN PRICE AND VOLUME\n",
    "# PRESUME EACH TRADE IS A RANDOM VARIABLE FOLLOWING (WHICH??) DISTRIBUTION\n",
    "# THEREFORE THE COLLECTIVE VOLUME FOLLOWS (WHICH? T-DISTRI.?) DISTRIBUTION\n",
    "\n",
    "# Obtain nominal price as mean of high and low prices (as we have no idea where the mean trade of each time instance may be so our most 'accurate' is probably the center of the limits\n",
    "# Note: can possibly assume a finite domain bell distribution for the nominal value\n",
    "data_MSFT_test['Nominal'] = (data_MSFT_test['High'] + data_MSFT_test['Low']) /2\n",
    "data_MSFT_test['PctChange_N'] = data_MSFT_test['Nominal'].diff()/data_MSFT_test['Nominal']*100\n",
    "#data_MSFT_test['PctChange_H'] = data_MSFT_test['High'].diff()/data_MSFT_test['High']*100\n",
    "#data_MSFT_test['PctChange_L'] = data_MSFT_test['Low'].diff()/data_MSFT_test['Low']*100\n",
    "\n",
    "\n",
    "# Bin count for histogram\n",
    "bincount_list = [5,10,20,40,80,160,320] # Note if too high, worse bell-curve representation\n",
    "# Interval to exclude outliers from\n",
    "exclusion_int = 0.01\n",
    "# Exclude outlier (in display)\n",
    "no_outliers = True\n",
    "\n",
    "# [min, max] ranges of the histogram\n",
    "rangemin = data_MSFT_test['PctChange_N'].min()\n",
    "rangemax = data_MSFT_test['PctChange_N'].max()\n",
    "# Exclusive interval range for histogram (excludes outliers)\n",
    "excl_min = data_MSFT_test['PctChange_N'].quantile(exclusion_int)\n",
    "excl_max = data_MSFT_test['PctChange_N'].quantile(1-exclusion_int)\n",
    "\n",
    "# Outlier data filtering\n",
    "data_filtered = data_MSFT_test[(data_MSFT_test['PctChange_N'] < excl_max) & (data_MSFT_test['PctChange_N'] > excl_min)]\n",
    "\n",
    "xmin = None\n",
    "xmax = None\n",
    "if no_outliers:\n",
    "    xmin = excl_min\n",
    "    xmax = excl_max\n",
    "else:\n",
    "    xmin = rangemin\n",
    "    xmax = rangemax\n",
    "\n",
    "# Bin sequence list for dataset histogram (for the range of bincounts)\n",
    "binseq = []\n",
    "for bincount in bincount_list:\n",
    "    binseq += [[xmin + i*(xmax-xmin)/bincount for i in range(bincount+1)]] # Can truncate for efficiency\n",
    "\n",
    "# CREATE MODEL TO FIT/SHOW ON PLOT AND ASSESS ACCURACY\n",
    "# CREATE CURVE AND EVALUATE CURVE VALUE ON HIST LOCATION, GET ITS RESIDUAL\n",
    "# JUST USE STATS TO FIND WHAT RESIDUAL WOULD BE FOR CURVES GIVEN DATAPOINTS\n",
    "# TEST CONFIDENCE INTERVALS?\n",
    "# WHAT ABOUT KERNEL DENSITY ESTIMATION?\n",
    "\n",
    "# Distribution testing\n",
    "mean_out = data_MSFT_test['PctChange_N'].mean()\n",
    "std_out = data_MSFT_test['PctChange_N'].std()\n",
    "mean_nout = data_filtered['PctChange_N'].mean()\n",
    "std_nout = data_filtered['PctChange_N'].std()\n",
    "\n",
    "pdf = stats.norm.pdf(data_MSFT_test['PctChange_N'].sort_values(), mean_out, std_out) # Values sorted as iterated through index\n",
    "pdf2 = stats.norm.pdf(data_MSFT_test['PctChange_N'].sort_values(), mean_nout, std_nout) # Values sorted as iterated through index\n",
    "plt.plot(data_MSFT_test['PctChange_N'].sort_values(), pdf, label = 'Inc. Outliers')\n",
    "plt.plot(data_MSFT_test['PctChange_N'].sort_values(), pdf2, label = 'Excl. Outliers')\n",
    "plt.xlim([xmin, xmax])\n",
    "\n",
    "\n",
    "\n",
    "# Histogram (density) plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "for bins in binseq:\n",
    "    data_MSFT_test['PctChange_N'].hist(density = True, bins=bins)\n",
    "plt.plot(data_MSFT_test['PctChange_N'].sort_values(), pdf, label = 'Inc. Outliers')\n",
    "plt.plot(data_MSFT_test['PctChange_N'].sort_values(), pdf2, label = 'Excl. Outliers')\n",
    "plt.xlim([xmin, xmax])\n",
    "\n",
    "# Histogram - Residual relation\n",
    "# CALCULATE CENTER OF BIN, ASSESS VALUE OF PDF AT THAT LOCATION, (X-Y)^2\n",
    "\n",
    "\n",
    "\n",
    "# Plot of percentage change vs. time\n",
    "plt.title('Microsoft Data Test Plot')\n",
    "plt.figure()\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "plt.title('PctChange_N')\n",
    "plt.plot(data_MSFT_test['PctChange_N'], label='PctChange_N', marker = 'o',markersize=3,linestyle='dashed')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cols_to_plot = ['Open', 'Close', 'SMA', 'Standard Deviation (O, 10)']\n",
    "\n",
    "data_MSFT_test['SMA'] = data_MSFT_test['Open'].rolling(10).mean()\n",
    "data_MSFT_test['SMAC'] = data_MSFT_test['Close'].rolling(10).mean()\n",
    "data_MSFT_test['Standard Deviation (O, 10)'] = data_MSFT_test['Open'].rolling(10).std()\n",
    "data_MSFT_test['Standard Deviation (O, 5)'] = data_MSFT_test['Open'].rolling(5).std()\n",
    "data_MSFT_test['Standard Deviation (O, 3)'] = data_MSFT_test['Open'].rolling(3).std()\n",
    "data_MSFT_test['Standard Deviation (O, 20)'] = data_MSFT_test['Open'].rolling(20).std()\n",
    "data_MSFT_test['Standard Deviation (O, 40)'] = data_MSFT_test['Open'].rolling(40).std()\n",
    "data_MSFT_test['Standard Deviation (C, 10)'] = data_MSFT_test['Close'].rolling(10).std()\n",
    "# TO DO: MAKE HISTOGRAM OF STD.D.\n",
    "\n",
    "## PLOTTING DATA\n",
    "data_MSFT_test[cols_to_plot].plot()\n",
    "plt.title('Microsoft Data Test Plot')\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Volume')\n",
    "plt.plot(data_MSFT_test['Volume'], label='Volume')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "plt.title('SMAO/C')\n",
    "plt.plot(data_MSFT_test['SMA'], label='SMA (Open)', marker = 'o',markersize=3,linestyle='dashed')\n",
    "plt.plot(data_MSFT_test['SMAC'], label='SMA (Close)', marker = 'x',markersize=3,linestyle='dashed')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "plt.title('Standard Deviation')\n",
    "plt.plot(data_MSFT_test['Standard Deviation (O, 10)'], label='Standard Deviation (Open)', marker = 'o',markersize=3,linestyle='dashed')\n",
    "plt.plot(data_MSFT_test['Standard Deviation (C, 10)'], label='Standard Deviation (Close)', marker = 'o',markersize=3,linestyle='dashed')\n",
    "\n",
    "std_dev_cols = ['Standard Deviation (O, 3)', 'Standard Deviation (O, 5)', 'Standard Deviation (O, 10)', 'Standard Deviation (O, 20)', 'Standard Deviation (O, 40)']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "plt.title('Standard Deviation')\n",
    "plt.plot(data_MSFT_test['Standard Deviation (O, 3)'], label='Standard Deviation (Open)', marker = 'o',markersize=3,linestyle='dashed')\n",
    "plt.plot(data_MSFT_test['Standard Deviation (O, 5)'], label='Standard Deviation (Open)', marker = 'o',markersize=3,linestyle='dashed')\n",
    "plt.plot(data_MSFT_test['Standard Deviation (O, 10)'], label='Standard Deviation (Open)', marker = 'o',markersize=3,linestyle='dashed')\n",
    "plt.plot(data_MSFT_test['Standard Deviation (O, 20)'], label='Standard Deviation (Open)', marker = 'o',markersize=3,linestyle='dashed')\n",
    "plt.plot(data_MSFT_test['Standard Deviation (O, 40)'], label='Standard Deviation (Open)', marker = 'o',markersize=3,linestyle='dashed')\n",
    "\n",
    "plt.title('Standard Deviation')\n",
    "data_MSFT_test[std_dev_cols].plot()\n",
    "\n",
    "## Here we also do a auto-correlation test\n",
    "#fig, ax = plt.subplots(figsize=(12, 6))\n",
    "#sm.graphics.tsa.plot_acf(data_MSFT_test['Close'], lags=195, ax=ax)\n",
    "#plt.title('Autocorrelation Function (ACF)')\n",
    "#plt.xlabel('Lags')\n",
    "#plt.ylabel('ACF')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6166a2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### TO DO:\n",
    "# We are supposed to assess the % change of stock values\n",
    "# I will calculate this, and also its variance, I will do this for different resolutions and in a rolling window (I am thinking of sizes, 5, 10, 20, 50, 100)\n",
    "# Should I calculate a global (or yearly or monthly) variance and compare this to the window to see if theres a match for specific sizes (which correspond to same timeframe)?\n",
    "# Once I do this I can really go into the trade models I made previously\n",
    "\n",
    "##### THOUGHTS:\n",
    "# Is there a point in hypothesis testing a 'true' variance or mean for growth stocks? perhaps other than to test for a non-constant mean model?\n",
    "# There is point in hypothesis testing dividend stocks/ETFs (i.e. FTSE 100 Vanguard ETF)\n",
    "# Is it reasonable to model the mean (or exp. stock value) as fct of time in context of a fourier transform\n",
    "# (as many growth+dividend stocks have cyclical behaviour around quarterlies, typically goes up then down later) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_MSFT_test = checkManifest.loadData_fromcsv(\"MSFT\", 15, \"2024-08\")\n",
    "data_MSFT_test.set_index(pd.to_datetime(data_MSFT_test['DateTime']), inplace=True)\n",
    "\n",
    "\n",
    "############\n",
    "############\n",
    "# Modelling prices:\n",
    "# Get nominal price for each time, use to get percent change from last instance\n",
    "# Convert to histogram/empirical distribution and model the distribution\n",
    "\n",
    "# TO DO:\n",
    "# OBTAIN HISTOGRAM\n",
    "# I EXPECT OUTLIERS TO PROVIDE INFORMATION (PERHAPS W.R.T. INTER-DAY PRICE OR HIGHER VOLATILITY AT TRADING DAY START/END)\n",
    "\n",
    "# THE BIN RESOLUTION FOR HISTOGRAMS ARE IMPORTANT IN ASSESSING HOW CLOSELY THE DISTRIBUTION MATCHES THE DATA, MAYBE.\n",
    "# IF TOO COARSE, THE BIN CENTRE WILL BE TOO HIGH, OTHERWISE IT WILL BE TOO FLAT (AND UNEVEN)\n",
    "# IS THE BIN REPRESENTATION A BETTER ASSESSMENT OF THE RESIDUAL OF THE DATA <-> DISTRIBUTION FIT, OR CAN THE RAW DATA BE COMPARED DIRECTLY TO THE DISTRIBUTION?\n",
    "# THE LATTER SOUNDS LIKE IT DOESNT MAKE SENSE AS THE PDF IS TO DO WITH \"PROBABILITY DENSITY\" SO WE MUST USE BINS TO COLLECT REGIONS IN THE PRICE (X) DOMAIN\n",
    "# IF SO I MUST STUDY WHAT BIN SIZE IS OPTIMAL FOR RESIDUAL, IF THE OPTIMAL BIN SIZE CHANGES FOR DIFFERENT DISTRIBUTIONS (PROBABLY YES) OR FOR OUTLIER EFFECTS (ALSO YES)\n",
    "\n",
    "# TO CONSIDER:\n",
    "# MODELLING EACH TIME INSTANCE WITH ITS OWN LIMITED-DOMAIN DISTRIBUTION BASED ON MAX-MIN PRICE AND VOLUME\n",
    "# PRESUME EACH TRADE IS A RANDOM VARIABLE FOLLOWING (WHICH??) DISTRIBUTION\n",
    "# THEREFORE THE COLLECTIVE VOLUME FOLLOWS (WHICH? T-DISTRI.?) DISTRIBUTION\n",
    "\n",
    "# Obtain nominal price as mean of high and low prices (as we have no idea where the mean trade of each time instance may be so our most 'accurate' is probably the center of the limits\n",
    "# Note: can possibly assume a finite domain bell distribution for the nominal value\n",
    "data_MSFT_test['Nominal'] = (data_MSFT_test['High'] + data_MSFT_test['Low']) /2\n",
    "data_MSFT_test['PctChange_N'] = data_MSFT_test['Nominal'].diff()/data_MSFT_test['Nominal']*100\n",
    "#data_MSFT_test['PctChange_H'] = data_MSFT_test['High'].diff()/data_MSFT_test['High']*100\n",
    "#data_MSFT_test['PctChange_L'] = data_MSFT_test['Low'].diff()/data_MSFT_test['Low']*100\n",
    "\n",
    "\n",
    "# Bin count for histogram\n",
    "bincount_list = [5,10,20,40,80,160,320] # Note if too high, worse bell-curve representation\n",
    "# Interval to exclude outliers from\n",
    "exclusion_int = 0.01\n",
    "# Exclude outlier (in display)\n",
    "no_outliers = True\n",
    "\n",
    "# [min, max] ranges of the histogram\n",
    "rangemin = data_MSFT_test['PctChange_N'].min()\n",
    "rangemax = data_MSFT_test['PctChange_N'].max()\n",
    "# Exclusive interval range for histogram (excludes outliers)\n",
    "excl_min = data_MSFT_test['PctChange_N'].quantile(exclusion_int)\n",
    "excl_max = data_MSFT_test['PctChange_N'].quantile(1-exclusion_int)\n",
    "\n",
    "# Outlier data filtering\n",
    "data_filtered = data_MSFT_test[(data_MSFT_test['PctChange_N'] < excl_max) & (data_MSFT_test['PctChange_N'] > excl_min)]\n",
    "\n",
    "xmin = None\n",
    "xmax = None\n",
    "if no_outliers:\n",
    "    xmin = excl_min\n",
    "    xmax = excl_max\n",
    "else:\n",
    "    xmin = rangemin\n",
    "    xmax = rangemax\n",
    "\n",
    "# Bin sequence list for dataset histogram (for the range of bincounts)\n",
    "binseq = []\n",
    "for bincount in bincount_list:\n",
    "    binseq += [[xmin + i*(xmax-xmin)/bincount for i in range(bincount+1)]] # Can truncate for efficiency\n",
    "\n",
    "# CREATE MODEL TO FIT/SHOW ON PLOT AND ASSESS ACCURACY\n",
    "# CREATE CURVE AND EVALUATE CURVE VALUE ON HIST LOCATION, GET ITS RESIDUAL\n",
    "# JUST USE STATS TO FIND WHAT RESIDUAL WOULD BE FOR CURVES GIVEN DATAPOINTS\n",
    "# TEST CONFIDENCE INTERVALS?\n",
    "# WHAT ABOUT KERNEL DENSITY ESTIMATION?\n",
    "\n",
    "# Distribution testing\n",
    "mean_out = data_MSFT_test['PctChange_N'].mean()\n",
    "std_out = data_MSFT_test['PctChange_N'].std()\n",
    "mean_nout = data_filtered['PctChange_N'].mean()\n",
    "std_nout = data_filtered['PctChange_N'].std()\n",
    "\n",
    "pdf = stats.norm.pdf(data_MSFT_test['PctChange_N'].sort_values(), mean_out, std_out) # Values sorted as iterated through index\n",
    "pdf2 = stats.norm.pdf(data_MSFT_test['PctChange_N'].sort_values(), mean_nout, std_nout) # Values sorted as iterated through index\n",
    "plt.plot(data_MSFT_test['PctChange_N'].sort_values(), pdf, label = 'Inc. Outliers')\n",
    "plt.plot(data_MSFT_test['PctChange_N'].sort_values(), pdf2, label = 'Excl. Outliers')\n",
    "plt.xlim([xmin, xmax])\n",
    "\n",
    "\n",
    "\n",
    "# Histogram (density) plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "for bins in binseq:\n",
    "    data_MSFT_test['PctChange_N'].hist(density = True, bins=bins)\n",
    "plt.plot(data_MSFT_test['PctChange_N'].sort_values(), pdf, label = 'Inc. Outliers')\n",
    "plt.plot(data_MSFT_test['PctChange_N'].sort_values(), pdf2, label = 'Excl. Outliers')\n",
    "plt.xlim([xmin, xmax])\n",
    "\n",
    "# Histogram - Residual relation\n",
    "# CALCULATE CENTER OF BIN, ASSESS VALUE OF PDF AT THAT LOCATION, (X-Y)^2\n",
    "\n",
    "\n",
    "\n",
    "# Plot of percentage change vs. time\n",
    "plt.title('Microsoft Data Test Plot')\n",
    "plt.figure()\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "plt.title('PctChange_N')\n",
    "plt.plot(data_MSFT_test['PctChange_N'], label='PctChange_N', marker = 'o',markersize=3,linestyle='dashed')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cols_to_plot = ['Open', 'Close', 'SMA', 'Standard Deviation (O, 10)']\n",
    "\n",
    "data_MSFT_test['SMA'] = data_MSFT_test['Open'].rolling(10).mean()\n",
    "data_MSFT_test['SMAC'] = data_MSFT_test['Close'].rolling(10).mean()\n",
    "data_MSFT_test['Standard Deviation (O, 10)'] = data_MSFT_test['Open'].rolling(10).std()\n",
    "data_MSFT_test['Standard Deviation (O, 5)'] = data_MSFT_test['Open'].rolling(5).std()\n",
    "data_MSFT_test['Standard Deviation (O, 3)'] = data_MSFT_test['Open'].rolling(3).std()\n",
    "data_MSFT_test['Standard Deviation (O, 20)'] = data_MSFT_test['Open'].rolling(20).std()\n",
    "data_MSFT_test['Standard Deviation (O, 40)'] = data_MSFT_test['Open'].rolling(40).std()\n",
    "data_MSFT_test['Standard Deviation (C, 10)'] = data_MSFT_test['Close'].rolling(10).std()\n",
    "# TO DO: MAKE HISTOGRAM OF STD.D.\n",
    "\n",
    "## PLOTTING DATA\n",
    "data_MSFT_test[cols_to_plot].plot()\n",
    "plt.title('Microsoft Data Test Plot')\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Volume')\n",
    "plt.plot(data_MSFT_test['Volume'], label='Volume')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "plt.title('SMAO/C')\n",
    "plt.plot(data_MSFT_test['SMA'], label='SMA (Open)', marker = 'o',markersize=3,linestyle='dashed')\n",
    "plt.plot(data_MSFT_test['SMAC'], label='SMA (Close)', marker = 'x',markersize=3,linestyle='dashed')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "plt.title('Standard Deviation')\n",
    "plt.plot(data_MSFT_test['Standard Deviation (O, 10)'], label='Standard Deviation (Open)', marker = 'o',markersize=3,linestyle='dashed')\n",
    "plt.plot(data_MSFT_test['Standard Deviation (C, 10)'], label='Standard Deviation (Close)', marker = 'o',markersize=3,linestyle='dashed')\n",
    "\n",
    "std_dev_cols = ['Standard Deviation (O, 3)', 'Standard Deviation (O, 5)', 'Standard Deviation (O, 10)', 'Standard Deviation (O, 20)', 'Standard Deviation (O, 40)']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "plt.title('Standard Deviation')\n",
    "plt.plot(data_MSFT_test['Standard Deviation (O, 3)'], label='Standard Deviation (Open)', marker = 'o',markersize=3,linestyle='dashed')\n",
    "plt.plot(data_MSFT_test['Standard Deviation (O, 5)'], label='Standard Deviation (Open)', marker = 'o',markersize=3,linestyle='dashed')\n",
    "plt.plot(data_MSFT_test['Standard Deviation (O, 10)'], label='Standard Deviation (Open)', marker = 'o',markersize=3,linestyle='dashed')\n",
    "plt.plot(data_MSFT_test['Standard Deviation (O, 20)'], label='Standard Deviation (Open)', marker = 'o',markersize=3,linestyle='dashed')\n",
    "plt.plot(data_MSFT_test['Standard Deviation (O, 40)'], label='Standard Deviation (Open)', marker = 'o',markersize=3,linestyle='dashed')\n",
    "\n",
    "plt.title('Standard Deviation')\n",
    "data_MSFT_test[std_dev_cols].plot()\n",
    "\n",
    "## Here we also do a auto-correlation test\n",
    "#fig, ax = plt.subplots(figsize=(12, 6))\n",
    "#sm.graphics.tsa.plot_acf(data_MSFT_test['Close'], lags=195, ax=ax)\n",
    "#plt.title('Autocorrelation Function (ACF)')\n",
    "#plt.xlabel('Lags')\n",
    "#plt.ylabel('ACF')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8275aa8d-a137-4e2f-8df4-6d13fa6d987f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "## Here we read the .csv data and do a time-series analysis (rolling mean)\n",
    "\n",
    "data_ARM = pd.read_csv('StockHistData\\ARM.csv', index_col='Date', parse_dates=['Date'])\n",
    "\n",
    "#cols_to_plot = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'SMA', 'Standard Deviation']\n",
    "cols_to_plot = ['Close', 'SMA', 'Standard Deviation']\n",
    "print(data_ARM)\n",
    "data_ARM['SMA'] = data_ARM['Open'].rolling(10).mean()\n",
    "data_ARM['Standard Deviation'] = data_ARM['Open'].rolling(10).std()\n",
    "\n",
    "df_ARM = pd.DataFrame(data_ARM)\n",
    "\n",
    "data_ARM[cols_to_plot].plot()\n",
    "plt.title('ARM Technologies')\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Volume')\n",
    "plt.plot(data_ARM['Volume'], label='Volume')\n",
    "\n",
    "## Here we also do a auto-correlation test\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "sm.graphics.tsa.plot_acf(data_ARM['Close'], lags=195, ax=ax)\n",
    "plt.title('Autocorrelation Function (ACF)')\n",
    "plt.xlabel('Lags')\n",
    "plt.ylabel('ACF')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc7cd06-871d-43bd-8185-e34b0b16a972",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Same for MSFT\n",
    "data_MSFT = pd.read_csv('StockHistData\\MSFT.csv', index_col='Date', parse_dates=['Date'])\n",
    "\n",
    "#cols_to_plot = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'SMA', 'Standard Deviation']\n",
    "cols_to_plot = ['Close', 'SMA', 'Standard Deviation']\n",
    "\n",
    "data_MSFT['SMA'] = data_MSFT['Open'].rolling(10).mean()\n",
    "data_MSFT['Standard Deviation'] = data_MSFT['Open'].rolling(10).std()\n",
    "\n",
    "df_MSFT = pd.DataFrame(data_MSFT)\n",
    "\n",
    "data_MSFT[cols_to_plot].plot()\n",
    "plt.title('Microsoft')\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Volume')\n",
    "plt.plot(data_MSFT['Volume'], label='Volume')\n",
    "\n",
    "## Here we also do a auto-correlation test\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "sm.graphics.tsa.plot_acf(data_MSFT['Close'], lags=1257, ax=ax)\n",
    "plt.title('Autocorrelation Function (ACF)')\n",
    "plt.xlabel('Lags')\n",
    "plt.ylabel('ACF')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b3cd4b-a5f1-4d7b-ac2b-aabf79312517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebb5f8a-dc16-4314-8ddb-b50ff410405d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Then NVDA\n",
    "data_NVDA = pd.read_csv('StockHistData\\\\NVDA.csv', index_col='Date', parse_dates=['Date'])\n",
    "\n",
    "#cols_to_plot = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'SMA', 'Standard Deviation']\n",
    "cols_to_plot = ['Close', 'SMA', 'Standard Deviation']\n",
    "\n",
    "data_NVDA['SMA'] = data_NVDA['Open'].rolling(10).mean()\n",
    "data_NVDA['Standard Deviation'] = data_NVDA['Open'].rolling(10).std()\n",
    "\n",
    "df_NVDA = pd.DataFrame(data_NVDA)\n",
    "\n",
    "data_NVDA[cols_to_plot].plot()\n",
    "plt.title('Nvidia')\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Volume')\n",
    "plt.plot(data_NVDA['Volume'], label='Volume')\n",
    "\n",
    "## Here we also do a auto-correlation test\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "sm.graphics.tsa.plot_acf(data_NVDA['Close'], lags=1257, ax=ax)\n",
    "plt.title('Autocorrelation Function (ACF)')\n",
    "plt.xlabel('Lags')\n",
    "plt.ylabel('ACF')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5673a70-cb95-4f60-9607-2cfa82f35117",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets try for KO\n",
    "data_KO = pd.read_csv('StockHistData\\\\KO.csv', index_col='Date', parse_dates=['Date'])\n",
    "\n",
    "#cols_to_plot = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'SMA', 'Standard Deviation']\n",
    "cols_to_plot = ['Close', 'SMA', 'Standard Deviation']\n",
    "\n",
    "data_KO['SMA'] = data_KO['Open'].rolling(10).mean()\n",
    "data_KO['Standard Deviation'] = data_KO['Open'].rolling(10).std()\n",
    "\n",
    "df_KO = pd.DataFrame(data_KO)\n",
    "\n",
    "data_KO[cols_to_plot].plot()\n",
    "plt.title('Coca-cola')\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Volume')\n",
    "plt.plot(data_KO['Volume'], label='Volume')\n",
    "\n",
    "## Here we also do a auto-correlation test\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "sm.graphics.tsa.plot_acf(data_KO['Close'], lags=1257, ax=ax)\n",
    "plt.title('Autocorrelation Function (ACF)')\n",
    "plt.xlabel('Lags')\n",
    "plt.ylabel('ACF')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d167f87b-4ec3-44ad-90fe-4a6b8e9f7f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlations between the different stocks\n",
    "correlation1 = df_MSFT['Close'][-1257:].corr(df_NVDA['Close'][-1257:])\n",
    "correlation1k = df_MSFT['Close'][-1257:].corr(df_NVDA['Close'][-1257:], method='kendall')\n",
    "correlation1s = df_MSFT['Close'][-1257:].corr(df_NVDA['Close'][-1257:], method='spearman')\n",
    "\n",
    "print(f\"Correlation coefficient between MSFT and NVDA: {correlation1} ({correlation1k} with Kendall and {correlation1s} with Spearman)\")\n",
    "\n",
    "correlation2 = df_MSFT['Close'][-195:].corr(df_ARM['Close'][-195:])\n",
    "correlation2k = df_MSFT['Close'][-195:].corr(df_ARM['Close'][-195:], method='kendall')\n",
    "correlation2s = df_MSFT['Close'][-195:].corr(df_ARM['Close'][-195:], method='spearman')\n",
    "\n",
    "print(f\"Correlation coefficient between MSFT and ARM: {correlation2} ({correlation2k} with Kendall and {correlation2s} with Spearman)\")\n",
    "\n",
    "correlation3 = df_ARM['Close'][-195:].corr(df_NVDA['Close'][-195:])\n",
    "correlation3k = df_ARM['Close'][-195:].corr(df_NVDA['Close'][-195:], method='kendall')\n",
    "correlation3s = df_ARM['Close'][-195:].corr(df_NVDA['Close'][-195:], method='spearman')\n",
    "\n",
    "print(f\"Correlation coefficient between ARM and NVDA: {correlation3} ({correlation3k} with Kendall and {correlation3s} with Spearman)\")\n",
    "\n",
    "correlation4 = df_KO['Close'][-1257:].corr(df_NVDA['Close'][-1257:])\n",
    "correlation4k = df_KO['Close'][-1257:].corr(df_NVDA['Close'][-1257:], method='kendall')\n",
    "correlation4s = df_KO['Close'][-1257:].corr(df_NVDA['Close'][-1257:], method='spearman')\n",
    "\n",
    "print(f\"Correlation coefficient between KO and NVDA: {correlation4} ({correlation4k} with Kendall and {correlation4s} with Spearman)\")\n",
    "\n",
    "correlation5 = df_KO['Close'][-1257:].corr(df_MSFT['Close'][-1257:])\n",
    "correlation5k = df_KO['Close'][-1257:].corr(df_MSFT['Close'][-1257:], method='kendall')\n",
    "correlation5s = df_KO['Close'][-1257:].corr(df_MSFT['Close'][-1257:], method='spearman')\n",
    "\n",
    "print(f\"Correlation coefficient between ARM and NVDA: {correlation5} ({correlation5k} with Kendall and {correlation5s} with Spearman)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1165160f-72e0-4c4b-a765-a4c425686004",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we do simply Quantitative analysis (mean, std)\n",
    "import numpy as np\n",
    "\n",
    "# Sample Data\n",
    "returns = np.array([0.01, 0.02, -0.01, 0.03, 0.005])\n",
    "\n",
    "# Calculate Mean and Standard Deviation\n",
    "mean_return = np.mean(returns)\n",
    "std_deviation = np.std(returns)\n",
    "print(f\"Mean Return: {mean_return}, Standard Deviation: {std_deviation}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01726d8-1d92-4be2-b01e-2be0831d3c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we have code to do hypothesis testing\n",
    "from scipy import stats\n",
    "\n",
    "group_A = [0.01, 0.02, 0.015, 0.023, 0.016]\n",
    "group_B = [0.02, 0.025, 0.03, 0.019, 0.021]\n",
    "sig_value = 0.05\n",
    "\n",
    "## We are doing a t-test so see if the mean of group A and group B are the same\n",
    "## The test is bayesian, we assume they are until proven otherwise with a significance value of 0.05 (5%)\n",
    "t_statistic, p_value = stats.ttest_ind(group_A, group_B)\n",
    "print(f\"t-statistic: {t_statistic}, p-value: {p_value}\")\n",
    "\n",
    "trunc_p = '%.3f'%(100*p_value)\n",
    "\n",
    "if p_value >= sig_value:\n",
    "    print(f\"Hypothesis of Group A and Group B means being equal is NOT REJECTED (significance value of {100*sig_value}%, p-value of {trunc_p}%)\")\n",
    "else:\n",
    "    print(f\"Hypothesis of Group A and Group B means being equal is REJECTED (significance value of {100*sig_value}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e153e5-c7b7-4a20-ab09-cd41b6940bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we do some more time-series analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Seed for reproducibility (we will create synthetic data)\n",
    "np.random.seed(99)\n",
    "\n",
    "# Generate synthetic data\n",
    "n_samples = 100\n",
    "dates = pd.date_range(start='2024-01-01', periods=n_samples, freq='B')  # Business days\n",
    "price_changes = np.random.normal(loc=0, scale=1, size=n_samples)  # Random price changes\n",
    "prices = np.cumsum(price_changes) + 100  # Simulated stock prices (random walk)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Date': dates,\n",
    "    'Close': prices\n",
    "})\n",
    "\n",
    "# Plot the time series\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df['Date'], df['Close'], label='Close Price')\n",
    "plt.title('Synthetic Stock Closing Prices')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and plot ACF\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "sm.graphics.tsa.plot_acf(df['Close'], lags=30, ax=ax)\n",
    "plt.title('Autocorrelation Function (ACF)')\n",
    "plt.xlabel('Lags')\n",
    "plt.ylabel('ACF')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c125341-7ef7-408f-a23d-fc4ca588924d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we do some predictive modelling\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Sample Data\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([2, 3, 2.5, 4, 4.5])\n",
    "\n",
    "# Train Model\n",
    "model = LinearRegression().fit(X, y)\n",
    "print(f\"Coefficient: {model.coef_}, Intercept: {model.intercept_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba1aa73-a950-4905-9c48-f4a6e0958223",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we have some code for option pricing (black-scholes)\n",
    "\n",
    "import scipy.stats as si\n",
    "import numpy as np\n",
    "\n",
    "def black_scholes(S, K, T, r, sigma, option_type='call'):\n",
    "    d1 = (np.log(S / K) + (r + 0.5 * sigma ** 2) * T) / (sigma * np.sqrt(T))\n",
    "    d2 = d1 - sigma * np.sqrt(T)\n",
    "    if option_type == 'call':\n",
    "        option_price = (S * si.norm.cdf(d1, 0, 1) - K * np.exp(-r * T) * si.norm.cdf(d2, 0, 1))\n",
    "    elif option_type == 'put':\n",
    "        option_price = (K * np.exp(-r * T) * si.norm.cdf(-d2, 0, 1) - S * si.norm.cdf(-d1, 0, 1))\n",
    "    return option_price\n",
    "\n",
    "# Example Parameters\n",
    "S = 100  # Current stock price\n",
    "K = 105  # Strike price\n",
    "T = 1    # Time to maturity in years\n",
    "r = 0.05 # Risk-free rate\n",
    "sigma = 0.2 # Volatility\n",
    "\n",
    "call_price = black_scholes(S, K, T, r, sigma, option_type='call')\n",
    "print(f\"Call Option Price: {call_price}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30655935-ea66-43cb-b321-97777facd378",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here's some code of logistic regression\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'Open': [100.0, 101.0, 100.5, 101.2],\n",
    "    'High': [102.0, 103.0, 102.5, 101.5],\n",
    "    'Low': [98.0, 99.5, 99.0, 99.8],\n",
    "    'Close': [101.0, 100.5, 101.5, 100.8],\n",
    "    'Volume': [1500000, 1700000, 1800000, 1300000],\n",
    "    'SMA_10': [99.5, 100.0, 100.2, 100.8],\n",
    "    'RSI': [55.0, 52.5, 58.0, 59],\n",
    "    'Label': [1, 0, 1, 0]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Features and target\n",
    "X = df[['Open', 'High', 'Low', 'Close', 'Volume', 'SMA_10', 'RSI']]\n",
    "y = df['Label']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler(Y) # type: ignore\n",
    "X_train_scaled = scaler.it_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled,y_train)\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c00d66-9492-4728-9d7e-738a8015fd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Seed for reproducibility\n",
    "np.random.seed(10)\n",
    "\n",
    "# Generate synthetic data\n",
    "n_samples = 1000\n",
    "dates = pd.date_range(start='2024-01-01', periods=n_samples, freq='B')\n",
    "open_prices = np.random.uniform(low=100, high=200, size=n_samples)\n",
    "high_prices = open_prices + np.random.uniform(low=0, high=10, size=n_samples)\n",
    "low_prices = open_prices - np.random.uniform(low=0, high=10, size=n_samples)\n",
    "close_prices = open_prices + np.random.uniform(low=-5, high=5, size=n_samples)\n",
    "volume = np.random.randint(low=100000, high=5000000, size=n_samples)\n",
    "\n",
    "# Simple Moving Average (SMA) with window of 10\n",
    "sma_10 = pd.Series(close_prices).rolling(window=10).mean().ffill()\n",
    "\n",
    "# Relative Strength Index (RSI)\n",
    "def calculate_rsi(series, window=14):\n",
    "    delta = series.diff().ffill()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "rsi = calculate_rsi(pd.Series(close_prices))\n",
    "\n",
    "# Label: 1 if next day's close price is higher, else 0\n",
    "labels = np.where(np.roll(close_prices, -1) > close_prices, 1, 0)\n",
    "labels[-1] = 0  # Last label cannot be determined\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Date': dates,\n",
    "    'Open': open_prices,\n",
    "    'High': high_prices,\n",
    "    'Low': low_prices,\n",
    "    'Close': close_prices,\n",
    "    'Volume': volume,\n",
    "    'SMA_10': sma_10,\n",
    "    'RSI': rsi,\n",
    "    'Label': labels\n",
    "})\n",
    "\n",
    "# Remove the last row as it doesn't have a valid label\n",
    "df = df[:-1]\n",
    "\n",
    "# Features and target\n",
    "X = df[['Open', 'High', 'Low', 'Close', 'Volume', 'SMA_10', 'RSI']]\n",
    "y = df['Label']\n",
    "t = df['Date']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test, t_train, t_test = train_test_split(X, y, t, test_size=0.33, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(\"Data type: \",X_train_scaled.dtype )\n",
    "print(\"Data type: \",y_train.dtype )\n",
    "print(\"Dimensions: \",np.shape(X_train_scaled))\n",
    "print(\"Dimensions: \",np.shape(y_train))\n",
    "\n",
    "# Train the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Plot some of the data to visualize\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(dates, close_prices, label='Close Price')\n",
    "plt.plot(dates, sma_10, label='SMA 10')\n",
    "plt.title('Stock Prices and SMA 10')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369ca1d0-f34e-4b65-87ce-d68b79efbb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Information store of how we run python script elsewhere\n",
    "\n",
    "%run \"D:\\Finance Study\\Python and MATLAB Code\\Core.py\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9d3225-e353-4a5f-b3e5-d8550b4e7085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we try to make a signal indicator from the SMIIO (Stochastic Momentum Index ergodic Indicator plus SMI ergodic Oscillator) of a stock\n",
    "# The code makes an SMI of the stock by calculating the True Strength Index (TSI) of the stock using a slow and a fast period. Also makes an\n",
    "# (ergodic) Indicator by using an Exponential Moving Average (EMA) using the signal period. Finally, makes (ergodic) Oscillator signal by\n",
    "# subtracting the Indicator from the TSI.\n",
    "\n",
    "\n",
    "\n",
    "fast_period = 3 # In the freq of file (days)\n",
    "slow_period = 15\n",
    "signal_period = 3\n",
    "\n",
    "\n",
    "df_smi = ta.momentum.smi(df_NVDA['Close'],fast_period,slow_period,signal_period)\n",
    "\n",
    "\n",
    "smi_name = \"_{}_{}_{}\".format(fast_period,slow_period,signal_period)\n",
    "#SMI + smi_name is the SMI of the stock\n",
    "#SMIs + smi_name is the indicator made from signal line\n",
    "#SMIo + smi_name is the oscillator made by SMI - SMIs\n",
    "\n",
    "\n",
    "cols_plot = [\"SMI\"+smi_name,\"SMIs\"+smi_name,\"SMIo\"+smi_name]\n",
    "\n",
    "df_smi[cols_plot].plot()\n",
    "\n",
    "df_smi.fillna(0, inplace = True) # Removing NaN values\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(df_smi)\n",
    "\n",
    "subtrac=df_smi.iloc[:,0] - df_smi.iloc[:,1] # This is the same as SMIo\n",
    "#plt.figure(figsize=(12,6))\n",
    "#plt.plot(subtrac)\n",
    "\n",
    "\n",
    "df_smi = df_smi.reset_index() # Get date as a non index col to use for bar plot\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.bar(df_smi[\"Date\"],df_smi[\"SMIo\"+smi_name],2,12,color=np.where(df_smi[\"SMIo\"+smi_name] < 0, 'crimson', 'green'))\n",
    "\n",
    "\n",
    "## BEAUTIFULSOUP FOR WEBSCRAPING IN PYTHON\n",
    "## SELENIUM FOR dynamic html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c3f118-822a-4f85-88c7-ae27322de30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we try to make a simple backtesting simulation model where we use an indicator to test approximate success in trading over course of a day\n",
    "# or longer. We use the SMIIO model and simulate a trade whenever a condition is reached (e.g. change from negative to positive).\n",
    "# The trade is initially done as 1 stock trade per instance and we assess the percentage of success (profit made), and size of success.\n",
    "\n",
    "fast = 3\n",
    "slow = 15\n",
    "sig = 4\n",
    "\n",
    "df_smiio = ta.momentum.smi(df_NVDA['Close'],fast,slow,sig)\n",
    "\n",
    "smiio_name = \"_{}_{}_{}\".format(fast,slow,sig)\n",
    "\n",
    "df_smiio = df_smiio.reset_index()\n",
    "\n",
    "#df_smiio.fillna(0, inplace = True) # Removing NaN values\n",
    "\n",
    "\n",
    "for i in range(0,len(df_smiio['SMIo'+smiio_name])):\n",
    "\n",
    "    if np.isnan(df_smiio['SMIo'+smiio_name][i]):\n",
    "        continue\n",
    "\n",
    "    ind_diff = df_smiio['SMIo'+smiio_name][i] - df_smiio['SMIo'+smiio_name][i-1]\n",
    "\n",
    "    last_val = df_smiio['SMIo'+smiio_name][i-1]\n",
    "    \n",
    "    #print(df_smiio['SMIo'+smiio_name][i])\n",
    "    #print(ind_diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5fef8b-5995-4a66-b64c-6b26df302711",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Here we will write basic functions to 'purchase' and 'sell' stocks, and a function to sum up the transactions for a profit/loss measurement\n",
    "histCols = ['Date','Ticker','Volume','Value','TimingState']\n",
    "global dfTradeHist\n",
    "dfTradeHist = pd.DataFrame(columns = histCols)\n",
    "\n",
    "#######\n",
    "def PrepDataFrame(dataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Make all necessary preparation to use the dataframe in model (NOT FULLY COMPLETE). Currently this is:\n",
    "    - Moving any date/time outside of dataframe index\n",
    "    - Changing date/time to a full date-time style (i.e. dates-only will also include time (00:00:00))\n",
    "    \"\"\"\n",
    "    # Guard function to get indexData (usually date) as a non index col to use (if as index) \n",
    "    if 'Date' not in dataFrame.columns: \n",
    "        dataFrame = dataFrame.reset_index()\n",
    "    \n",
    "    # Converts possible date-only to date-time\n",
    "    dataFrame['Date'] = pd.to_datetime(dataFrame.Date, format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # Convert date-time to string (seems to create more bugs)\n",
    "    #dataFrame['Date'] = dataFrame['Date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    return dataFrame\n",
    "#######\n",
    "\n",
    "#######\n",
    "## A FUNCTION TO REMOVE DATA? PROB NOT NEEDED\n",
    "#######\n",
    "\n",
    "\n",
    "\n",
    "#######\n",
    "## A SET OF FUNCTIONS TO ANALYSE STOCK DATA, TRAJECTORY, VOLATILITY ETC.\n",
    "#######\n",
    "\n",
    "\n",
    "#######\n",
    "## FUNCTION TO MODIFY AN ACCURACY/ANALYTICS MATRIX DIRECTLY (INSTEAD OF DOING IT WITHIN THE MODEL ITSELF) FOR INTEROPERABILITY WITH MODEL EVALUATOR\n",
    "#######\n",
    "\n",
    "\n",
    "\n",
    "#######\n",
    "def ResetTradeHist(name = 'dfTradeHist', cols = ['DateTime','Ticker','Volume','Value','TimingState']):\n",
    "    \"\"\" Reset the trade history dataframe and remakes its columns (cols, if specified). \"\"\"    \n",
    "    globals()[name] = pd.DataFrame(columns = cols)\n",
    "    return\n",
    "#######\n",
    "\n",
    "#######\n",
    "def MakeTrade (dateTime, volume, ticker, history = None, buy = True, timingState = 'Close', declare = False) -> [[float, str, float, float, str], pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Simulates making a specified trade and records it (using a sub-routine). Ticker price dataframe must have 'df_' prefix\n",
    "    \"\"\"\n",
    "    # Default history input save status (take dfTradeHist as default storage dataframe)\n",
    "    isDefaultHist = False\n",
    "    if history is None:\n",
    "        global dfTradeHist\n",
    "        history = dfTradeHist.copy()\n",
    "        isDefaultHist = True\n",
    "    \n",
    "    \n",
    "    datasetName = 'df_'+ticker\n",
    "\n",
    "    # The 1st part gets the global dataset, 2nd searches the datetime and 3rd gets the timings state price\n",
    "    unitValue = globals()[datasetName][ globals()[datasetName]['Date'] == dateTime ][timingState].values[0]\n",
    "    \n",
    "    totalValue = -unitValue * volume\n",
    "    transaction = 'bought'\n",
    "    \n",
    "    if not buy:\n",
    "        transaction = 'sold'\n",
    "\n",
    "    if declare == True:\n",
    "        print(f\"{volume} {ticker} stock(s) {transaction} at total price {-totalValue} (unit price {unitValue}) at {dateTime}\")\n",
    "\n",
    "    if not buy:\n",
    "        totalValue = -totalValue\n",
    "\n",
    "    result = [dateTime, ticker, volume, totalValue, timingState]\n",
    "    \n",
    "    ## Store in history\n",
    "    history = RecordAction(result, history)\n",
    "\n",
    "    # Default history location (if None then it won't update the original dataframe otherwise)\n",
    "    if isDefaultHist:\n",
    "        dfTradeHist = history.copy()\n",
    "    \n",
    "    return [[dateTime, ticker, volume, totalValue, timingState], history]\n",
    "#######\n",
    "#######\n",
    "def RecordAction(action, dataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Records action taken into a history dataframe.\n",
    "    Note: Make sure the action row size is the same as the column size in the dataFrame.\n",
    "    For each column take action indices and add to the existing history dataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    # A slightly different process for first action recorded to avoid python warning\n",
    "    if dataFrame.size == 0: # If no data\n",
    "        \n",
    "        print('Recording first input into trade history')\n",
    "        dataFrame = pd.DataFrame(columns = dataFrame.columns)\n",
    "\n",
    "    # Adding all actions into a dictionary to append to dataFrame\n",
    "    appendDict = {}\n",
    "    \n",
    "    for i in range(0,len(action)):\n",
    "        appendDict[dataFrame.columns[i]] = action[i]\n",
    "    appendSeries = pd.Series(appendDict)    \n",
    "    \n",
    "    return pd.concat([dataFrame,appendSeries.to_frame().T], ignore_index=True)\n",
    "#######\n",
    "#######\n",
    "def EvaluateModel(dataFrame, *arguments, **keywords) -> float:\n",
    "    \"\"\"\n",
    "    This function is supposed to take the trade/call history and evaluate its properties,\n",
    "    such as success (profit/loss) and other properties (e.g. profit vs. loss freq. etc.).\n",
    "    \"\"\"\n",
    "    # Additional things to implement:\n",
    "    # Exchange rate of currency effects\n",
    "    # Trading costs\n",
    "    \n",
    "    \n",
    "    print('------------------')\n",
    "    print('Evaluating current algo model...')\n",
    "    \n",
    "    # Stratifying data based on ticker\n",
    "    stratDF = dataFrame.groupby('Ticker').apply(lambda x: x)\n",
    "    stratDF = stratDF.drop(columns=['Ticker'])\n",
    "\n",
    "    # Extended output\n",
    "    if any(val == 'Extended' for val in keywords.values()):\n",
    "        print('List of trades to be evaluated:')\n",
    "        print(stratDF)\n",
    "\n",
    "\n",
    "    # Make a DF of compiled stats\n",
    "    basicStatsCols = ['LongVol','ShortVol','RemainVol','Cost','Income','Profit']\n",
    "    compileDF = pd.DataFrame(columns = basicStatsCols)\n",
    "    \n",
    "\n",
    "    # Takes list of the (multi-level) index of DF (tuple) and converts to\n",
    "    # dictionary for unique 'Ticker' key \n",
    "    print('List of tickers traded:')\n",
    "    for key in dict(stratDF.index.tolist()).keys():\n",
    "        print(key)\n",
    "        # key is the different ticker names\n",
    "\n",
    "    # Implement the evaluation for each ticker traded here\n",
    "    for key in dict(stratDF.index.tolist()).keys():\n",
    "        # stratDF.loc[key] is the different ticker trades each in own DF\n",
    "        tickerDF = stratDF.loc[key]\n",
    "\n",
    "        # Extended output\n",
    "        for value in keywords.values():\n",
    "            if value == 'Verbose':\n",
    "                print('Trade history of ticker ' + key +':')\n",
    "                print(tickerDF)\n",
    "\n",
    "        sumVal = tickerDF['Value'].sum()\n",
    "        \n",
    "        ## Basic stats\n",
    "        sumShortVol = 0\n",
    "        sumCost = 0 # Cost is for purchasing the share etc.\n",
    "        sumLongVol = 0\n",
    "        sumIncome = 0 # Income is obtained by selling\n",
    "        for i in range(0,len(tickerDF['Value'])):\n",
    "            if tickerDF['Value'].iloc[i] > 0:\n",
    "                sumShortVol = sumShortVol + tickerDF['Volume'].iloc[i]\n",
    "                sumIncome = sumIncome + tickerDF['Value'].iloc[i]\n",
    "            else:\n",
    "                sumLongVol = sumLongVol + tickerDF['Volume'].iloc[i]\n",
    "                sumCost = sumCost - tickerDF['Value'].iloc[i]\n",
    "        profit = sumIncome - sumCost\n",
    "        specProfit = profit*2/(sumShortVol+sumLongVol)\n",
    "    \n",
    "        # To implement a existing position closing system (to evaluate the model more accurately)\n",
    "        remainVol = sumLongVol - sumShortVol\n",
    "        if remainVol != 0:\n",
    "            print(Fore.RED)\n",
    "            print(' !!!!!!!!!!')\n",
    "            print('There is an existing open position in ' + key + '! This may impact the accuracy of the model evaluation.')\n",
    "            if value == 'Extended':\n",
    "                if remainVol > 0:\n",
    "                    print('Open position size: ' + str(remainVol) + ' shares long.')\n",
    "                else:\n",
    "                    print('Open position size: ' + str(-remainVol) + ' shares short.')\n",
    "            \n",
    "            print('Existing positions will be closed.')\n",
    "            print(' !!!!!!!!!!')\n",
    "            print(Fore.BLACK)\n",
    "        \n",
    "            # Closing existing positions using last unit price\n",
    "            datasetName = 'df_' + key\n",
    "            \n",
    "            # Portfolio value is positive if long (remain vol > 0)\n",
    "            portValue = remainVol * globals()[datasetName]['Close'][len(globals()[datasetName]['Close'])-1]\n",
    "            \n",
    "            finalProfit = profit + portValue\n",
    "            if portValue > 0:\n",
    "                finalCost = sumCost\n",
    "                finalIncome = sumIncome + portValue\n",
    "            else:\n",
    "                finalCost = sumCost + portValue\n",
    "                finalIncome = sumIncome\n",
    "    \n",
    "        ## Advanced stats \n",
    "\n",
    "\n",
    "\n",
    "        # Adding all basic stats for each ticker into an array then converting into dictionary to append to dataFrame\n",
    "        # basicStats uses 'final' position if unclosed     \n",
    "        basicStats = []        \n",
    "        if remainVol != 0:\n",
    "            basicStats = [sumLongVol, sumShortVol,remainVol,finalCost,finalIncome,finalProfit]\n",
    "        else:\n",
    "            basicStats = [sumLongVol, sumShortVol,remainVol,sumCost,sumIncome,profit]\n",
    "\n",
    "        # Append dictionary for dataFrame\n",
    "        appendDict = {}\n",
    "        for i in range(0,len(basicStats)):\n",
    "            appendDict[compileDF.columns[i]] = basicStats[i]\n",
    "\n",
    "        compileDF = compileDF._append(appendDict, ignore_index=True)\n",
    "    \n",
    "        # Currently only evaluates the profit/loss levels (not including closing existing positions)\n",
    "        print('===================\\n\\nCalculating basic evaluation stats in ticker '+key+':')\n",
    "        \n",
    "        print('Current profit/loss stats (not closing existing positions):')\n",
    "        print('Total profit from all trades: ' + str(sumVal))\n",
    "        print('Total cost: ' + str(sumCost))\n",
    "        print('Total income: ' + str(sumIncome))\n",
    "        print('Total profit: ' + str(profit))\n",
    "        \n",
    "        print('Profit Margin: ' + str(profit/sumCost))\n",
    "        print('Specific Profit (per volume traded): ' + str(specProfit)) # Calculated as avg. of long and short vol. (if shares outstanding)\n",
    "    \n",
    "        if remainVol != 0:\n",
    "            print(Fore.BLUE)\n",
    "            print('Current profit/loss stats (after closing existing positions):')\n",
    "            print('Remaining portfolio value (before close): ' + str(portValue))\n",
    "            print('Final cost: ' + str(compileDF['Cost'].sum()))\n",
    "            print('Final income: ' + str(compileDF['Income'].sum()))\n",
    "            print('Final profit from all trades: ' + str(compileDF['Profit'].sum()))\n",
    "            \n",
    "            print('Final Profit Margin: ' + str(compileDF['Profit'].sum()/compileDF['Cost'].sum()))\n",
    "            print(Fore.BLACK)\n",
    "        \n",
    "        print('===================')\n",
    "\n",
    "\n",
    "    if any(val == 'AddEval' for val in keywords.values()):\n",
    "        for input in arguments: # Do not need to use *args this way but I chose to, its fine until I want to use more than 1 args dataframe\n",
    "            if isinstance(input, pd.DataFrame):\n",
    "\n",
    "                ## TO ADD TRADING ANALYTICS HERE, THIS SHOULD ONLY TAKE IN OUTPUT FROM ELSEWHERE\n",
    "                ## SHOULD CREATE FUNCTIONS TO DIRECTLY MODIFY ACCURACY MATRIX INSTEAD OF MAKING WITHIN EACH MODEL\n",
    "                ## SHOULD ALSO CREATE STOCK ANALYSER METHOD SEPARATELY\n",
    "\n",
    "            \n",
    "                print(Fore.GREEN)\n",
    "                print('#################################\\nCalculating additional model evaluations from given accuracy dataFrame:')\n",
    "                print('Model efficiency stats:')\n",
    "                print('Trade accuracy (trades being incorrect by instance, not volume?): (NOT IMPLEMENTED YET)')\n",
    "                print('Trade efficiency (how many trades are not correct?): (NOT IMPLEMENTED YET)')\n",
    "                print('Model Loss (how far are trades from actual optimal points): (NOT IMPLEMENTED YET)')\n",
    "                print('#################################')\n",
    "                print(Fore.BLACK)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print('------------------')\n",
    "    return 1\n",
    "\n",
    "df_NVDA = PrepDataFrame(df_NVDA)\n",
    "df_MSFT = PrepDataFrame(df_MSFT)\n",
    "df_KO = PrepDataFrame(df_KO)\n",
    "\n",
    "[NVDA_L1,_] = MakeTrade('2019-07-01 00:00:00', 10.5, 'NVDA')\n",
    "[MSFT_L1,_] = MakeTrade('2019-10-01 00:00:00', 6, 'MSFT')\n",
    "[NVDA_S1,_] = MakeTrade('2022-07-01 00:00:00', 10.5, 'NVDA', buy = False)\n",
    "[NVDA_S2,_] = MakeTrade('2023-06-01 00:00:00', 100, 'NVDA', buy = False)\n",
    "[NVDA_L2,_] = MakeTrade('2023-08-01 00:00:00', 99, 'NVDA', buy = True)\n",
    "[MSFT_S1,_] = MakeTrade('2022-07-01 00:00:00', 6, 'MSFT', history = None, buy = False)\n",
    "\n",
    "num = EvaluateModel(dfTradeHist, depth = 'AddEval', debug = 'Simple')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b4f61f-d884-45ea-bf1d-8431a33ba96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating sub-routines for models here\n",
    "\n",
    "# Here we will create a sub-routine that provides conditions for specified trades when a condition is met.\n",
    "# The basic one will be when a set of values becomes positive (from a negative/zero value).\n",
    "\n",
    "\n",
    "def WhenPositive(dataset,searchData,indexData):\n",
    "    \"\"\"\n",
    "    This function returns set of independent variables (outData, e.g. date/time) in the selected data\n",
    "    to be searched (indexData) from the dataset, when searchData (e.g. price) turns positive from negative.\n",
    "    searchData and indexData are strings of the column names.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Indexed (observed) dataset (independent var.)\n",
    "    obsDataset = dataset[indexData]\n",
    "\n",
    "    # Searched dataset (dependent var.)\n",
    "    searchDataset = dataset[searchData]\n",
    "\n",
    "    # Outputted dataset array (indep. var.)\n",
    "    outDataset = []\n",
    "\n",
    "    # Goes through all points to get when turn positive\n",
    "    for i in range(1,len(obsDataset)):\n",
    "        # range starts from 1 because need the i-1'th datapoint\n",
    "        # If negative, irrelevant so skip step\n",
    "        if searchDataset.iloc[i] <= 0:\n",
    "            continue\n",
    "\n",
    "        # If positive but last step negative, record\n",
    "        if searchDataset.iloc[i-1] < 0:\n",
    "            outDataset = outDataset + [obsDataset.iloc[i]]\n",
    "\n",
    "    # Note output is an array for efficiency\n",
    "    return outDataset\n",
    "\n",
    "def WhenNegative(dataset,searchData,indexData):\n",
    "    \"\"\"\n",
    "    This function returns set of independent variables (outData, e.g. date/time) in the selected data\n",
    "    to be searched (indexData) from the dataset, when searchData (e.g. price) turns negative from positive.\n",
    "    searchData and indexData are strings of the column names.\n",
    "    Same as WhenPositive but reverse as you cant just use -dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Indexed (observed) dataset (independent var.)\n",
    "    obsDataset = dataset[indexData]\n",
    "\n",
    "    # Searched dataset (dependent var.)\n",
    "    searchDataset = dataset[searchData]\n",
    "\n",
    "    # Outputted dataset array (indep. var.)\n",
    "    outDataset = []\n",
    "\n",
    "    # Goes through all points to get when turn negative\n",
    "    for i in range(1,len(obsDataset)-1):\n",
    "        # range starts from 1 because need the i-1'th datapoint\n",
    "        # range ends at len(obsDataset)-1 if last datapoint is 'live' and pending update\n",
    "        # If positive, irrelevant so skip step\n",
    "        if searchDataset.iloc[i] >= 0:\n",
    "            continue\n",
    "\n",
    "        # If negative but last step positive, record\n",
    "        if searchDataset.iloc[i-1] > 0:\n",
    "            outDataset = outDataset + [obsDataset.iloc[i]]\n",
    "\n",
    "    # Note output is an array for efficiency\n",
    "    return outDataset\n",
    "\n",
    "def JumpChecker(dataset, searchData, indexData, jumpThresh, jumpPeriod = 1, signalPeriod = 1):\n",
    "    \"\"\"\n",
    "    This function returns set of independent variables (outData, e.g. date/time) in the selected data\n",
    "    to be searched (indexData) from the dataset, when searchData (e.g. price) jumps (or changes) a\n",
    "    certain percentage (jumpThresh) on average of a (signalPeriod) time period, from its last (jumpPeriod)\n",
    "    time periods ago.\n",
    "    searchData and indexData are strings of the column names.\n",
    "    \"\"\"\n",
    "\n",
    "    # Error input guard (fixes negative input values for real numbers)\n",
    "    jumpThresh = abs(jumpThresh)\n",
    "    jumpPeriod = abs(jumpPeriod)\n",
    "    \n",
    "    # Indexed (observed) dataset (independent var.) - dataset[indexData]\n",
    "    # Searched dataset (dependent var.) - dataset[searchData]\n",
    "\n",
    "    # Outputted dataset array pair (independent var., and bool) [['Time'],['Jump?']]\n",
    "    outDataset = []\n",
    "    jumpTime = []\n",
    "    isJump = []\n",
    "    \n",
    "    # Get rolling simple moving average of dataset (SMA as signal safety vs instant drop/jumps or spread drop/jumps)\n",
    "    copyDataset = dataset.copy() # Make shallow copy to not bloat original dataset\n",
    "    copyDataset['SMA ' + searchData] = copyDataset[searchData].rolling(window = signalPeriod).mean()\n",
    "    \n",
    "    # Goes through all points to get when turn negative\n",
    "    for i in range(jumpPeriod,len(dataset[indexData])-1):\n",
    "        \n",
    "        # Range starts from jumpPeriod because SMA starts from jumpPeriod-1'th datapoint (as index starts at 0),\n",
    "        # but need the value before jumpPeriod (so +1) since that is the drop signal datapoint. \n",
    "        # Range ends at len(obsDataset)-1 if last datapoint is 'live' and pending update\n",
    "\n",
    "\n",
    "        # Error guard function (0 val input), skip step\n",
    "        if dataset[searchData][i-jumpPeriod] == 0:\n",
    "            continue\n",
    "\n",
    "        # Jump ratio is the [amount at time-index i] - [amount at i-jumpPeriod (pre-jump) time] / pre-jump value (normalisation) \n",
    "        jumpRatio = (copyDataset['SMA ' + searchData][i] - copyDataset['SMA ' + searchData][i-jumpPeriod])/copyDataset['SMA ' + searchData][i-jumpPeriod]\n",
    "        #jumpRatio = (dataset[searchData][i] - dataset[searchData][i-jumpPeriod])/dataset[searchData][i-jumpPeriod]\n",
    "        \n",
    "        # Check for value jump (above jumpThresh) or drop (below -jumpThresh), if so, record point\n",
    "        if jumpRatio > jumpThresh:\n",
    "            #print('Value jump detected: ' + str(jumpRatio*100) + '%.')\n",
    "            # Record data (as jump)\n",
    "            jumpTime = jumpTime + [dataset[indexData][i]]\n",
    "            isJump = isJump + [True]\n",
    "\n",
    "            #outDataset = outDataset + [[dataset[indexData][i], True]]\n",
    "\n",
    "        elif jumpRatio < -jumpThresh:\n",
    "            #print('Value drop detected: ' + str(-jumpRatio*100) + '%.')\n",
    "            # Record data (as drop)\n",
    "            jumpTime = jumpTime + [dataset[indexData][i]]\n",
    "            isJump = isJump + [False]\n",
    "            \n",
    "            #outDataset = outDataset + [[dataset[indexData][i], False]]\n",
    "\n",
    "    outDataset = [jumpTime, isJump]\n",
    "    # Note output is an array for efficiency\n",
    "    return outDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0db583-0d19-41a3-aee1-5ca37928f374",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing playground to add into the model evaluator\n",
    "\n",
    "\n",
    "# Note: using frozenset() for checks can make it faster for big datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be29d90-8b86-41ca-b435-1ffabfd07d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def SMIFlipTradeModel(ticker, fast_period, slow_period, signal_period, timingState = 'Close'):\n",
    "    \"\"\"This trade model trades when SMIIO of ticker becomes ('flips') to positive (long) and negative (short).\"\"\"\n",
    "\n",
    "    global histCols\n",
    "    # Create trade history dataframe\n",
    "    tradeHist = pd.DataFrame(columns = histCols)\n",
    "    \n",
    "    # Get and prep relevant dataFrame\n",
    "    df_ticker = PrepDataFrame(globals()['df_'+ticker])\n",
    "\n",
    "    # Apply SMI model to get result dataframe (and add date column)\n",
    "    df_smi = ta.momentum.smi(df_ticker['Close'],fast_period,slow_period,signal_period)\n",
    "    df_smi['Date'] = df_ticker['Date'].copy()\n",
    "    \n",
    "    smiConfigName = \"_{}_{}_{}\".format(fast_period,slow_period,signal_period)\n",
    "    # SMI + smi_name is the SMI of the stock\n",
    "    # SMIs + smi_name is the indicator made from signal line\n",
    "    # SMIo + smi_name is the oscillator made by SMI - SMIs\n",
    "    \n",
    "    smiTypeNames = [\"SMI\"+smiConfigName,\"SMIs\"+smiConfigName,\"SMIo\"+smiConfigName]\n",
    "    \n",
    "    # Removing NaN values\n",
    "    df_smi.fillna(0, inplace = True) \n",
    "    \n",
    "    buyFlip = WhenPositive(df_smi,smiTypeNames[2],'Date')\n",
    "    sellFlip = WhenNegative(df_smi,smiTypeNames[2],'Date')\n",
    "\n",
    "    # Currently only trade one stock per instance, can be made variable.\n",
    "    for i in range(0,len(buyFlip)):\n",
    "        [maketrade, tradeHist] = MakeTrade(buyFlip[i].strftime('%Y-%m-%d %H:%M:%S'), 1, ticker, history = tradeHist)\n",
    "    \n",
    "    for i in range(0,len(sellFlip)):\n",
    "        [maketrade, tradeHist] = MakeTrade(sellFlip[i].strftime('%Y-%m-%d %H:%M:%S'), 1, ticker, history = tradeHist, buy = False)\n",
    "    \n",
    "    eval = EvaluateModel(tradeHist, depth = 'AddEval', debug = 'Extended')\n",
    "    \n",
    "    return tradeHist\n",
    "\n",
    "def SuddenChangeTradeModel(ticker, changeThresh = 0.05, changePeriod = 1, revertRatio = 0.9, safetyPeriod = 0, timingState = 'Close'):\n",
    "    \"\"\"\n",
    "    This model trades when a ticker suddenly changes up (short) or down (long) by a certain amount\n",
    "    (Thresh), and 'reverts' position when it goes back up.\n",
    "    \n",
    "    Details: Can revert the up or down jumps partially using a setting (revertRatio), and can add safety factors\n",
    "    to the initation signal (safetyPeriod). Currently only takes one ticker.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create trade history (global, local seems to make issues)\n",
    "    global histCols\n",
    "    tradeHist = pd.DataFrame(columns = histCols)\n",
    "    \n",
    "    # Get relevant dataFrame and pre-process it\n",
    "    df_ticker = globals()['df_'+ticker].copy()\n",
    "    df_ticker = PrepDataFrame(df_ticker)\n",
    "\n",
    "    # Obtain timings of significant price changes ([Date of change ending, bool if its jump (up)])\n",
    "    changeTimings = JumpChecker(df_ticker, timingState, 'Date', changeThresh, jumpPeriod = changePeriod, signalPeriod = safetyPeriod)\n",
    "    # Make dummy ticker to establish repurchase date\n",
    "    df_dummy = copy.deepcopy(df_ticker)\n",
    "    \n",
    "    # Currently only trade one stock per instance, can be made variable.\n",
    "    for i in range(0,len(changeTimings[0])):\n",
    "        \n",
    "        # If jump\n",
    "        if changeTimings[1][i]:\n",
    "            # Sell now, buy later\n",
    "            [makeTrade, tradeHist] = MakeTrade(changeTimings[0][i].strftime('%Y-%m-%d %H:%M:%S'), 1, ticker, history = tradeHist, buy = False)\n",
    "\n",
    "        else: # If drop\n",
    "            # Buy now, sell later\n",
    "            [makeTrade, tradeHist] = MakeTrade(changeTimings[0][i].strftime('%Y-%m-%d %H:%M:%S'), 1, ticker, history = tradeHist)\n",
    "\n",
    "    \n",
    "    # Get the time and value of stock pre-jump after all 'procedure starting' trades are done (to be used for revert trades)\n",
    "    # Sudden change is generally referred to as Jump here for simplicity\n",
    "    # This is done at end to not enlarge list during iterations\n",
    "\n",
    "    # This is a list comprehension; result (preJumpIndex) is left (i - changePeriod) for output (i, _)\n",
    "    # (i is index num, x/t is value itself) in input variable. Can also add boolean\n",
    "    # condition after (i.e. if tradeHist['Date'] = 01-01-2023)\n",
    "    jumpTime = [x for _, x in enumerate(tradeHist['Date'])] # Note x is in str not timestamp\n",
    "    jumpIndex = [i for i, t in enumerate(df_ticker['Date']) if t.strftime('%Y-%m-%d %H:%M:%S') in jumpTime]\n",
    "    jumpValue = [df_ticker[timingState][i] for i in jumpIndex]\n",
    "    preJumpValue = [df_ticker[timingState][i - changePeriod] for i in jumpIndex]\n",
    "\n",
    "    revertValue = [jumpValue[i] + revertRatio*(preJumpValue[i] - jumpValue[i]) for i in range(0,len(jumpValue))]\n",
    "\n",
    "    # Get (spot - revert) values and check for negative/positive first swap point for each initiation trade and\n",
    "    # make a reversion trade at that point.\n",
    "    # We will put each reversion point of a trade on the initiation trade time in the dataframe\n",
    "    df_dummy['ReversionVal'] = None\n",
    "    df_dummy['ReversionDate'] = None\n",
    "    df_dummy['UnclosedTrade'] = None # To highlight unclosed trades\n",
    "    \n",
    "    for i in range(0,len(jumpIndex)):\n",
    "        # i is out of the number of jumps traded, j is the index (in the dataframe) where the trade is done.\n",
    "        j = jumpIndex[i]\n",
    "\n",
    "        # We use .loc to get j'th point in 'Reversion' column as it takes the variable in memory and not its mirror/copy.\n",
    "        df_dummy.loc[j,('ReversionVal')] = revertValue[i]\n",
    "        # df_dummy['Reversion'][i] is CHAINED INDEXING and won't work because it calls\n",
    "        # df_dummy.__getitem__('Reversion).__setitem__(i) = ... which may not be applied to df_dummy location\n",
    "        # in memory layout (as getitem) and be thrown out immediately. But, .loc dodges this by having __setitem__ only.\n",
    "        # Note: this wouldnt be an issue if the chained indexing happened on the other side (unless doing assignment?).\n",
    "\n",
    "        # The spot - reversion values (i'th value happens at index j within dataframe)\n",
    "        df_dummy['DistToRev'] = None\n",
    "        df_dummy['DistToRev'] = df_dummy['Close'][j:] - revertValue[i]\n",
    "        \n",
    "    \n",
    "        # Distance (of value at first trade) to reversion point is positive if jump (as immediately greater than\n",
    "        # reversion point), and negative if drop (immediately below reversion point).\n",
    "        # Thus, depending on first value we know what if buy or sell first, then select if WhenPositive or WhenNegative\n",
    "        if df_dummy['DistToRev'][j] > 0:\n",
    "            # Buy back now\n",
    "            # To scan and find the first point distance-to-reversion value changes sign (becomes negative)\n",
    "            scanRange = range( j, len(df_dummy['DistToRev']) )\n",
    "            reverseTrade = WhenNegative(df_dummy.iloc[scanRange],'DistToRev','Date')\n",
    "            if len(reverseTrade) != 0:\n",
    "                [makeTrade, tradeHist] = MakeTrade(reverseTrade[0].strftime('%Y-%m-%d %H:%M:%S'), 1, ticker, history = tradeHist)\n",
    "            else:\n",
    "                print('Unclosed short detected at time: ' + df_dummy['Date'][j].strftime('%Y-%m-%d %H:%M:%S'))\n",
    "                print('Unclosed trades can cause significant losses! Initial trade elected to be cancelled.')\n",
    "                df_dummy.loc[j,('UnclosedTrade')] = df_dummy['Close'][j]\n",
    "                [makeTrade, tradeHist] = MakeTrade(changeTimings[0][i].strftime('%Y-%m-%d %H:%M:%S'), 1, ticker, history = tradeHist)\n",
    "\n",
    "\n",
    "        elif df_dummy['DistToRev'][j] < 0:\n",
    "            # Sell back now\n",
    "            # To scan and find the first point distance-to-reversion value changes sign (becomes positive)\n",
    "            scanRange = range( j, len(df_dummy['DistToRev']) )\n",
    "            reverseTrade = WhenPositive(df_dummy.loc[scanRange],'DistToRev','Date')\n",
    "            if len(reverseTrade) != 0:\n",
    "                [makeTrade, tradeHist] = MakeTrade(reverseTrade[0].strftime('%Y-%m-%d %H:%M:%S'), 1, ticker, history = tradeHist, buy = False)\n",
    "            else:\n",
    "                print('Unclosed long detected at time: ' + df_dummy['Date'][j].strftime('%Y-%m-%d %H:%M:%S'))\n",
    "                print('Unclosed trades can cause significant losses! Initial trade elected to be cancelled.')\n",
    "                df_dummy.iloc[j,('UnclosedTrade')] = df_dummy['Close'][j]\n",
    "                [makeTrade, tradeHist] = MakeTrade(changeTimings[0][i].strftime('%Y-%m-%d %H:%M:%S'), 1, ticker, history = tradeHist, buy = False)\n",
    "\n",
    "        else:\n",
    "            raise ValueError('The immediate distance of price to reversion trade point should not be 0 or NaN.')\n",
    "\n",
    "    # Display graphical data of the model (when trades were done, size of trades). (Also try to display the reversion datasets.)\n",
    "    print(df_dummy.to_string())\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(df_dummy['Date'],df_dummy['ReversionVal'])\n",
    "    plt.plot(df_dummy['Date'],df_dummy['Close'])\n",
    "    plt.plot(df_dummy['Date'],df_dummy['UnclosedTrade'])\n",
    "    \n",
    "\n",
    "    eval = EvaluateModel(tradeHist, df_dummy, depth = 'AddEval', debug = 'Extended')\n",
    "\n",
    "    # AFTER THIS CREATE A MODEL CLASS AND USE IT TO CREATE A CUSTOM MODEL TYPE VARIABLE WITH CALLABLE OUTPUT VALUES\n",
    "    # AND CUSTOMISABLE INPUTS\n",
    "    \n",
    "    return tradeHist\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6fb804-56fa-498b-8ab1-75ccafdd95cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = SuddenChangeTradeModel('NVDA', 0.1, 10, 0.75, 2)\n",
    "\n",
    "#data = SMIFlipTradeModel('NVDA', 3, 10, 5)\n",
    "\n",
    "\n",
    "###\n",
    "###\n",
    "# The code seems to work as intended but I will check again, but the method seems to not work. There could be at least 3-5 reasons.\n",
    "# 1) Selling the buying (and vice versa) does not work if the stock trajectory long-term is upward as there will be cases where you cannot sell again\n",
    "# (and when forced to close position, are at a loss).\n",
    "# 2) Linked to 1) the periodicity of the model application means if applied at the wrong timeframe (granularity) i.e. each datapoint is day and\n",
    "# not hour or 15 mins, it causes loss as the trajectory of the model is more refined and there will be more dips and peaks.\n",
    "# 3) The code should be also tested for models of different trajectories (long-term, not related to granularity), as perhaps that influences the\n",
    "# outcome more than the effect of granularity.\n",
    "# 4) A general loss-stop missing, maybe the biggest losses are due to a lack of loss-stop method and a bleed in the earlier trades.\n",
    "# 5) Perhaps the method itself is statistically bad/incorrect (i.e. when a jump happens the stock is likelier than not to keep going up and not\n",
    "# reverting)\n",
    "###\n",
    "# Note that removing unclosed trades fixes all issues as hypothesized. Max. profit margin is approx. jumpRatio*reversionRatio. But issue is we cannot\n",
    "# know in advance if there will be unclosed trades unless we know the trajectory of the stock or if we use a stop-loss\n",
    "# need to record if trade is incomplete, add statistical modelling as well to assess shortfall?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ace4030-e4db-41a8-982a-97e04e65bb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "## THIS CODE IS SHELVED, NO NEED TO USE (CODE DISABLED, CAN REENABLE WITH THE BOOL SETTINGS BELOW)\n",
    "\n",
    "\n",
    "## Here we scrape data from websites or from yahoo finance api\n",
    "\n",
    "# Have them here at the end as they are not being used currently but may have some use\n",
    "# Examples of future use are: sentiment analysis, or low temporal resolution results\n",
    "# Also raw code so I do not need to immediately go through using data manifest class\n",
    "\n",
    "#############\n",
    "# User settings (to avoid looking for lines and changing manually)\n",
    "# Enable various scraping mechanisms\n",
    "enableSelenium = False\n",
    "enableyFinance = False\n",
    "enableTVS = False\n",
    "enableAlphavantage = False\n",
    "\n",
    "# Quit chrome after selenium use complete\n",
    "chromeQuit = True\n",
    "\n",
    "# Export data to a file?\n",
    "export = True\n",
    "\n",
    "###########\n",
    "# Taking direct data using Alphavantage's API of intraday and daily values\n",
    "if enableAlphavantage:\n",
    "    # Alphavantage API key (intraday upto a month length each time, limited daily request, free), no scraping\n",
    "    # Something like '69SFCX93J1H8V9K0'\n",
    "\n",
    "    # Documentation for API here: https://www.alphavantage.co/documentation/\n",
    "\n",
    "    ### Extract data manifest from folder and ignore existing datasets in API request\n",
    "    # Load manifest file (if exists, if not, create empty file)\n",
    "    try:\n",
    "        dataManifestFile = open(r'StockHistData\\dataManifest.txt',\"r\")\n",
    "    except FileNotFoundError:\n",
    "        dataManifestFile = open(r'StockHistData\\dataManifest.txt',\"w\")\n",
    "\n",
    "    # EXTRACT JSON AND \n",
    "    #textdata = dataManifestFile.read()\n",
    "\n",
    "    \n",
    "    #print(textdata) # NEED TO HAVE CODE TO CONVERT OUTPUT STRAIGHT INTO A DF OR\n",
    "    dataManifestFile.close()\n",
    "    \n",
    "    # Here we scrape past intraday stock data (not interested in testing current prices as can test it later by making it past :D )\n",
    "    dataManifestFile = open(r'StockHistData\\dataManifest.txt',\"a\")\n",
    "    \n",
    "    for symbol in dltickers:\n",
    "        for month in dlmonths:\n",
    "            for interval in dlintervals:\n",
    "                # Check data file here\n",
    "                ## NEED TO CHECK FILE DATA HERE, IF NO FILE, NEED TO CATCH NOFILEERROR AND CREATE NEW ONE AND BLANK MIDF\n",
    "                # READ FILE, IF DOESN'T EXIST, DOWNLOAD DATA AND SAVE AND ADD TO MANIFEST\n",
    "                \n",
    "                alphaURL = rf\"https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY&symbol={symbol}&interval={interval}min&month={month}&outputsize=full&apikey={alphaAPIkey} \"\n",
    "                print(alphaURL)\n",
    "                r = requests.get(alphaURL)\n",
    "                data = r.json()\n",
    "\n",
    "                # We add this dataset to the manifest, and save the data itself\n",
    "                #dataManifestFile.write()\n",
    "    \n",
    "    dataManifestFile.close()\n",
    "                \n",
    "    scrapeDF = pd.DataFrame.from_dict(data, orient='columns')\n",
    "    print(scrapeDF)\n",
    "    # Check if dataset exists before making request (to avoid wasting limited daily calls)\n",
    "\n",
    "    \n",
    "    #dataManifestFile.close()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "##########\n",
    "# Selenium scraper to get intra-day prices (not completed so doesn't get intraday prices)\n",
    "# May need higher level expertise to extract data from jscript objects \n",
    "if enableSelenium:\n",
    "    DRIVERPATH = r\"D:\\Finance Study\\chromedriver-win64\\chromedriver-win64\\ \"\n",
    "    \n",
    "    # Set Chrome options\n",
    "    options = Options()\n",
    "    options.headless = False #True # Enable headless mode (no GUI)\n",
    "    options.add_argument(\"--window-size=1920,1200\")  # Set the window size\n",
    "    \n",
    "    \n",
    "    # Init Chrome driver (I guess it's a semi-manual task?)\n",
    "    driver = webdriver.Chrome()#executable_path = DRIVERPATH)\n",
    "    \n",
    "    # Navigate to the desired page\n",
    "    for url in yUrls:\n",
    "        print(\"==================\")\n",
    "        driver.get(r''+url)\n",
    "        time.sleep(5)\n",
    "    \n",
    "    \n",
    "    # Testing here (to develop interaction code here)\n",
    "    #print(driver.page_source)\n",
    "    \n",
    "    \n",
    "    # Good practice to quit when done\n",
    "    if chromeQuit: driver.quit()\n",
    "\n",
    "##########\n",
    "# yFinance to scrape Yahoo Finance\n",
    "if enableyFinance:\n",
    "    # yFinance (Yahoo Finance Historical Data (daily))\n",
    "    # Ticker object array\n",
    "    tickObjArr = [yf.Ticker(ticker) for ticker in dltickers]\n",
    "    \n",
    "    # Fetch historical data\n",
    "    tframe = \"5d\"#\"1mo\"#\"1y\"\n",
    "    histData = [tickObj.history(period = tframe) for tickObj in tickObjArr]\n",
    "    for i in range(len(histData)):\n",
    "        print(\"Historical data for \" + tickObjArr[i].ticker + \":\")\n",
    "        print(histData[i])\n",
    "    \n",
    "    \n",
    "    # Fetch basic financial data\n",
    "    finData = [tickObj.financials for tickObj in tickObjArr]\n",
    "    for i in range(len(finData)):\n",
    "        print(\"Basic Financial data for \" + tickObjArr[i].ticker + \":\")\n",
    "        print(finData[i])\n",
    "    \n",
    "    # Fetch stock actions like dividends and splits\n",
    "    actionData = [tickObj.actions for tickObj in tickObjArr]\n",
    "    for i in range(len(actionData)):\n",
    "        print(\"\\nStock Actions for \" + tickObjArr[i].ticker +  \":\")\n",
    "        print(actionData[i])\n",
    "    \n",
    "    # Using soup\n",
    "    yUrls = [ f'https://finance.yahoo.com/quote/{ticker}/' for ticker in dltickers\n",
    "    ]\n",
    "    #print(urls)\n",
    "    r = requests.get(url=yUrls[0], headers=user_header)\n",
    "    #print(r.content)\n",
    "    \n",
    "    soup = BeautifulSoup(r.content, 'html5lib') # If this line causes an error, run 'pip install html5lib' or install html5lib\n",
    "    #print(soup.prettify())\n",
    "    #table = soup.find('div',)\n",
    "    \n",
    "    company = soup.find('h1', {'class': 'yf-xxbei9'}).text\n",
    "    #print(company)\n",
    "    closePrice = soup.find('div', {'class': 'stx-btn-panel stx-show'})\n",
    "    print(closePrice)\n",
    "    closePrice = soup.find('span', {'class': 'stx-ico-close'})\n",
    "    print(closePrice)\n",
    "\n",
    "\n",
    "\n",
    "##########\n",
    "# Tradingview Scraper\n",
    "if enableTVS:\n",
    "    \n",
    "    # Ideas scraper\n",
    "    # Ideas are the tab in the webpage with articles of sorts\n",
    "    # Initialize the Ideas scraper with default parameters\n",
    "    \n",
    "    # Default: export_result=False, export_type='json'\n",
    "    ideas_scraper = Ideas(\n",
    "      export_result=True,  # Set to True to save the results\n",
    "      export_type='csv'    # Specify the export type (json or csv)\n",
    "    )\n",
    "    \n",
    "    # Default symbol: 'BTCUSD'\n",
    "    # Scrape ideas for the NVDA symbol, from page 1 to page 1\n",
    "    ideas = ideas_scraper.scrape(\n",
    "      symbol=\"NVDA\",\n",
    "      startPage=1,\n",
    "      endPage=1,\n",
    "      sort=\"popular\"  #  Could be 'popular' or 'recent'\n",
    "    )\n",
    "    \n",
    "    #print(\"Ideas:\", ideas)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##########\n",
    "    # Indicators\n",
    "    from tradingview_scraper.symbols.technicals import Indicators\n",
    "    \n",
    "    # Scrape all indicators for the BTCUSD symbol\n",
    "    indicators_scraper = Indicators(export_result=True, export_type='json')\n",
    "    indicators = indicators_scraper.scrape(\n",
    "        symbol=\"BTCUSD\",\n",
    "        timeframe=\"4h\",\n",
    "        allIndicators=True\n",
    "    )\n",
    "    #print(\"All Indicators:\", indicators)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
